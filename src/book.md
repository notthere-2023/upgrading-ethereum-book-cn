# Preface <!-- /preface/ -->

## Upgrading Ethereum

Welcome to my book about upgrading Ethereum: Ethereum on proof of stake and beyond.

### Work in progress!

I am writing this book backwards. Bottom up. Starting with the details and working towards the big picture.

The first pretty much complete part is [Part 3: The Annotated Spec](/part3/). These are the guts of the machine. Like the innards of a computer, all the components are showing and the wires are hanging out: everything is on display. But with the guts in place, everything else can be built around them with the messiness all neatly tucked away.

I'm now working on [Part 2: Technical Overview](/part2/) which wraps a first, hopefully more accessible, layer around the Annotated Spec. Again, I'm writing this backwards, starting with the protocol's [Building Blocks](/part2/building_blocks/) and its [Incentive Mechanisms](/part2/incentives/) and working forwards towards a higher level narrative of how it all fits together. The current focus is on the [Consensus](/part2/consensus/) chapter.

**Warning:** until Edition 1.0 is out, anything may change. I'll try not to change URLs and anchors in the Annotated Spec part, but no promises. Anything else, including entire chapters and sections, should be considered unstable.

### What to expect

This is a book for those who want to understand Ethereum&nbsp;2.0 &ndash; Ethereum on proof of stake &ndash; at a technical level. What does it do? How does it work? Why is it like this?

Who am I writing for? For people like me! People who enjoy understanding how things work. But more than that, who like to know _why_ things are the way they are. This is not intended to be an academic work; I am more interested in insight than in rigour. But I try to link out to relevant academic papers and formal analyses where relevant.

Although I am an Ethereum staker and an Ethereum user, I am not writing primarily for stakers or users here. Some of the generic material on [Staking](/appendices/staking/) may be relevant (once I have written it), but you will find better help in places like the excellent [EthStaker](https://ethstaker.cc/) community.

The scope of the book concerns (what I consider to be) the Ethereum&nbsp;2.0 protocol. Ethereum&nbsp;2.0 has become a less well-defined term recently. But for me, it broadly includes,

  - all things proof of stake and the beacon chain,
  - the process of The Merge by which Ethereum moved to proof of stake,
  - in-protocol data sharding, and
  - an array of potential future enhancements.

I will not be covering any of the historic Ethereum&nbsp;1.0 protocol, except as it touches upon The Merge. The [Mastering Ethereum book](https://github.com/ethereumbook/ethereumbook) is an excellent resource, though rather out of date now. Although rollups and other so-called layer 2 solutions have rapidly become part of the overall Ethereum&nbsp;2.0 system, they are by definition not in-protocol, and I will not be covering them here. I will not be discussing, DeFi, DAOs, NFTs, or any of the wonderful things that can be built on top of this amazing technology.

It's a chunky list of exclusions, but there's still [plenty to talk about](/contents/).

### Versions

This edition covers the Capella version of the deployed Ethereum&nbsp;2.0 consensus layer. The beacon chain went live with [Phase&nbsp;0](/part4/history/phase0/) on December 1st, 2020. It was upgraded to [Altair](/part4/history/altair/) on October 27th, 2021, to [Bellatrix](/part4/history/bellatrix/) on September 6th, 2022, and to [Capella](/part4/history/capella/) on April 12th, 2023.

Specifically, unless otherwise stated, any reference to the consensus specifications is to the version [tagged v1.3.0](https://github.com/ethereum/consensus-specs/tree/v1.3.0) (the [Gamlum](https://github.com/ethereum/consensus-specs/releases/tag/v1.3.0) release).

Historical and current versions of Upgrading Ethereum are available online:

  - The old [Altair spec version](/../altair/),
  - The old [Bellatrix spec version](/../bellatrix/),
  - The current [Capella spec version](/../capella/), and
  - The [latest version](/../latest/), which is currently Capella.

My original annotated specification for the Phase&nbsp;0 version is [also available](https://benjaminion.xyz/eth2-annotated-spec/phase0/beacon-chain/), though largely of historical interest and now subsumed into this book.

### In defence of &ldquo;Eth2&rdquo;

When I started this writing project, Ethereum on proof of stake was universally known as Ethereum&nbsp;2.0, Eth2 for short, as it had been since 2014. This reflects the fact that the Ethereum&nbsp;2.0 vision diverged from the original Ethereum&nbsp;1.0 implementation in many respects. The direction of travel was well-captured in the [Ethereum.org 2.0 Mauve Paper](https://docs.google.com/document/d/1maFT3cpHvwn29gLvtY4WcQiI6kRbN_nbCf3JlgR3m_8/edit#) in 2016. The details of our eventual implementation differ from what's presented there, but the elements were all in place even then. Eth2 was to deliver not only proof of stake, but new cryptography, new peer-to-peer networking, new cryptoeconomics, and new directions in scalability, among other things. This is what we achieved.

In 2022, the Ethereum Foundation [declared](https://blog.ethereum.org/2022/01/24/the-great-eth2-renaming) that we must emphasise continuity over discontinuity by rebranding Ethereum&nbsp;2.0 back to plain old Ethereum. With the way that the Merge was coming together, that made some sense from an Ethereum user's point of view. The user and developer experience was to be almost unchanged by the move to proof of stake.

However, I am not writing primarily as a user of Ethereum or as a developer on Ethereum. From the point of view of a _protocol engineer_, the Merge was a moment of profound discontinuity. Ethereum on proof of stake is utterly different from Ethereum on proof of work - the size and complexity of this book is ample evidence of that. With Ethereum's new architecture we have delivered a good chunk of what the Mauve Paper laid out all those years ago.

This is why I often continue to refer to Ethereum on proof of stake as Ethereum&nbsp;2.0 or Eth2[^fn-ef-overreach], and I make no apology for that. I am confident that you will understand what I mean, and that it will help clarify rather than confuse.

[^fn-ef-overreach]: That, and being a bloody-minded reactionary who dislikes being told what to do and say by a centralised authority. The EF's claim that, "As of late 2021, core developers stopped using the terminology" is laughably untrue.

<a id="british-english"></a>
While we're here, another thing I won't be apologising for is using British English spelling, punctuation, and quaint idioms. It's a feature, not a bug.

### Acknowledgements

Above all, I want to thank my employer, [Consensys](https://consensys.io/). Much of the writing has been done in my own time, but Consensys has been increasingly supportive of me working on this in the course of my day job. In particular, during Q2 2023, the company granted me three months of writing leave to work full-time on the book. They also allowed me to put a liberal licence on everything. All this is extremely cool of them, and I am deeply grateful. Consensys is a wonderful employer, a terrific force for good in the ecosystem, and an incredible place to work.

So much of what I do involves writing about other people's work, and pretty much everything in this book is other people's work. I deeply value the openness and generosity of the Ethereum community. For me, this is one of its defining characteristics. Many people's contributions are cited throughout this book, and I am indebted to all of you. Being part of the Eth2 dev community has been the best experience of my life.

Thank you to the many GitCoin grant supporters who donated in support of the original annotated specification and my regular What's New in Eth2 newsletter. And to generous crypto friends, anon and otherwise, for your kind gifts over the years. Your support has encouraged me hugely as I've wrestled with the minutiae of the spec. I bloody love this community.

Shout-out to the EthStaker community: you rock!

Finally, to circle back to Consensys: working daily with such brilliant, talented, generous, and knowledgeable people is a joy. The Protocols group, PegaSys, has been my home for the past five-plus years. It is where I helped establish the fabulous Protocols R&D team, and later kicked off the project that became Teku. Thank you for all your support and encouragement. I love working with all you wonderful people.

# Part 1: Building <!-- /part1/ -->

## Introduction <!-- /part1/introduction/* -->

TODO

### Why Ethereum 2.0? <!-- /part1/introduction/whyeth2/* -->

TODO

### The Cathedral and the Bazaar <!-- /part1/introduction/catb/* -->

TODO

### A Brief History of Ethereum's Future <!-- /part1/introduction/history/* -->

TODO

### Who's who <!-- /part1/introduction/who/* -->

TODO

### Outline of the Book <!-- /part1/introduction/outline/* -->

TODO

## Goals <!-- /part1/goals/* -->

### Introduction

TODO

### Design Goals <!-- /part1/goals/design/* -->

TODO

### Attacks and Defences <!-- /part1/goals/attacks/* -->

TODO

## Making the Sausage <!-- /part1/making/* -->

### Introduction

TODO

### The Specifications <!-- /part1/making/specs/* -->

TODO

### The Implementations <!-- /part1/making/implementations/* -->

TODO

# Part 2: Technical Overview <!-- /part2/ -->

## Introduction <!-- /part2/introduction/* -->

TODO: Intro

## The Beacon Chain <!-- /part2/beacon/* -->

### Introduction

TODO

### Terminology <!-- /part2/beacon/terms/* -->

TODO

### Design Overview <!-- /part2/beacon/overview/* -->

TODO

### Architecture of a Node <!-- /part2/beacon/arch/* -->

TODO

### Genesis <!-- /part2/beacon/genesis/* -->

TODO

## 共识 <!-- /part2/consensus/ -->

这是一篇关于对以太坊&nbsp;2.0 共识协议的攻击的[论文](https://arxiv.org/abs/2110.10086)的开篇句：

> 以太坊的权益证明（PoS）共识协议是通过在分叉选择规则 LMD GHOST 之上应用最终确定性小工具 Casper FFG 来构建的，LMD GHOST 是贪婪的、最重的可观察子树（Greedy Heaviest-Observed Sub-Tree, GHOST）规则的一种变体，它只考虑每个参与者的最近一次投票（最新消息驱动，Latest Message Driven, LMD）。

如果以上内容对你来说完全没问题，那么你可以跳过这一整章。否则，往下读吧！

我们的目标是理解这句话的所有部分。这里有很多东西要拆解，但我们会慢慢来。我们将从一些[初步的内容](/part2/consensus/preliminaries/) 开始，涵盖一些不限于以太坊的共识基础。

在对整个共识协议如何组合在一起的高层次[概述](/part2/consensus/overview/)之后，我们将深入探讨它的组成部分，首先是 [LMD GHOST](/part2/consensus/lmd_ghost/)，然后是 [Casper FFG](/part2/consensus/casper_ffg/)。在 [Gasper](/part2/consensus/gasper/) 部分，我们将看到两者是如何结合在一起的。

由于 LMD GHOST 和 Casper FFG 协议之间的互操作方式导致很多微妙之处和边缘情况，我专门在这一章的结尾部分讨论了这些[议题](/part2/consensus/issues/)。

### 基础知识 <!-- /part2/consensus/preliminaries/ -->

<div class="summary">

  - 共识是一种通过不可靠的组件构建可靠的分布式系统的方法。
  - 基于区块链的分布式系统旨在就单一的交易历史达成一致。
  - 工作量证明和权益证明不是共识协议，而是使共识协议成为可能。
  - 许多区块链共识协议是“可分叉”的。
  - 可分叉的链使用分叉选择规则，有时也会发生重组。
  - 在一个“安全”的协议中，永远不会发生坏事。
  - 在一个“活跃”的协议中，总会发生好事。
  - 现实中不存在始终保持安全和活跃的协议。

</div>

#### 引言

本节介绍与共识、分叉选择和最终确定性相关的基础知识。其中大部分内容并不特定于以太坊，而是为提供一般的背景理解。

共识协议试图解决的挑战是在不可靠的基础设施之上构建可靠的分布式系统。对共识协议的研究可以追溯到 20 世纪 70 年代甚至更早，但我们在以太坊中所寻求解决的挑战的规模要大得多。

我们在以太坊共识层的目标是使全球数万个独立节点能够完全同步运行。每个节点维护一个包含每个账户状态的账本，而每个账本必须与其他所有账本相匹配。不能有任何差异；节点必须达成一致，迅速地达成一致。这就是我所说的“可靠的分布式系统”。

这些节点通常运行在[消费级的硬件](https://stakefromhome.com/)上。它们通过互联网这样一个可能有着低带宽、高延迟、丢包、或无限期中断的不可靠的异步网络进行通信。节点运营者有时会错误配置他们的软件，或者不及时更新。而且，更刺激的是，有可能有大量的攻击者为了获得利益而运行恶意节点或者篡改通信。这就是我所说的“不可靠的基础设施”。

以太坊的一个明确的设计目标是，它不仅仅在每个节点稳定运行和通信时才运行良好。我们已经尽力设计了一个系统，即使其下的世界崩溃了，它也会尽可能继续运行。

#### 达成共识

以太坊网络由大量的单个节点组成。每个节点独立行动，节点之间通过不可靠的异步网络，即互联网进行通信。任一单个节点可能是诚实的 &ndash; 始终正确行动 &ndash; 或者以各种方式出错。简单的错误如停机或无法通信，或者是遵循了不同版本的协议、主动试图误导其他节点、发布矛盾的消息、以及任何其他形式的故障。

用户提交交易至节点网络后，共识协议确保所有正确节点就交易历史（即交易处理的顺序及结果）达成一致。例如，如果我拥有 1&nbsp;个以太币并尝试同时发送给 Alice 和 Bob，我们会预期网络最终确认只有 Alice 或 Bob 其中一方收到这个以太币。若两者都收到或均未收到，则交易失败。

共识协议是就交易的排序达成一致的过程。

实际上，以太坊的共识协议“拧和”了 [LMD GHOST](/part2/consensus/lmd_ghost/) 和 [Casper FFG](/part2/consensus/casper_ffg/) 两种协议，这一组合即 [Gasper](/part2/consensus/gasper/)。我们将在后续内容中详细探讨这两种协议。


#### 拜占庭将军

在 1982 年的一篇[论文]((https://lamport.azurewebsites.net/pubs/byz.pdf))中，Leslie Lamport 用相当[诙谐的方式](https://www.microsoft.com/en-us/research/publication/byzantine-generals-problem/)描述了共识系统试图解决的基本问题——构建可靠的分布式系统。

> 让我们想象拜占庭军队的几个军团分别扎营在敌方城市外，每个军团由自己的将军指挥。将军们只能通过信使相互通信。观察敌情后，他们需要制定统一的行动计划。

这一表述说的很清楚——没有全局的整体视角，没有上帝模式，我们无法”一览众山小“并做出决定。我们只是将军中的一个，关于其他将军的唯一信息来源是所收到的消息——它们可能是真实的，也可能是虚假的，或者是基于有限信息的误解，又或者是在传递过程中被延误或篡改。我们的视角非常有限，但我们必须尽可能全面理解整个系统的状态。

需要时刻记住这一点。当我们绘制区块链和区块树时，往往假设这就是整个系统的某种“状态（the state）”。但这些图只能代表系统中单个参与者的局部视角。我的节点对系统的理解可能与你的不同，哪怕只是暂时的，因为我们都在不可靠的网络中运行。例如，我俩会在不同的时间，或者以不同的顺序看到同样的区块，更有甚者，我们所见的区块也许彼此大相径庭。

Lamport 用下面的话捕捉到系统的缺陷:

> 然而,部分将军可能是叛徒，试图阻止忠诚的将军达成协议。

这些变节的将军象征着我们所称的“拜占庭行为”或“拜占庭故障”。他们可以以任意方式行事：拖延消息、对消息重新排序、径直撒谎、向不同收件人发送矛盾的消息、完全不作回应、或任何我们能想到的其他行为。

<a id="img_consensus_messages"></a>
<figure class="diagram" style="width: 50%">

![A picture of a node with messages coming in.](images/diagrams/consensus-messages.svg)

<figcaption>

我收到来自其他节点的大量消息，但我不知道哪些是准确的，也不知道它们的发送顺序，以及是否有任何消息丢失或只是延迟。但无论如何，我们需要达成协议。

</figcaption>
</figure>

忠诚的将军需要一种方法，以能够可靠地产生如下结果：

> A. 所有忠诚的将军决定采取相同的行动计划（例如“进攻”或“撤退”），以及
>
> B. 少数叛徒无法致使忠诚的将军们采用错误计划。

在这种拜占庭分布式系统中达成共识并非易事，但多年来已有一些相当成功的方法。

第一种主流的解决方案是 1999 年 Liskov 和 Castro 发表的实用拜占庭容错（[Practical Byzantine Fault Tolerance](https://www.scs.stanford.edu/nyu/03sp/sched/bfs.pdf)，PBFT）算法。 这种算法依赖于相对较小和有限的已知共识参与者（被称为副本，replica）集合。在[下面](#safety)讨论的语境中，PBFT 总是“安全的”，且不会产生分叉。

中本聪共识——由中本聪为比特币[发明](https://bitcoinpaper.org/bitcoin.pdf)于 2008 年——采取了截然不同的方法。它不是将参与者限制在某个已知集合中，而是使用工作量证明来无许可地选择临时领导者进行共识。与 PBFT 不同,中本聪共识允许分叉，且在形式上是不“安全的”。

自此以后，这些方法和其他的新颖替代方案（如 [Avalanche 系列协议](https://arxiv.org/pdf/1906.08936)）的许多变体已大量涌现。[Avalanche 白皮书](https://arxiv.org/pdf/1906.08936)的第 7 节“相关工作（Related Work）”对目前在区块链世界中使用的各种共识协议进行了很好的概述。

#### 权益证明和工作量证明

在此，我们不妨指出，工作量证明和权益证明本身都不是共识协议。它们经常被（懒散地）称为共识协议，但都只是共识协议的辅助工具。

大多数情况下，工作量证明和权益证明都是[抵抗女巫攻击](/part2/incentives/staking/#introduction)的机制，它们为参与协议设定一种成本，而这可以防止攻击者以低成本或零成本压垮协议[^fn-types-of-proof]。

[^fn-types-of-proof]：在工作量证明中,你提供的”证明”是一个使区块哈希成为特定值的数字。这证明了你确实做了计算工作。在权益证明中，你的证明则是与区块链上的质押存款相关联的私钥。还有其他可用的证明机制,比如[时空证明](https://en.wikipedia.org/wiki/Proof_of_space#Proof_of_space-time)。

尽管如此，通过[分叉选择规则](#fork-choice-rules)，工作量证明和权益证明通常都与它们所支持的共识机制紧密耦合。它们提供了一种有用的方法来为区块链分配权重或分数：在工作量证明中，是已完成的总工作量；在权益证明中，是支持特定区块链的价值量。

除了这些基本因素外，工作量证明和权益证明都支持在其基础上建立多种不同的共识协议，而每种协议都有自己的动态和权衡。同样，[Avalanche 白皮书](https://arxiv.org/pdf/1906.08936)第 7 节“相关工作”中的概述很有启发性。

#### 区块的链

区块链技术背后的基本原语当然是区块。

一个区块由一个领导者（区块提议者）收集的一组交易组成。一个区块的内容（有效负荷）可能因协议而异。

  - 以太坊执行层链上的区块有效载荷是用户交易列表。
  - 合并前的权益证明信标链上的区块有效载荷（大部分）是由其他验证者做出的一系列认证。
  - 合并后的信标链区块也包含了执行层的有效载荷（用户交易）。
  - 当 [EIP-4844](https://eips.ethereum.org/EIPS/eip-4844) 在以太坊上实现，区块中将包含对不透明的二进制大对象（Binary Large Object，blob）数据的承诺，以及用户交易的有序列表。

除了特殊的创世区块外，每个区块都建立在父区块之上，并指向父区块。这样，我们就得到了一个由区块组成的链条：区块链。无论区块的内容如何，协议的目标都是让网络上的所有节点对区块链的历史达成一致。

<a id="img_consensus_block_chain"></a>
<figure class="diagram" style="width: 90%">

![A picture of a linear chain of blocks.](images/diagrams/consensus-block_chain.svg)

<figcaption>

一条区块链。时间自左向右流动，除创世区块外，每个区块都指向它的父母区块。

</figcaption>
</figure>

当节点将它们的区块添加到链顶端时，链就会增长。这是通过临时选择一个“领导者”来实现的，“领导者”是有权扩展链的单个节点。在工作量证明中，领导者是首先为其区块解决工作量证明难题的矿工。在以太坊的权益证明中，“领导者”是从活跃的质押者池中随机选出的。

领导者（通常称为区块提议者）向链上添加一个单独的区块，并全权负责选择和排列该区块的内容，但其区块必须符合协议规则，否则网络的其他部分将直接忽略它。

使用区块是一种优化。原则上，我们可以将单个交易逐个添加到链上，但这会增加巨大的共识开销。因此，区块是成批的交易，有时人们会[争论](https://www.bitrawr.com/bitcoin-block-size-debate-explained)这些区块应该有多大。在比特币中，区块大小受区块中数据字节数的限制。在以太坊的执行层链中，区块大小受区块的燃料限制（gas limit，即运行区块中的交易所需的工作量）。[信标区块](/part3/containers/blocks/#beaconblockbody)大小由硬编码的常数限制。

#### 区块树

我们最初绘制的整洁线性链条大多数情况下可以反映实践中的情况，但不总是如此。有时，可能由于网络延迟、不诚实的区块提议者或客户端错误，任何特定节点都可能会看到类似下面的情况。

<a id="img_consensus_block_tree"></a>
<figure class="diagram" style="width: 90%">

![A diagram of a block tree.](images/diagrams/consensus-block_tree.svg)

<figcaption>

一般来说，我们可能最终得到一棵区块树而不是区块链。时间同样是从左向右流动，每个区块都指向它的父母块。

</figcaption>
</figure>

在实际的网络中，我们可能会得到更像区块树而不是区块链的东西。在这个例子中，很少有区块建立在“明显”的父母区块上。

为什么区块 $C$ 的提议者接在 $A$ 区块后，而非 $B$ 的后面?

  - 可能是 $C$ 的提议者在准备好做出提议时还没有接收到区块 $B$。
  - 可能是 $C$ 的提议者特意要将 $B$ 从自己从自己的链中排除出去，例如为了窃取 $B$ 中的交易，或审查 $B$ 中的某些交易。
  - 可能是由于某些原因，$C$ 的提议者认为 $B$ 区块无效。

对于更广泛的网络来说，至少前两个原因是无法区分的。我们只知道 C 建立在 A 之上，但永远无法确定为什么。

同样，为什么区块 D 的提议者构建在区块 B 之上，而不是 C？上述的原因仍然适用，而且我们还可以添加另一个：

  - D 的提议者可能基于某些理由，认为更广泛的网络最终纳入 B 的可能性大于纳入 C。因此，在 B 之上构建 D 比在 C 之上构建 D 更有机会进入最终的区块链。

区块树中的各种分支被称为“分叉”。在网络和处理延迟的情况下会自然产生分叉。但也可能是由于客户端故障、客户端恶意行为或协议升级改变了规则，使得旧的区块在新规则下失效。后者通常被称为“硬分叉”。

共识协议中分叉的存在是将活性置于安全性之上的结果，这一点在[下文](#safety-and-liveness)中会讨论：如果你去问那些遵循不同分叉的节点，它们会就系统状态给出不同的答案。存在不产生分叉的共识协议，例如古典共识世界中的 [PBFT](https://www.scs.stanford.edu/nyu/03sp/sched/bfs.pdf) 和区块链世界的 [Tendermint](https://blog.cosmos.network/the-4-classes-of-faults-on-mainnet-bfabfbd2726c#a2f1)。这些协议总是产生单一的线性链，因此在形式上是“安全的”。不过，在互联网等异步网络上，它们牺牲了活性：与其分叉，它们彻底停止运转。

#### 分叉选择规则

正如我们所见，由于各种原因——网络延迟、网络中断、消息接收顺序错误、对等节点的恶意行为——网络上的节点最终会对网络状态有不同的视图（view）。最终，我们希望网络上的每个正确节点都能对历史达成一致的线性视图，从而对系统状态形成共同的视图。协议的分叉选择规则（fork choice rule）就是为了达成这种一致。

当给定一个区块树，以及一些基于节点对网络的本地视图的决策标准时，分叉选择规则的设计初衷是从所有可用的分支中，选择最有可能成为最终的线性的规范的链的那一个。也就是说，当节点试图向规范视图靠拢时，它将选择最不可能被修剪出区块树的分支。

<a id="img_consensus_block_tree_resolved"></a>
<figure class="diagram" style="width: 90%">

![A diagram of a block chain as a subset of the block tree.](images/diagrams/consensus-block_tree_resolved.svg)

<figcaption>

分叉选择规则从候选者中选择一个头块。头块标识出一条唯一的、可追溯到创世区块的线性区块链。

</figcaption>
</figure>

分叉选择规则通过选择一个分支顶端的区块（被称为头块）来隐含地选择一个分支。

对于任何正确的节点，任何分叉选择规则的第一标准是：它选择的区块必须是有效的，遵守了协议的规则，并且它的所有祖先也必须是有效的。任何无效的区块都会被忽略，而建立在无效区块上的任何区块自身也是无效的。

鉴于此，有许多不同的关于分叉选择规则的例子。

  - 以太坊和比特币中的工作量证明协议使用“最重链规则”[^fn-no-ghost]（有时称为“最长链”，尽管这并不完全准确）。头块是在工作量证明下完成最多累积“工作”的链的顶端。
  - 以太坊的权益证明 Casper FFG 协议中的分叉选择规则是“跟随包含最高的合理检查点的链”，并且永远不会回滚一个已经最终确定的区块。
  - 以太坊的权益证明 LMD GHOST 协议中的分叉选择规则在其名称中有所体现：采用“最贪婪、最重的被观察子树”。它涉及去计算验证者对区块及其后代区块的累积投票。它也适用与 Casper FFG 相同的规则。

我们将在它们各自的章节中详细解释第二和第三个例子。

你可能已经看出来了，这些分叉选择规则都是为一个区块分配分数的方法。获胜的区块——头块——有最高的分数。背后的想法是，当所有正确的节点最终看到某个区块时，将毫不含糊地认同它是头块，并选择跟随其分支，无论自己对网络有什么其它视图。因此，所有正确的节点最终都会就一个被追溯到创世纪的单一规范链的共同视图达成一致。

[^fn-no-ghost]: 与流行的看法相反，以太坊的工作量证明协议[并没有](https://ethereum.stackexchange.com/a/50693)在其分叉选择中使用任何形式的 GHOST。这是个非常顽固的误解，可能是因为[以太坊白皮书](https://ethereum.org/en/whitepaper/#modified-ghost-implementation)的缘故。我最终询问了 Vitalik，他向我确认，尽管曾计划在 PoW 中使用 GHOST，但由于担心一些未被指明的攻击，它从未被实施。最重链规则更简单，也经过良好测试。它干的很棒。

#### 重组（Reorgs）和回滚（reversions）

当一个节点接收到新的区块（和在权益证明中对区块的新投票）时，它将根据新信息重新评估分叉选择规则。最常见的情况是，新区块将是节点当前视为头块的区块的子区块，并将成为头块。

然而，有时新区块可能是区块树中某个其他区块的后代。（请注意，如果节点还没有新区块的父区块，它需要询问其对等节点以获得父区块，对于它知道自己所缺失的任何其它区块也是如此。）

无论在何种情况下，在更新的区块树上运行分叉选择规则可能会指向一个与先前头块不同分支上的头块。当这种情况发生时，节点必须执行重组（reorg，是 reorganisation 的简写），这也被称为回滚。它将踢出（回滚）之前已经包含在其链中的区块，并将采用新的头块分支上的区块。

在以下图表中，节点评估区块 F 为头块，因此其链由区块 A, B, D, E, F 组成。节点知道区块 C，但它不在节点的链的视图中；区块 C 在一个侧分支上。

<a id="img_consensus_reversion_0"></a>
<figure class="diagram" style="width: 70%">

![A diagram of a blockchain prior to a reversion.](images/diagrams/consensus-reversion-0.svg)

<figcaption>

此时，节点认为区块 $F$ 最可能是头块，因此其链是区块 $[A \leftarrow B \leftarrow D \leftarrow E \leftarrow F]$.

</figcaption>
</figure>

一段时间后，节点接收到区块 G，它不是建立在节点当前的头块 F 上 ，而是建立在区块 C 的分支上。根据分叉选择规则的细节，节点可能仍然评估 F 为比 G 更好的头块，因此忽略 G。但在当前情形中，我们将假设分叉选择规则表明 G 是更好的头块。

区块 D, E, 和 F 不是区块 G 的祖先，所以它们需要从节点的规范链中移除。这些区块包含的任何交易或信息都将被回滚，就好像它们从未被接收过一样。节点必须完全倒回处理区块 B 之后的状态。

倒回区块 B 后，节点可以将区块 C 和 G 和添加到其链中并相应地处理它们。一旦结束，节点就完成了其链的重组。

<a id="img_consensus_reversion_1"></a>
<figure class="diagram" style="width: 70%">

![A diagram of a blockchain after a reversion.](images/diagrams/consensus-reversion-1.svg)

<figcaption>

现在节点认为区块 $G$ 是头块的最佳选择，因此其链必须更改为区块 $[A \leftarrow B \leftarrow C \leftarrow G]$.

</figcaption>
</figure>

稍后，可能会出现一个建立在区块 F 之上的区块 H。如果分叉选择规则表明新的头块应当是 H，那么节点将再次执行重组，回滚区块到 B，并重新建立分支 H 上的区块。

在工作量证明和以太坊的权益证明协议中，由于区块传播的网络延迟，短暂的一两个区块的重组并不罕见。除非链遭到攻击、分叉选择规则的制定或分叉选择规则客户端的实现存在漏洞，否则非常长的重组应该极为罕见。

#### 安全性与活性

在讨论共识机制时，经常蹦出来两个重要概念：安全性（safety）和活性（liveness）。

##### 安全性

非正式地说，如果“没有坏事发生”[^fn-safety-liveness]，某算法就被认为安全。

[^fn-safety-liveness]: 那个我所引用的关于安全性和活性的有用且直观的定义，最早以简短的形式出现在 Lamport 1977年的论文《多进程程序正确性的证明》（[Proving the Correctness of Multiprocess Programs](https://lamport.azurewebsites.net/pubs/proving.pdf)）中，Gilbert 和 Lynch 在 2012 年的论文《关于 CAP 定理的观点》（[Perspectives on the CAP Theorem](https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf)）中也这样定义它们。

在区块链环境中可能发生的坏事情的例子包括加密货币的双花（double-spend），或者两个彼此冲突的检查点的最终确定。

分布式系统中安全性的一个重要方面是“一致性”。也就是说，如果我们询问不同的（诚实的）节点在链的某个进展点上的状态，例如在特定区块高度时某个账户的余额，那么无论我们询问哪个节点，我们应该总是得到相同的答案。在一个安全的系统中，每个节点对链的历史都有着永不改变的相同视图。

实际上，安全性意味着我们的分布式系统“表现得像一个中心化实例，一次只执行一个原子化的操作。”（[引用 Castro 和 Liskov](https://www.scs.stanford.edu/nyu/03sp/sched/bfs.pdf)）。在 Vitalik 的[去中心化分类中](https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274)，一个安全的系统是在逻辑上是中心化的。

##### 活性

再次非正式地说，如果“最终会发生一些好事”，那么某种算法就被认为具有活性。

在区块链环境中，我们通常认为这意味着链总是可以添加一个新区块；它永远不会陷入无法产生包含交易的新区块的僵局。

“可用性（Availability）”是看待这个问题的另一种方式。我希望链是可用的，这意味着如果我向一个诚实的节点发送一个有效的交易，它最终会被包含在一个扩展了这条链的区块中。

##### 二者不可兼得！

CAP 定理是分布式系统理论中的一个著名结果，它指出没有分布式系统可以同时提供（1）一致性（consistency），（2）可用性，和（3）分区容错性（partition tolerance）。分区容错性是指节点之间的通信不可靠时仍能正常运行的能力。例如，网络故障可能将节点分成两个或多个无法相互通信的群组。

在区块链的语境中很容易证明 CAP 定理。假如亚马逊网络服务（AWS）下线，使得所有由 AWS 托管的节点可以相互通信，但没有一个节点可以与外界通信；或者一个国家阻止所有进出的连接，使得没有任何流言（gossip）流量可以通过。这两种情况都将节点分成两个不相干的组，如 A 与 B。

<a id="img_consensus_partition"></a>
<figure class="diagram" style="width: 50%">

![A diagram of a network partition.](images/diagrams/consensus-partition.svg)

<figcaption>

网络被分区：$A$ 中的节点可以彼此沟通，但它们不能与 $B$ 中的任何节点对话，反之亦然。

</figcaption>
</figure>

假设连接到 A 组网络的某账户发送了一个交易。如果 A 组中的节点处理了该交易，那么他们最终的状态就与 B 组中的没有看到该交易的节点们不同。因此，总的来说，我们失去了所有节点之间的一致性，因此失去了安全性。避免这种情况的唯一方法是 A 组中的节点拒绝处理交易，在这种情况下我们失去了可用性，以及活性。

总之，CAP 定理意味着，我们无法指望设计出一个在任何情况下都既安全又具有活性的共识协议，因为我们别无选择，只能在一个不可靠的网络上运行，即互联网。[^fn-flp-theorem]

[^fn-flp-theorem]: CAP 定理与 Fisher, Lynch 和 Paterson 在 1985 年的论文《故障进程中分布式共识的不可能性》（[Impossibility of Distributed Consensus with One Faulty Process](https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf)）中描述的另一个著名结果有关，通常称为 FLP 定理。这证明了，即使在一个可靠的异步网络中（即，消息传递所需时间没有限制），只要有一个故障节点，就可以阻止系统达成共识。也就是说，即使这个未分区的系统也不能同时具备活性和安全性。Gilbert 和 Lynch 在[一篇论文](https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf)的第 3.2 节讨论了 FLP 定理。

##### 以太坊优先考虑活性

在网络状况良好的情况下，以太坊共识协议可同时提供安全性和活性，但在网络运行不太顺畅时，则优先考虑活性。在网络分区的情况下，分区两侧的节点将继续产生区块。不过，最终确定性（finality，安全性的一种属性）将不再在分区两侧一起发生。根据两侧管理的质押比例，要么一侧继续获得最终确定性，要么两侧都不会继续获得最终确定性。

最终，除非分区得到解决，否则双方都会因新颖的怠惰惩罚（[inactivity leak](/part2/incentives/inactivity/)）机制而重新获得最终确定性。但这也最终会导致安全故障。每条链将最终确定不同的历史，两条链将永远变得不可调和与独立。

#### 最终确定性（Finality）

我们将在接下来的章节中大量讨论最终确定性，这是链的安全性的属性。

最终确定性是指有些区块永远不会被回滚。当一个区块被最终确定时，网络上的所有诚实节点都同意该区块将永远保留在链的历史中，因此它的所有祖先也将保留在链的历史中。最终确定性让你对比萨饼的支付不可撤销，就像用现金一样。这是对双花的终极保护。[^fn-finality-not-absolute]

[^fn-finality-not-absolute]: 值得注意的是，最终确定性从来都不是绝对的。无论任何协议怎样声称，如果绝对多数节点同意（例如通过软件升级）回滚一堆已经最终确定的区块，那么这就会发生。归根结底，就像所有事物一样，最终确定性的概念服从于社会共识。有关进一步讨论，请参见《关于结算的最终确定性》（[On Settlement Finality](https://blog.ethereum.org/2016/05/09/on-settlement-finality)）。

一些共识协议，如经典的 PBFT 或 Tendermint，每轮（每个区块）都会最终确定。一旦一轮交易被包含在链上，所有节点都同意它将永远存在。一方面，这些协议非常“安全”：一旦交易被包含在链上，它将永远不会被回滚。另一方面，它们容易发生活性故障：如果节点无法达成一致——例如，如果超过三分之一的节点关闭或不可用——那么就没有交易可以被添加到链上，链将停止运行。

其他共识协议，如比特币的中本聪共识，根本没有任何最终确定性机制。总是存在有人呈现出一个更重的替代链的可能性。当这种情况发生时，所有诚实的节点必须相应地重组他们的链，回滚他们之前处理的任何交易。诸如你的区块有多少确认之类的启发式方法只是对最终确定性的近似值，而无法保证。[^fn-cdc-40k]

[^fn-cdc-40k]: 在撰写本文时，至少有一个交易所要求从以太坊经典网络的存款中获得 [40000 个确认](https://www.reddit.com/r/Crypto_com/comments/w9qmbx/40000_confirmations_and_7_days_to_send_etc_to_cdc/)。这意味着在包含存款交易的区块之上必须建立四万个区块，交易所才会处理它，这大约需要六天时间。这一要求反映了对 ETC低哈希率工作量证明链易受 51% 攻击的脆弱性的担忧——对于攻击者来说，随意回滚区块相对容易。事实上，面对一个精心设计的 51% 攻击，无论多少确认次数都无法达到真正安全。

以太坊的共识层优先考虑活性，但也努力在有利的情况下以最终确定性的方式提供安全保证。这是试图两全其美的尝试。Vitalik 这样[辩护](https://ethresear.ch/t/explaining-the-liveness-guarantee/4228/8?u=benjaminion)：[^fn-liveness-during-nonfinality]

> 一般原则是你想给予用户“尽可能多的共识”：如果达成共识的节点 $>2/3$ 那么我们就会得到常规的共识。但如果 $<2/3$，那也不需要停下来什么也不做，显然，尽管新区块的安全性暂时降低，链仍然可能继续增长。如果个别应用不满意较低的安全级别，它可以自由地忽略那些区块，直到它们被最终确定。

[^fn-liveness-during-nonfinality]: 这一点的价值在 2023 年 5 月 12 日信标链在大约一个小时中停止最终确定（[stopped finalising](https://offchain.medium.com/post-mortem-report-ethereum-mainnet-finality-05-11-2023-95e271dfd8b2)）时显现。在此期间，参与共识的验证者从超过99% 下降到大约 40%。然而，普通的以太坊用户和应用程序几乎没有察觉。区块继续被生产（尽管少于正常情况），交易继续被执行。

在以太坊的共识层中，最终确定性是由 Casper FFG 机制提供的，我们很快就会探讨这一机制。其原理是，所有诚实的验证者定期就最近的检查点区块达成一致，他们永远不会撤销这些区块。然后，该区块及其所有祖先区块就是“最终确定的”区块——它们永远不会改变，如果你向网络中的任何诚实节点询问它们或其祖先区块的情况，你总会得到相同的答案。

<a id="img_consensus_finality"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing a finalised portion of chain and a forkful portion.](images/diagrams/consensus-finality.svg)

<figcaption>

诚实的节点已经同意检查点及其所有祖先区块是“最终确定的”且永远不会被回滚。因此，在检查点之前没有分叉。检查点之后的链仍然可能发生分叉。

</figcaption>
</figure>

以太坊的最终确定性是“经济最终确定性”。理论上，协议可能会最终确定两个相冲突的检查点，即对链的历史的两个矛盾视图。然而，这只有在巨大且可量化的成本下才可能出现。除了最极端的攻击或失败情景外，最终确定就是最终确定。

[Casper FFG 部分](/part2/consensus/casper_ffg/)深入探讨了这种最终确定性机制的工作原理。

#### 另见

Leslie Lamport 参与的任何内容总是值得一读，他与 Shostak 和 Pease 合著的 1982 年原始论文《拜占庭将军问题》（[The Byzantine Generals Problem](https://lamport.azurewebsites.net/pubs/byz.pdf)）包含了许多洞见。虽然他们提出的算法在当今条件下已经效率极低，但该论文对推理一般共识协议是很好的引入。Castro 和 Liskov 在1999 年发表的开创性论文《实用拜占庭容错》（[Practical Byzantine Fault Tolerance](https://www.scs.stanford.edu/nyu/03sp/sched/bfs.pdf)）也是如此，它对以太坊的 Casper FFG 协议的设计产生了重大影响。但是，你可能会想将这些“经典”方法与中本聪在 2008 年[比特币白皮书](https://bitcoinpaper.org/bitcoin.pdf)中描述的工作证明优雅的简洁性相对比。如果说工作量证明有什么优点的话，那就是它的简洁。

上文中我们提到了 Gilbert 和 Lynch 在 2012 年的论文《CAP 定理的视角》（[Perspectives on the CAP Theorem](https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf)）。这篇论文对一致性和可用性（或我们语境中的安全性和有效性）概念进行了深入探讨，有很强的可读性。

由于分叉选择规则的客户端实现之间存在差异，以太坊信标链在 2022 年 5 月经历了七个区块的重组。这些差异在当时是众所周知的，并且被认为是无害的。事实证明并非如此。巴纳贝-蒙诺（Barnabé Monnot）对这一事件的[描述](https://barnabe.substack.com/p/pos-ethereum-reorg)非常有启发性。

Vitalik 的博客文章《关于结算的最终确定性》（[On Settlement Finality](https://blog.ethereum.org/2016/05/09/on-settlement-finality/)）提供了对最终确定性概念更深入、更细致的探索。

对于我们正在构建的系统，我们的理想是它们是政治去中心化的（以实现无许可和抗审查），架构去中心化的（以实现无单点故障的抗逆力），但在逻辑上是中心化的（以实现一致的结果）。这些标准对我们如何设计共识协议有很大影响。Vitalik 在《去中心化的意义》（[The Meaning of Decentralization](https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274)）一文中探讨了这些问题。

### 概述 <!-- /part2/consensus/overview/ -->

<div class="summary">

  - 节点和验证者是共识系统中的行动者。
  - 时隙和时段调节共识时间。
  - 区块和认证是共识的货币。
  - 以太坊的共识协议结合了两个独立的共识协议。
  - “LMD GHOST”本质上提供活性。
  - “Casper FFG”提供最终确定性。
  - 它们有时被合称为“Gasper”。

</div>

#### 引言

上一节描述了区块链共识的大致情况；在本节中，我们将聚焦于以太坊的权益证明共识。我试图遵循一条通过提供足够的信息以理解事物的路径，而不深入两侧的技术细节的杂草。所有这些细节都将在[规范注解](/part3/)和其他章节中被深入探讨。我也加了一些链接，以供那些想要去分支探索的人使用。

首先必须介绍的是我们将在整个过程中使用的、以太坊特有的术语。

##### 节点和验证者

以太坊网络的主要参与者是节点（nodes）。节点的角色是验证共识并与其他节点形成通信的主干。

验证者（validators）形成共识，而“验证者”（符合以太坊的一致作风）是一种可怕的误称，因为它们实际上并不验证任何东西——验证是由节点完成的。每个验证者代表最初质押的 32 个以太币。它有自己的秘钥（[secret key](/part2/building_blocks/signatures/#key-pairs)），以及作为其在协议中身份的相关公钥。验证者附在节点上，一个节点可以托管从零到数百数千个验证者。附到同一个节点的验证者们不会独立行动，它们共享着对世界的同一视图。[^fn-validators-nodes]

[^fn-validators-nodes]: 当我们谈论到以太坊的去中心化时，牢记这一点是非常有益的。比如说，网络上有 60 万个活跃的验证者，这与有 60 万个独立行动者相比还相去甚远。观察节点的数量以及验证者在节点间的分布，将为以太坊的去中心化提供更有用的衡量指标。

使权益证明不同于工作量证明的一个有趣特点是，在权益证明中，我们知道我们的验证者集（validator set）。我们有一份完整的列表，上面有我们预计会在任意时刻活跃的所有公钥。知道验证者集使我们能够实现最终确定性，因为我们可以识别出在何时获得了参与者的多数票。[^fn-accountable-safety-jargon]

[^fn-accountable-safety-jargon]: 在共识术语中，我们可以拥有“可问责的安全性（accountable safety）”。

##### 时隙和时段

在以太坊的权益证明共识中，时间被严格规定，这与工作量证明相比是一个重大变化，工作量证明只与时间有着偶然的关系——它试图保持区块间隔的平均恒定，但仅此而已。

最重要的两种时间间隔是时隙（slot，[12 秒](/part3/config/configuration/#seconds_per_slot)）和时段（epoch，[32 个时隙](/part3/config/preset/#slots_per_epoch)或 6.4 分钟）。无论网络上发生了什么，时隙和时段都会有规律地持续进行。

##### 区块和认证

每个时隙中都会[选择](/part3/helper/accessors/#get_beacon_proposer_index)出一个验证者去提议一个区块。区块[包含](/part3/containers/blocks/#beaconblockbody)对信标状态的更新，这些更新包括提议者所知道的认证，还有以太坊用户交易的执行有效载荷（[execution payload](/part3/containers/execution/#executionpayload)）。通过广播协议（gossip protocol），提议者与整个网络共享其区块。

时隙可以是空的：区块提议者可能离线，或提出一个无效的区块，或其区块随后被重组出链。在一条运行良好的信标链中，这些事情不应该经常发生，但协议有意在空时隙出现时保持稳健。

在每个时段中，每个验证者都会通过认证（attestation）的形式分享一次它对世界的视图。认证[包含](/part3/containers/dependencies/#attestationdata)对链头的投票（LMD GHOST 协议将会使用）和对检查点的投票（Casper FFG 协议将会使用）。认证也会向整个网络广播。与区块一样，认证也可能因各种原因而缺失，而协议可以在不同程度上容忍这种情况——粗略地说，随着认证者参与率的降低，共识的质量也会下降。[^fn-attestation-rate]

[^fn-attestation-rate]: [Beaconcha.in](https://beaconcha.in) 网站按时段显示认证参与率（也被称为投票参与）。这是衡量网络健康的一个好方法。该比率通常超过 99%，对于一个大规模分布式共识协议来说，这是非常出色的水准。

时隙的功能是将处理所有这些验证的工作量分散化。通过认证，每个验证者都会将自己对世界的视图告知其他验证者，如果一次性完成，可能会导致巨大的网络流量和处理负载。将认证的工作量分散到一个时段中的所有 32 个时段，可以保持低资源使用率。在每个时段，委员会只由占总量 $\frac{1}{32}$ 的进行认证的验证者们组成。

通过对验证者的奖惩系统，协议激励区块和认证的生产和准确性。我们当下不需要深入探讨这些；会有完整[单独的一章](/part2/incentives/)来阐述所有这些问题。

##### 罚没

在工作量证明中，生产一个区块的成本很高。这极大地激励了矿工，使他们按照协议的目标正确行事，以确保自己的区块被纳入。

在权益证明中，创建区块和证明几乎是免费的 [^fn-nothing-at-stake]。我们需要一种方法来防止攻击者利用这一点破坏网络。这就是罚没（slashing）的作用。对区块或认证模棱两可的验证者将被[罚没](/part2/incentives/slashing/)，这意味着他们会被移除出验证者集，并被处以部分质押的罚款。简单地说，模棱两可（equivocation）就是说出两件相互矛盾的事情。它可能是为同一个时隙提出两个不同的区块，或者做出两个相互矛盾的认证，而任何诚实遵守协议的验证者都不会如此行事。

[^fn-nothing-at-stake]: 这有时被称为“无利害关系问题（nothing at stake problem）”。

#### 机器中的幽灵

了解了一些术语之后，让我们开始概述以太坊实际的共识机制。

以太坊的权益证明共识协议实际上是两个独立共识协议的结合，它们分别被称为 LMD GHOST[^fn-lmd-name] 和 Casper FFG[^fn-ffg-name]。这两个协议已经被“拧和在一起”，形成了我们为 Eth2 实现的共识协议——这个组合协议有时被称为 “Gasper”。

[^fn-lmd-name]: “最新消息驱动的，贪婪、最重的被观察子树”。我将在具体的 [LMD GHOST 章节](/part2/consensus/lmd_ghost/#naming)中解释这个命名。

[^fn-ffg-name]: “友好的最终确定性小工具 Casper”。同样，我将在进入特定的 [Casper FFG 章节](/part2/consensus/casper_ffg/#naming)时解释这个略显奇怪的命名。

在 Gasper 中将这两者结合是为了在活性和安全性两个方面都获得最佳效果。本质上，LMD GHOST 为每个时隙依次提供活性（它让链保持运行），而 Casper FFG 提供安全性（它保护链免受大规模回滚）。LMD GHOST 使我们继续在彼此之上产出区块，但是这种产出是可分叉的，因此从理论上说并不安全。Casper FFG 修改 LMD GHOST 的分叉选择规则，定期赋予链最终确定性。尽管如此，[如前所述](/part2/consensus/preliminaries/#ethereum-prioritises-liveness)，以太坊优先考虑活性。因此，在 Casper FFG 无法赋予最终确定性的情况下，链仍然会通过 LMD GHOST 机制继续增长。

这种拧和在一起的共识机制并不总是那么好。有时我们会看到接合处，两种共识机制之间的互动导致了一些我们[稍后](/part2/consensus/issues/)将讨论的微妙问题。然而，本着以太坊的精神，它是一个实用的工程解决方案，在实践中运行良好。

##### 历史

Gasper 的详细历史与其组件 LMD GHOST 和 Casper FFG 的发展密切相关，我们将在它们各自的部分中进行回顾。但我们要在这里指出，Casper FFG 从未被设计为一个独立的共识机制。

正如 [Casper FFG 论文](https://arxiv.org/abs/1710.09437)中所述：

> 友好的最终确定性小工具 Casper 是一个建立在提议机制（proposal mechanism，提议区块的机制）之上的覆盖层。

因此，有一个底层区块提议机制——这意味着有一个提供某种元共识的底层共识机制，为区块链赋予最终确定性——而 Casper FFG 建立于其上。

最初的计划是将 Casper FFG 作为权益证明覆盖层，叠加在以太坊的工作量证明共识之上。Casper FFG 将在链上定期赋予（如，每 100 个区块）最终确定性——这是工作量证明链所缺乏的属性。这被视作是让以太坊摆脱工作量证明的第一步。有了最终确定性的保证，我们可以减少工作量证明的区块奖励，从而减少总的哈希算力。这是一个过渡步骤，在未来某个日期最终会用完全的权益证明替代挖矿。

到 2017 年底，这个计划已经相当成熟。[EIP-1011](https://eips.ethereum.org/EIPS/eip-1011), 混合 Casper FFG (Hybrid Casper FFG)，详细描述了该架构，且在 2017 年 12 月 31 日甚至有一个[测试网](https://hackmd.io/@aTTDQ4GiRVyyce6trnsfpg/Hk6UiFU7z?type=view)[上线](https://web.archive.org/web/20230630135033/https://nitter.it/karl_dot_tech/status/947503029166546946)。

然而，在 2018 年初，该计划被取代。以太坊虚拟机的有限带宽限制了EIP-1011 能支持的验证者集的大小，进而导致最低质押量为 1500 个以太币，这被视作是不可取的。大约在同一时间，通向一个完整的、更具可扩展性的权益证明协议的路径变得更加清晰，我们开始着手设计，也就是后来的以太坊 2.0。

由于其通用性，Casper FFG 能够在重新设计中幸存下来，并被以太坊 2.0 采用——不是作为工作量证明而是一个新的权益证明协议 LMD GHOST 的覆盖层。

##### 最终确定性小工具

当说到 Casper FFG 覆盖了现有的区块提议机制时，我们的意思是它采用了一个现有的区块树并以特定方式对其进行了修剪。通过使底层区块树的某些分支无法访问，Casper FFG 修改了这些区块树的分叉选择。

请将这个区块树看作是由某种底层共识机制产生的，无论是工作量证明还是权益证明中的 LMD GHOST。

<a id="img_gasper_blocktree"></a>
<figure class="diagram" style="width: 70%">

![A Diagram of a block tree with three forks.](images/diagrams/gasper-blocktree.svg)

<figcaption>

有三个分叉（分支）的任意区块树。区块 $I$, $E$, $M$ 中的任意一个都可能是区块顶端（这些区块标识的选择是为了方便，而非暗示某种特定的排序）。

</figcaption>
</figure>

在上图情形中，我有三个候选的头块：$I$, $E$, $M$。在工作量证明的最长链规则下，头块的选择显而易见：我必须选择 $M$，因为它具有最大的区块高度，或者（几乎）等同于完成了做多的工作量。在 LMD GHOST 下，我们不能仅凭这一信息去选择头块，还需要看到其他验证者的投票才能做出选择。

挑战在于，从区块 $J$ 到 $M$ 的链可能来自于攻击者。攻击者可能秘密挖掘了该链，然后在所谓的 51% 攻击中使用它。工作量证明节点别无选择，只能进行重组以使 $M$ 成为头部，从而偏向攻击者的链，可能容易受到双花攻击。

Casper FFG 的价值在于它赋予了最终确定性。假设区块 $D$ 被 Casper FFG 标记为是最终的（这会自动最终确定区块 $A$、$B$ 和 $C$）。最终确定性会修改底层协议的分叉选择规则，以排除任何与区块 $D$ 竞争的分支——即任何不是由 $D$ 派生的区块。换句话说，分支会被修剪，以确保在最终确定的区块之前没有分叉。

<a id="img_gasper_blocktree_finalised"></a>
<figure class="diagram" style="width: 70%">

![A diagram of the same block tree after a block on one of the branches has been finalised.](images/diagrams/gasper-blocktree_finalised.svg)

<figcaption>

这里的区块树和上一幅图片一样。但现在区块 $D$ 已经获得最终确定性。Casper FFG 分叉选择表明任何不包括区块 $D$ 的链都应该被忽略。毫无疑问，我们的头块现在是 $E$。

</figcaption>
</figure>

当区块 $D$ 被最终确定，分叉选择必须忽略以区块 $F$ 和区块 $J$ 打头的分支。我们最后只保留了一个候选头块：$E$。

从根本上说，Casper FFG 所提供的最终确定性可以防止长时间的重组（回滚）。任何已获得最终确定性的区块或已获得最终确定性的区块的祖先都永远不会被回滚。在以太坊的 Casper FFG 实现中，我们必须对”永远”加以限定，即“不会烧掉至少 1/3 的全部以太币”的前提下。这就是权益证明链提供的经济最终确定性。

#### 结论

As a reminder, [this](/part2/consensus/) is the sentence we are trying to understand in all its parts.回过头来，[这](/part2/consensus/)是那句我们尝试理解其所有部分的话。

> 以太坊的权益证明（PoS）共识协议是通过在分叉选择规则 LMD GHOST 之上应用最终确定性小工具 Casper FFG 来构建的，LMD GHOST 是贪婪、最重的被观察子树（Greedy Heaviest-Observed Sub-Tree, GHOST）规则的一种变体，它只考虑每个参与者的最近一次投票（最新消息驱动，Latest Message Driven, LMD）。

我们花了一些时间来讨论什么是共识协议，以及它做了什么，并简单了解了权益证明。我们讨论了最终确定性，并在较高层面上说明了 Casper FFG 如何形成了一个“最终确定性小工具”，被应用于作为区块提议机制的 LMD GHOST 之上。

但还有很多工作要做。在接下来的章节中，我们将更深入地探讨 [LMD GHOST](/part2/consensus/lmd_ghost/), [Casper FFG](/part2/consensus/casper_ffg/)，以及它们如何结合而成 [Gasper](/part2/consensus/gasper/) 协议。

#### 另见

关于这一切是如何汇聚在一起的历史，Vitalik 发起过一场精彩的[推文风暴](https://web.archive.org/web/20230630135150/https://nitter.it/VitalikButerin/status/1029900695925706753)。可以在[这里](https://www.trustnodes.com/2018/08/16/vitalik-buterin-tells-story-race-vlad-zamfir-implement-proof-stake-casper)和[这里](https://hackmd.io/@liangcc/BJZDR1mIX?type=view)找到整理后的版本。他稍微讨论了一下弱主观性，我们[后面](/part2/validator/weak_subjectivity/)会处理这个问题。

《权益证明常见问题》（[Proof of Stake FAQ](https://vitalik.ca/general/2017/12/31/pos_faq.html)）依然是我们将要讨论的许多话题的极好入门读物。

在 Devconnect 2022 的 ETHconomics 上，Joachim Neu 的演讲《以太坊权益证明的共识问题的缘由与解决方案》（[The Why and How of PoS Ethereum's Consensus Problem](https://www.youtube.com/watch?v=2nMS-TK_tMw)）对于可用性与最终确定性的权衡以及以太坊如何管理这一问题提供了非常易懂的见解。当我们讨论到 [Gasper 协议](/part2/consensus/gasper/)时，我们将再次提到“嵌套账本”的概念。

### LMD Ghost <!-- /part2/consensus/lmd_ghost/ -->

<div class="summary">

  - LMD GHOST 是一种分叉选择规则，被节点用来确定最佳的链。
  - 它根据所有活跃验证者的投票而赋予链的不同分支以权重。
  - LMD GHOST 不提供最终确定性，但的确支持确认规则（confirmation rule）。
  - 罚没被用来解决“无利害关系”问题。

</div>

#### 介绍

在本节中，我们将单独探讨 LMD GHOST，完全忽略 Casper FFG 的最终确定性覆盖层[^fn-casper-and-gasper-next]。LMD GHOST 自身就是一个共识机制的本质——它是一个分叉选择规则，就像中本聪共识中的最重链规则一样——并且有其自身的属性和权衡。

[^fn-casper-and-gasper-next]: The next section covers [Casper FFG](/part2/consensus/casper_ffg/), and the one after that the combination of the two into [Gasper](/part2/consensus/gasper/).

当前，我们只考虑故事中“它如何运作”这个部分——畅通无阻的流程。在后面的“议题和修复（[Issues and Fixes](/part2/consensus/issues/)）”部分，我们会去探索“它如何出错”的那部分。

#### 命名

LMD GHOST 由两个缩写词组成，分别是”最新信息驱动的（Latest Message Driven, LMD），和”贪婪的、最重的可被观察子树（Greedy Heaviest-Observed Sub-Tree, GHOST）。我们会先打开 GHOST，然后是 LMD。

##### GHOST

GHOST 协议源自 [Sompolinsky 和 Zohar 在 2013 年的一篇论文](https://eprint.iacr.org/2013/881)，论文讨论了如何在比特币区块链安全地提高交易吞吐量。增加区块大小或减少区块之间的间隔，会使链更容易在无法控制延迟的网络（如互联网）中分叉。分叉的链会出现更多的重构，而重构不利于交易的安全性。用 GHOST 分叉选择规则替换比特币的最长链分叉选择规则，被证明在存在延迟的情况下会更加稳定，允许更频繁的区块生产。

GHOST 全称是“贪婪的、最重的可被观察子树”，这描述了 GHOST 算法的工作方式。我们将在[下面](#finding-the-head-block)展开讨论这一点。简而言之，GHOST 的分叉选择不是遵循最重的链，而是遵循最重的子树。它认识到对某个区块的投票不仅仅是针对该区块自身，也隐含着对该区块每一个祖先的投票，因此整个子树都获得相关的权重。

比特币从未采用 GHOST。和论文中声称的不同，工作量证明下的以太坊也没有采用这一协议。尽管以太坊[最初](https://ethereum.org/en/whitepaper/#modified-ghost-implementation)计划采用 GHOST，且曾经的工作量证明中的“叔块（uncle）”奖励与此相关。

##### LMD

我们在以太坊的权益证明中使用的 GHOST 协议已经扩展，能够处理认证。在工作量证明中，投票者是区块提议者。他们通过将自己的区块构建在某个特定分支上来为其投票。在我们的权益证明协议中，所有验证者都是投票者。通过发布认证，每个验证者平均每 6.4 分钟为其对网络的视图投票一次。因此，在权益证明下，我们可以获得更多关于参与者对网络的视图的信息。

这就是所谓的“消息驱动”，我们有了 LMD 中的 MD。分叉选择不是由提议者添加的区块驱动的，而是由所有验证者发布的消息（认证、投票）驱动的。

这里的 ”L“ 代表”最新（latest）“：LMD GHOST 只考虑每个验证者的最新信息，即我们从该验证者收到的最新认证。验证者之前的所有信息都会被丢弃，但其最新的投票被保留，并且无限期地具有权重。

顺便说一句，还有其他版本的消息驱动的 GHOST 可用。Vitalik [最初倾向](https://web.archive.org/web/20230630135311/https://nitter.it/VitalikButerin/status/1029906757512966144#m)于 IMD GHOST，即”即时消息驱动的（Immediate Message Driven）“  GHOST。据我所知[^fn-imd-tricky]，这会无限期地保留所有认证，且分叉选择是基于当时的最新认证运行的。然后是 [FMD](https://ethresear.ch/t/saving-strategy-and-fmd-ghost/6226?u=benjaminion) GHOST，即“新鲜消息驱动（Fresh Message Driven）”的 GHOST，它只考虑当前时段和上一个时段的认证。还有 [RLMD](https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920?u=benjaminion)，即“近期的最新消息驱动（Recent Latest Message Driven）”的 GHOST，它只记下验证者在某个可参数化的时段内的最新认证。

[^fn-imd-tricky]: I've yet to find a lucid exposition of IMD GHOST. Looking back through the history on the [original mini-spec](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760?u=benjaminion) gives some information, but it's hard to understand what was really happening. It was first known as ["recursive proximity to justification"](https://web.archive.org/web/2/https://nitter.it/VitalikButerin/status/1029906887376961536#m), since it was bound up with Casper FFG in a way that LMD GHOST is not.

#### 它如何运作
LMD GHOST 首先是一个[分叉选择规则](/part2/consensus/preliminaries/#fork-choice-rules)。给定一个区块树和一系列投票，LMD GHOST 会告诉我应该将哪个区块视为最佳头块，从而提供一个从该头块一直到创世区块的线性历史视图。这一决策基于我对链的本地视图，即我的节点收到的消息（区块和认证）——记住，没有“上帝视角”，我的本地视图是我所能利用的全部，它很可能与其他节点的本地视图不同。我们的想法是，诚实的验证者会根据他们看到的最佳头块来构建自己的区块，并将根据他们看到的最佳头块进行投票。

一个好的分叉选择规则将提供以下几点：[^fn-good-fork-choice-rule]

[^fn-good-fork-choice-rule]: I've adapted this from an old post of Vitalik's, [PoS fork choice rule desiderata](https://ethresear.ch/t/pos-fork-choice-rule-desiderata/2636?u=benjaminion). I'm postponing his finality point for now. I am not aware of much formal analysis of LMD GHOST with respect to properties like these; in fact, our implementation of LMD GHOST [may not do too well](https://arxiv.org/pdf/2302.11326.pdf) in some respects. But these goals are worth bearing in mind as we explore how the mechanism works.

  - 多数节点诚实进展：如果超过 50% 的节点按照分叉选择规则构建区块，链就会前进，并且（指数级地）不太可能回滚旧区块。
  - 稳定性：分叉选择可以很好地预测未来的分叉选择。
  - 抗操纵性：即使攻击者在一小部分参与者中成为绝对多数，攻击者也不太可能做到回滚区块。

这几点都是相互关联的。对于区块提议者来说，稳定性尤其重要。当我提出一个区块时，我希望尽可能确定该区块将永远保留在链上。也就是，我希望尽可能确信它不会被重组出链。找头块意味着当我找到它并在其基础上构建新区块时，我找到的头块最有可能使我的构建的新区块成为其他节点的视图中的头块。

我们将一分为二地探讨 LMD GHOST 如何运作。先看 LMD 部分，即最新消息，然后再看 GHOST 部分，即寻找头块。

##### 最新消息

在此处的语境中，“消息”是指在认证中对头块的投票。

###### LMD GHOST 中的投票

在认证的[数据](/part3/containers/dependencies/#attestationdata)中, 头块投票是 `beacon_block_root` 字段:

```python
class AttestationData(Container):
    slot: Slot
    index: CommitteeIndex
    # LMD GHOST vote
    beacon_block_root: Root
    # FFG vote
    source: Checkpoint
    target: Checkpoint
```

每个诚实的验证者在每个时段都恰好进行一次认证，其中包含在进行认证时它对最佳头块的投票。在每个时段内，验证者集被分成 32 份，以使每个时隙中只有 $1/32$ 的验证者进行认证。（该结构中的 `index` 字段与进行认证的验证者有关。出于实施的原因，每个时隙中的验证者被进一步划分为最多 64 个[委员会](/part2/building_blocks/committees/)，但这与 LMD GHOST 的机制无关，我们在此先忽略它。）

节点既可以通过认证广播（attestation gossip）直接接收认证，也可以通过包含在区块中的消息来间接接收认证。原则上，节点可以通过其他方式接收认证——如果愿意，我可以在键盘上输入认证——但在实践中，投票只经由认证广播和区块而得到传播。

###### 存储最新消息

节点接收到一个认证后，无论通过何种方式，都会调用分叉选择的 [`on_attestation()`](/part3/forkchoice/phase0/#on_attestation) 处理器（handler）。`on_attestation()` 处理器会先对认证进行一些基本的有效性检查（[validity checks](/part3/forkchoice/phase0/#validate_on_attestation)）：

  - 它是不是太旧了？
    - 它必须来自当前或上一个时段。参见 [Attestation consideration delay](/part2/consensus/issues/#attestation-consideration-delay)。
  - 它是不是太新了？
    - 它不能晚于上一个时隙。参见 [Attestation recency](/part2/consensus/issues/#attestation-recency)。
  - 我们是否知道它投票支持的区块，即 `beacon_block_root`？
    - 我们必须已经收到那个区块。如果没有，我们可能会尝试从对等节点那里获取它。
  - 它的签名是否正确？
    - 验证者对认证签名并对其负责。
  - 该认证是否是可被罚没的（slashable）？
    - 我们必须忽略与其他认证相冲突的认证。参见 [Attestation equivocation](/part2/consensus/issues/#attestation-equivocation)。

在通过这些和其他一些检查后，认证被考虑插入节点的存储库（Store）中，这是其关于链状态的信息库，即节点的视图。这是在 [`update_latest_messages()`](/part3/forkchoice/phase0/#update_latest_messages) 中完成的。如果我们还没有该验证者的头块投票，那么现在就可以存储。如果我们已经有验证者的头块投票，而当前认证是更近期的，它就会替代原来的投票。

随着时间的推移，节点的存储库会逐渐建立起一个列表，其中包含每个验证者的最新投票。

请注意，只有我们在与投票所完成的同一个时段或在该时段后的时段获取它，投票才能被插入存储库。然而，一旦投票在存储库中，它就会无限期地留在那里，并继续为分叉选择做出贡献，直到它被更近期的投票更新。这是 LMD GHOST 与例如 [Goldfish 协议](https://arxiv.org/pdf/2209.03255.pdf)或 [RLMD GHOST](https://arxiv.org/pdf/2302.11326.pdf) 之间的一个关键区别。

##### 寻找头块

本质上，LMD GHOST 分叉选择规则是一个函数 $\text{GetHead}(\text{Store}) \rightarrow \text{HeadBlock}$。正如我们所见，节点的存储库是其对世界的视图：它所接收到的所有可能影响分叉选择的相关信息。对于我们在这里看到的纯粹 LMD GHOST 算法，存储库（[Store](/part3/forkchoice/phase0/#store)）的相关信息如下。

  - 区块树（它只是一个区块列表）。区块的父母级链接将它们在逻辑上连成一棵树。
  - 来自于验证者的最新消息（投票）列表。
  - 验证者的有效余额（[effective balances](/part2/incentives/balances/)）(基于某种状态)，因其提供了算法中使用的权重。

GHOST 算法的目标是从给定的区块树中选择一个叶子区块，其中叶子是指一个没有任何后代的区块。这将是我们所选择的头块。

我们将假设区块树中的所有区块都源自一个根区块。在纯粹的 GHOST 算法中，那将是创世区块：根据定义，所有区块都必须源自创世。在我们完整的共识实现中，那个根区块将是最后一个被合理化的检查点区块。就我们当前的目的而言，我们只需知道 GHOST 算法从一个给定的区块开始，并忽略所有不是从该区块派生出来的区块。

###### 获取权重

我们首先做的是计算树中每个分支的“权重”。从某种意义上说，一个分支的权重就是它的得分。

投票的权重是进行投票的验证者的[有效余额](/part2/incentives/balances/)。这通常是 32 个以太币，即最大有效余额，但也可能更少。回想一下，投票就是我们从验证者那里得到的最新消息。

一个分支的权重是其根部区块得到的投票权重，加上该块子分支的权重。通过包括子分支的权重，我们承认对一个块的投票也是对该块每个祖先的投票。只由一个叶块组成的分支的权重将仅是该叶块获得票数的权重。

<a id="img_annotated_forkchoice_get_weight_0"></a>
<figure class="diagram" style="width: 90%">

![Diagram of a block tree with branch weights and vote weights shown for each block.](images/diagrams/annotated-forkchoice-get-weight-0.svg)

<figcaption>

$B_N$ 是将最新的头块投票给了区块 $N$ 的验证者们, $W_N$ 是以区块 $N$ 为根的分支的权重。

</figcaption>
</figure>

分支的权重 $W_x$ 和对区块的投票权重 $B_x$ 之间存在一些明显的关系。

  - 对于只包含一个叶块的分支 $L$, $W_L = B_L$.
  - 一个分支的权重是其对根部区块投票的权重加上其下所有分支的权重之和。所以，在图表中，$W_1 = B_1 + W_2 + W_3$.
  - 一个分支的权重是构成该分支的子树中的所有区块所获得的投票权重的和。

由于投票总是带有正权重，因此没有任何区块的权重会大于根块的权重，根区块的权重是树中所有区块的投票权重之和。每个验证者最多有一个最新消息——也就是一票——因此，权重被限制在所有活跃验证者的总有效余额之下。[^fn-ignoring-proposer-boost]

[^fn-ignoring-proposer-boost]: Ignoring proposer boost, which we shall deal with [later](/part2/consensus/issues/#proposer-boost).

###### 获取头块

一旦我们得到每个分支或子树的权重，算法就会递归进行。给定一个区块，我们选择从从它开始的最重的分支。然后，我们对位于该分支根部的区块重复这个过程。如果某区块只有一个子块，那么选择显而易见，不需要做任何工作。如果两个或更多分支的权重相同，我们就任意选择一个根位于具有最高区块哈希值的子区块的分支。最终会找到一个没有子块的区块。这是一个叶子区块，也将是算法的输出结果。

展开 GHOST，我们就会发现该算法：是贪婪的（Greedy），即立即选取最重的可被观察的（Heaviest-Observed）分支，而不进一步搜寻；并且处理子树（Sub-Trees），一个分支的权重是子树中所有区块投票权重的总和。

下面是一个简单的例子。在图中，我区分了（1）特定区块的票数的权重，即附在每个区块上的数字；（2）分支的权重，我将其添加到连接区块与其父母辈区块的线上。

第一，根据存储库中的最新消息，我们来计算树中每个区块的投票权重。

<a id="img_annotated_forkchoice_lmd_ghost_0"></a>
<figure class="diagram" style="width: 85%">

![Diagram of a block tree showing the weight of votes for each block.](images/diagrams/annotated-forkchoice-lmd-ghost-0.svg)

<figcaption>

`get_head()` 函数以区块树的根区块 $A$ 开始。数字表示每个区块的票数权重。

</figcaption>
</figure>

第二，根据每个区块的票数权重，我们可以计算出每个分支或子树的权重。

<a id="img_annotated_forkchoice_lmd_ghost_1"></a>
<figure class="diagram" style="width: 85%">

![Diagram of a block tree showing the weight of each block and the weight of each branch, or subtree.](images/diagrams/annotated-forkchoice-lmd-ghost-1.svg)

<figcaption>

当函数 `get_weight()` 被应用于一个区块时，会返回该区块的子树及子树所有后代的总权重。这些权重显示在子块和父母代区块之间的线上。

</figcaption>
</figure>

第三，我们在子树根部的区块中递归移动，始终选择权重最高的分支或子树。最终，我们将到达一片叶子，也就是我们选择的头块。

<a id="img_annotated_forkchoice_lmd_ghost_2"></a>
<figure class="diagram" style="width: 85%">

![Diagram of a block tree showing the branch chosen by the GHOST rule.](images/diagrams/annotated-forkchoice-lmd-ghost-2.svg)

<figcaption>

在区块 $A$ 处，分支 $C$ 更重。在 $C$ 处，分支 $E$ 更重。区块 $E$ 是一个叶块，因而是 GHOST 所选择的区块。我们的区块链是 $[A \leftarrow C \leftarrow E]$。“最长链”的规则将在此选择区块 $G$，尽管该分支仅获得验证者的少数支持。

</figcaption>
</figure>

如果说，以区块 B 和区块 C 为根的子树有相同权重，我们将选择具有最大的区块哈希值的那个区块——这是一个完全任意的决断机制。

###### 规范

在规范中实现这一切的代码是 [`get_head()`](/part3/forkchoice/phase0/#get_head)，它从区块树的根向上走，每到一个分叉处就取最重的分支。计算子树权重的代码是 [`get_weight()`](/part3/forkchoice/phase0/#get_weight)。

如果你查看一下 [`get_weight()`](/part3/forkchoice/phase0/#get_weight)，就会发现它比我们在这里介绍的要复杂，这是由于一种叫做 “proposer boost（提议者权重提升）”的东西。我们将在 “议题与修复（[Issues and Fixes](/part2/consensus/issues/)）”部分详细讨论它。

#### 直觉

在了解了 GHOST 协议的工作原理后，我们或许能更直观地理解为什么更喜欢它而不是最长链规则。分叉的出现表明，区块传播时间已经与区块生产间隔（时隙）相近，甚至超过了区块生产间隔（时隙）。简而言之，并非所有验证者都能及时看到所有区块，对其进行认证或在其上构建新区块。[^fn-toward-12s]

[^fn-toward-12s]: See Vitalik's [Toward a 12-second block time](https://blog.ethereum.org/2014/07/11/toward-a-12-second-block-time) for a fascinating analysis of this in a proof of work context. However, not much of it carries over to our PoS implementation, except that GHOST helps to make sense of a forkful network.

在这种情况下，我们需要最大限度地利用可用信息。对同一个父母区块的两个不同子区块的投票，应被视为是所有验证者都支持父母块分支，即使它们对子区块存在不同意见。GHOST 实现这一点的方法很简单，就是允许对子区块的投票增加其所有祖先区块的权重。因此，在面临选择时，我们会倾向于得到了验证者最大支持的分支。我在[上面的图表](#img_annotated_forkchoice_lmd_ghost_2)中说明了这一点：即使区块 $B$ 比区块 $C$ 得到更多的直接投票，分支 $C$ 还是比分支 $B$ 得到更多支持，因为总体上支 $C$ 拥有更多的验证者的最新投票。

最长链规则会扔掉所有这些信息。即使只有少数验证者在在某个分支上工作，最长链规则也可能会让该分支获得胜利。

#### 确认规则

在工作量证明中，唯一可用于分叉选择的数据来自区块生产，它代表了单个矿工在发布该区块时的视图。

而在以太坊的权益证明协议中，除了区块提议者的观点外，我们可以获得更多的信息，这些信息以头块投票的形式来自于每 12 秒中 $\frac{1}{32}$ 的验证者集。

原则上，所有这些额外的信息应该能使我们非常快和非常确定地知道区块是否会保持规范或是否有被回滚的危险。工作量证明链倾向于围绕区块所收到的确认数量使用启发式方法。也就是说，区块被回滚的可能性随着在其上构建的区块数量的增加而呈指数级减少。这种假设通常是正确的，但在高延迟环境中可能会摔得很惨（例如攻击者秘密创建更长的链）。

对这个问题，我们将在下一节中看到，权益证明的最终答案是最终确定性。LMD GHOST 本身并不提供最终确定性。不过，我们可以考虑是否存在某种类似于工作证明确认规则的启发式方法，而事实证明确实[存在](https://ethresear.ch/t/confirmation-rule-for-ethereum-pos/15454?u=benjaminion)。实际上，它通过给出关于区块安全性的“是/否”声明，而不是概率，改进了工作量证明的确认规则。

Aditya Asgaonkar 在一篇[博文](https://www.adiasg.me/confirmation-rule-for-ethereum/)和随附的论文中介绍了确认规则的细节。总体思路是，对于一个区块 b，我们计算 $q$ 和 $q_\text{min}$ 两个值。当 $q > q_\text{min}$ ，且网络仍接近同步时，则该区块被确认为“安全”。我们可以完全确信，它不会被重组。

数量 $q^n_b$ 被定义为在时隙 $n$ 时，以 $b$ 区块为根的子树的权重除以自 $b$ 被提议以来投出的总投票权重。简单来说，自区块 $b$ 被提议以来，在各个时隙中，如果有 80% 的验证者为 $b$ 或其后代区块投票，那么 $q^n_b$ 就是 $0.8$。

$q_\text{min}$ 被定义为 $\frac{1}{2} + \beta$，其中 $\beta$ 是我们认为由对手控制的质押比例。这个比例是未知的，但假设是少于三分之一，不然我们就会有大问题。

现在，如果对于 $b$ 及其所有（还未最终确定的）祖先 $b'$，都有 $q^n_{b'} > q_\text{min}$，那么 $b$ 就被视作是已确认的，或者说是“安全的”。

背后的想法是，当一个从区块 b 开始的分支累积了可用投票权重的简单多数后，所有诚实的验证者将继续为该分支投票，因此它将无限期地保持其多数地位。如果不诚实的验证者把他们的票转移到另一个分支，使该分支获得多数票，这个方法就可能失败。这就是为什么我们在基本多数安全参数 $\frac{1}{2}$ 上增加了一个分数 $\beta$。这样即使所有不诚实的验证者更换分支，我们的分支也能保持多数票。另一种失败的原因是，如果网络开始出现延迟或分区（失去同步性），以至于一些诚实的验证者看不到其他诚实验证者的投票，因此这一方法依赖网络保持足够的同步性。

虽然这看起来非常直观，但与 Casper FFG 集成相关的一些重要细微差别和复杂性修改了确认规则，因此任何与确认规则打交道的人都应该查阅[完整论文](https://ethresear.ch/uploads/short-url/fV7zyTggJtMn8xlUUNVXTOB532G.pdf)（仍然是草稿）。此外，提议者增强 ([proposer boost](/part2/consensus/issues/#proposer-boost)) 使得敌对方的区块提议者更容易重组一个区块，因此我们也需要考虑到这一点。

下表总结了区块的确认规则和最终确定性之间的差异。

| &nbsp;   | 确认      | 最终确定性 |
| ----     | --------          | -------- |
| 时间 | 理想情况下一个时隙，一般情况下不到一分钟。| 通常需要至少两个时段/13分钟。|
| 假设 | 网络保持同步直到最终确定。| 不假设网络同步 |
| 破坏 | 如果网络不保持同步，已确认的区块可以被重组。| 如果超过 $\frac{1}{3}$ 的验证者进行罚没性的行动，一个冲突区块也可以被最终确定。|

在我写这篇文章的时候，客户端软件还没有实现确认规则，但应该会在适当的时候通过[安全区块](/part3/safe-block/)规范提供。

#### LMD GHOST 中的激励

加密经济系统保障自身安全的一种方式是奖励良好行为，惩罚不当行为。在我们对 LMD GHOST 的实现中，提议者和认证者都会因准确找到链头而以不同的方式获得奖励。

对于区块提议者来说，在最佳头块上构建区块的激励是显而易见的。如果不这样做，那么它的区块很可能不会被最终的规范链包含——它将被孤立——在这种情况下，提议者将无法获得任何区块奖励。这是一种隐性激励而非显性激励；工作量证明中的矿工也处于类似的情况。

相比之下，验证者会因为准确投票而直接获得奖励。当验证者做出准确的头块投票，并且其认证被包含在下一个时隙的区块中，它会获得一个[微小奖励](/part2/incentives/rewards/#introduction)。一个表现完美的验证者大约会从准确的头块投票中获得其总协议奖励的 22%。提议者则被激励将这些认证包含在区块中，因为他们每成功加入一个验证，就能获得相应的微小奖励。

如果投票结果与规范链中的最终结果一致，那么投票就是准确的。 因此，如果验证者投票给了前一个时隙中的一个区块，而规范链中有一个相匹配的区块，那么验证者就会得到奖励。 如果验证者投票支持的是前一个时隙之前的的区块，这表示跳过了一个时隙；而规范链在该区块和当前区块之间也跳过了一个时隙，那么验证者也将获得奖励。

如果某个验证者对头块的投票不准确，或其对头块的投票在一个时隙内未列入链中，则不会受到惩罚。 当信标链处于压力状态下，进行正确的头块投票时很难的，也会出现迟报和漏报区块。 这往往不是验证者本身的过错，在这种情况下对他们进行惩罚是不公平的。[^fn-head-block-penalty]

[^fn-head-block-penalty]: 最初的第 0 阶段规范确实包含对遗漏头块投票的[惩罚](https://benjaminion.xyz/eth2-annotated-spec/phase0/beacon-chain/#components-of-attestation-deltas)。 [Altair 升级](/part4/history/altair/)中的会计改革取消了这一惩罚。

#### LMD GHOST 中的罚没

权益证明设计的一大突破是采用了罚没来解决“无利害关系”问题（["nothing at stake" problem](https://ethereum.stackexchange.com/questions/2402/what-exactly-is-the-nothing-at-stake-problem)）。 这个问题的实质是，在权益证明中，验证者通过发布多条相互矛盾的信息来模棱两可几乎是没有成本的。

结果证明，这个解决方案相当优雅。 我们可以检测出验证者是否模棱两可，并对其进行惩罚。 这种惩罚被称为“罚没（[slashing](/part2/incentives/slashing/)）”，它会移除验证者一部分的质押，并将其逐出协议。 由于验证者会对自己的信息进行数字签名，因此发现两个相互矛盾的签名信息就是验证者不当行为的绝对证明，所以我们可以放心地“罚没”。

“罚没”这个名字源自 Vitalik 在 2014 年初提出的 [Slasher](https://blog.ethereum.org/2014/01/15/slasher-a-punitive-proof-of-stake-algorithm) 算法。 那是为解决“无利害关系”问题的非常早期的提案。 目前的设计与 Slasher 不太相似，但有些东西还是沿用了下来，尤其是名字。

##### 提议者罚没 

当轮到某个验证者在特定时隙生成区块时，该验证者应该运行[分叉选择规则](/part2/consensus/preliminaries/#fork-choice-rules)，以决定在哪个现有区块的基础上构建自己的区块。 其目标是——根据所掌握的证据，确定最有可能成为最终规范的分叉。也就是说，正确验证者的集合将汇合在一起的那个分叉。

<a id="img_consensus_nas_0"></a>
<figure class="diagram" style="width: 70%">

![Diagram of a block tree with a fork in which the proposer has a choice of head block to build on.](images/diagrams/consensus-nas-0.svg)

<figcaption>

提议者需要选择在哪个区块上构建自己的区块。 根据分叉选择规则，最佳策略是在最不可能被重组的区块上构建新区块。

</figcaption>
</figure>

然而，为什么要选择呢？ 在权益证明下——与工作量证明不同——验证者生产区块几乎不需要成本。 因此，一个好的策略似乎是提议多个区块，每个区块建立在每个可能的头块之上，这样就能保证至少有一个区块能成为最终规范链的一部分。

<a id="img_consensus_nas_1"></a>
<figure class="diagram" style="width: 70%">

![Diagram of a block tree with a fork in which the proposer builds on both head blocks.](images/diagrams/consensus-nas-1.svg)

<figcaption>

在没有惩罚的情况下，懒惰或不诚实的提议者可能会选择把同时拓展分叉的两边。

</figcaption>
</figure>

这是不可取的，因为它会延长任何分叉，并阻止网络向线性历史汇聚。 链上用户可能无法确定哪个分叉是正确的，这就使他们容易受到双花攻击，而这正是我们希望避免的。 

这展示了“[无利害关系](https://ethereum.stackexchange.com/questions/2402/what-exactly-is-the-nothing-at-stake-problem)”问题。 如上所述，解决办法是检测出两个相互矛盾的区块，并罚没提议它们的验证者。

提议者的模棱两可行为不会在协议内被检测到，而是依靠第三方以 [`ProposerSlashing`](/part3/containers/operations/#proposerslashing) 对象的形式构建模棱两可证明。该证明只包括两个已签名的信标区块头：这足以证明提议者在同一个时隙中签署了两个区块。 随后的区块提议者将在区块中纳入该证明（并因此获得丰厚奖励），协议将对违规验证者的余额进行罚没，并将其驱逐出活跃验证者的集合。

##### 认证者罚没

同样，当轮到某个验证者发布认证时，验证者应该运行其分叉选择规则，并投票给它认为是最好的头块。这里的问题是，攻击者可能会做出多个相互矛盾的认证，以引起或延长分叉，阻止网络聚合成单一链。即使基本诚实的验证者也可能受到诱惑，为一个似晚非晚的区块做出两个认证：一个为该区块投票，一个为空时隙投票。如果这不是应受惩罚的行为，那么这样做就能避免因投错票而错失奖励的机会。

纠正方法是一样的：检测并惩罚相互矛盾的认证——也就是同一验证者在同一时隙里为不同头块所做的认证。

#### 历史

以太坊中的 LMD GHOST 分叉选择起源于 Vlad Zamfir 在 Casper CBC 协议（在那时被称为 Casper TFG，请不要与 Casper FFG 相混淆，后者相当不同[^fn-naming-in-ethereum]）上的工作。 Casper TFG [最初发布](https://blog.ethereum.org/2015/08/01/introducing-casper-friendly-ghost)于 2015 年，在 2017 年发表的鬼马小精灵 ([Casper the Friendly Ghost](https://raw.githubusercontent.com/vladzamfir/research/master/papers/CasperTFG/CasperTFG.pdf)) 论文中，Zamfir 描述了将 Sompolinsky 和 Zohar 的 [GHOST](https://eprint.iacr.org/2013/881) 协议与“最新消息”结构相结合的情况。

[^fn-naming-in-ethereum]：欢迎体验在以太坊里起名字的乐趣

在 2018 年 8 月，Vitalik [仍然倾向于](https://web.archive.org/web/20230630135358/https://nitter.it/VitalikButerin/status/1029906887376961536#m)一种名为 IMD GHOST（原名为“递归接近合理性”, Recursive Proximity to Justification）的分叉选择，它比如今纯粹的 LMD GHOST 更注重最终确定性与合理性。 随着 [Eth2 共识迷你规范](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760?u=benjaminion)的进展，IMD GHOST 于 2018 年 11 月[^fn-see-the-changes]更名为 LMD GHOST。 这是因为人们担心 IMD GHOST 的[稳定性能](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760/17?u=benjaminion)。

[^fn-see-the-changes]：点击标题附近的铅笔图标，可以查看[迷你规范](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760?u=benjaminion)的历史更改记录。

2018 年 11 月[迷你规范](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760?u=benjaminion)中对 LMD GHOST 的描述基本上就是我们今天使用的版本。

#### 另见

请注意，本节只介绍了 LMD GHOST 的纯粹形式。在以太坊的完整共识协议中，通过与 Casper FFG 集成，LMD GHOST 被修改了，我们将在 [Gasper](/part2/consensus/gasper/) 部分了解到这一点。 它也被[提议者增强](/part2/consensus/issues/#proposer-boost)修改，我们稍后会讨论。

共识规范仓库中的[分叉选择](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/fork-choice.md)文档包含了相关规范。在我注解的规范中，它们被包含在以下几处。

  - 当我们想获得当前最佳头块的信息时，[`get_head()`](/part3/forkchoice/phase0/#get_head) 是分叉选择的入口点。
  - [`get_weight()`](/part3/forkchoice/phase0/#get_weight) 是对 LMD GHOST 算法的实现。
  - The [`on_attestation()`](/part3/forkchoice/phase0/#on_attestation) 处理程序让分叉选择获得 LMD GHOST 的票数。
    - 主要由 [`validate_on_attestation()`](/part3/forkchoice/phase0/#validate_on_attestation) 验证这些投票。
    - [`update_latest_messages()`](/part3/forkchoice/phase0/#update_latest_messages) 负责保存最新信息的记录。

Vitalik [注解的分叉选择](https://github.com/ethereum/annotated-spec/blob/master/phase0/fork-choice.md)的引言部分是很好的概述（尽管规范本身的一些细节已经过时）。 Vitalik 的 [CBC Casper 教程](https://vitalik.ca/general/2018/12/05/cbc_casper.html)中的 LMD GHOST 部分也有精彩的解释。请忽略后面关于 Casper CBC 最终确定性的内容——这与我们无关。

[RLMD GHOST](https://arxiv.org/pdf/2302.11326.pdf) 论文回顾了以太坊的 LMD GHOST 实现的一些弱点。 例如，当大量验证者离线时，LMD GHOST 无法安全地处理动态参与问题。 我们稍后会考虑其中一些议题，但如果你想先了解一下，可以阅读该论文的引言部分。

### Casper FFG <!-- /part2/consensus/casper_ffg/ -->

<div class="summary">

  - Casper FFG 为 Eth2 共识协议增加了最终确定性。
  - 作为 LMD GHOST 共识的覆盖层，它修改了前者的分叉选择规则。
  - 在异步条件下，当少于 $\frac{1}{3}$ 的验证者存在故障或是恶意时，Casper FFG 具有经典的安全性。
  - 此外，当超过 $\frac{1}{3}$ 的验证者是敌对方时，Casper FFG 能够通过罚没机制来提供可问责的安全性，也被称为经济最终确定性。

</div>

#### 引言

已经有许多文章解释了 Casper FFG 的基本机制——它是如何工作的——但关于它为什么有效的文章却很少。我希望在本节结束时，你会对 Casper FFG 为何有效有所了解。

Casper FFG 的机制并不复杂。即便如此，当你继续阅读时，请记住Casper FFG 的有效性实际归结于两个重要的概念。首先是两阶段承诺（合理性和最终确定性），其次是可问责的安全性。

两阶段承诺为 Casper FFG 提供了经典的共识安全性。它使得宣布区块为最终确定的区块成为可能，并确保诚实的验证者不会回滚它们。但这只有在超过三分之二的质押由诚实的验证者控制时才能实现，而我们并不能总是这样假设。

在此基础上，Casper FFG 为超过三分之一的验证者不诚实的情况提供了额外的保证，即“可问责的安全性”。如果链出现相冲突的最终确定性，至少会有三分之一的质押以太币会被销毁。如果验证者违反了 Casper 的两条戒律中的任何一条，销毁就会被强制执行。

#### 概述

Casper FFG 是一种元共识协议。它是一个可以在底层共识协议之上运行的覆盖层，以便为其增加最终确定性。回想一下，[最终确定性](/part2/consensus/preliminaries/#finality)是指链中的有些区块永远不会被回滚：它们将永远是链的一部分。在以太坊的权益证明共识中，底层协议是 [LMD GHOST](/part2/consensus/lmd_ghost/)，它不提供最终确定性——总是存在验证者决定构建一个相竞争的链的可能性，而这么做也不会受到真正的惩罚。Casper FFG 作为一个“最终确定性小工具”，被用来为 LMD GHOST 增加最终确定性。

Casper FFG 利用了这一点——作为权益证明协议，我们知道谁是参与者：管理质押的以太币的验证者们。这意味着，我们可以通过计票来判断诚实验证者的票数是否占多数。更准确地说，是管理多数质押的验证者的投票——在接下来的所有内容中，每个验证者的投票都会根据其管理的质押的价值进行加权，但为了简单起见，我们不会每次都详细说明。

我们在一个异步网络上运行，即互联网，这意味着如果想要同时实现安全性和活性，我们最多只能容忍 $\frac{1}{3}$ 的验证者是敌对的（或有故障的）。这是共识理论中一个众所周知的结果[^fn-bracha-toueg], 其推理如下。

  - 总共有 $n$ 个验证者，其中一些数量的验证者 $f$ 可能以某种方式发生故障或与我们为敌。
  - 为了保持活性，我们需要在只听到 $n - f$ 个验证者的情况下做出决定，因为 $f$ 个存在故障的验证者可能会保留他们的投票。
  - 但这是一个异步环境，所以 $f$ 个未响应者可能只是出于延迟，而没有发生故障。
  - 因此，在我们收到的 $n - f$ 个响应中，最多有 $f$ 个响应可能来自故障或敌对的验证者。
  - 为了保证我们在听到 $n - f$ 个验证者后总是能够达到诚实验证者的简单多数，需要 $(n - f)/2 > f$。也就是说， $n > 3f$。

[^fn-bracha-toueg]: 例如，请参阅 Bracha 和 Toueg 的论文《异步共识与广播协议》（[Asynchronous consensus and broadcast protocols](https://dl.acm.org/doi/10.1145/4221.214134)）(1985)。 Pease、Shostak 和 Lamport 的早期著作 《在故障存在时达成一致》（[Reaching Agreement in the Presence of Faults](https://lamport.azurewebsites.net/pubs/reaching.pdf)）（1980）给出了相同的约束，但它是基于在验证者可以伪造信息的系统中的同步通信。我们的环境是异步的，信息是不可伪造的，因此与 Bracha 和 Toueg 的分析相关。

总结来说，像所有经典的拜占庭容错（BFT）协议一样，当总体的验证者集中少于三分之一存在故障或与链相敌对时，Casper FFG 能够提供最终确定性。当足够多的诚实验证者宣布一个区块被最终确定时，所有诚实的验证者都会跟进，那个区块将不会被回滚。然而，正如我们将看到的，与经典的 BFT 协议不同，即使超过三分之一的验证者集存在故障与链相敌对，Casper FFG 也能够提供经济最终确定性（可问责的安全性）。

在这一节中，我们将只考虑 Casper FFG，而不会花太多时间讨论它与 LMD GHOST 如何集成。这符合 [Casper FFG 论文](https://arxiv.org/pdf/1710.09437.pdf)的精神，该论文很少涉及底层区块链共识机制。我们将在[下一节](/part2/consensus/gasper/)中看看这两者是如何结合而成 Gasper。

#### 命名

重复一下：Casper FFG 这个名字由两部分组成，两部分都值得一看。

##### Casper
名字中的 Casper 似乎是 Vlad Zamfir 起的。 他在《Casper 历史》的[第 5 部分](https://medium.com/@Vlad_Zamfir/the-history-of-casper-chapter-5-8652959cef58)中这样解释。 

> 在这一章中，我讲述了 Casper 的诞生故事，它是 Aviv Zohar 和 Jonatan Sompolinsky 的 GHOST 原则在权益证明中的应用。
>
> 之所以称它为“友好的幽灵”，是因为设计了一些激励机制，以确保对寡头垄断者的审查抵抗：这些激励迫使垄断集团友好对待非垄断集团中的验证者。

他提到的 GHOST 协议与我们在[上一节](/part2/consensus/lmd_ghost/#ghost)中提到的相同。如果你知道这个文化背景——“鬼马小精灵”是一个自 [20 世纪 40 年代](https://en.wikipedia.org/wiki/Casper_the_Friendly_Ghost)以来就一直存在的卡通人物——就会有助于理解这一切。

Zamfir 的协议最初被称为 Casper TFG（The Friendly Ghost，友好的幽灵），后来更名为 Casper CBC（Correct By Construction）。 Vitalik 的 Casper FFG 与扎姆菲尔的 Casper TFG/CBC 一起成长，这大概可以解释在命名上的一致，但两者间的共同点很少。 事实上，Casper FFG 甚至没有使用 GHOST 协议。[^fn-casper-confusion]

[^fn-casper-confusion]：当然，这让人很摸不着头脑。但它远不是以太坊的命名中最令人困惑的地方，所以我们就接受它。

##### FFG

FFG 代表 “友好的最终确定性小工具（Friendly Finality Gadget）”，正如 [Casper FFG 论文](https://arxiv.org/pdf/1710.09437.pdf)的标题。

这显然是对 Zamfir 的 TFG 名字的戏仿，但也表明 Casper FFG 不是一个完全自成一体的区块链协议，而是一个可以为底层共识协议增加最终确定性的“小工具（gadget）”。

#### 术语

如往常一样，我们使用的行话是理解协议如何构建的途径。

##### 时段与检查点

为了就最终确定性做出决定，Casper FFG 机制需要至少处理验证者集投票的 $\frac{2}{3}$。在以太坊中，验证者集可能非常庞大，同时广播、传递流言和处理数十万验证者的投票是不可行的。

为了解决这个问题，投票分布在一整个时段[^fn-epoch-dynasty]中。在 Eth2 中，一个时段包括 32 个时隙，每个时隙 12 秒。在每个时隙中，总验证者集的 $\frac{1}{32}$ 被安排进行投票，因此每个验证者在每个时段恰好投票一次。为了提高效率，我们将每个验证者的 Casper FFG 投票与其 LMD GHOST 投票捆绑在一起，尽管这并非必要。

[^fn-epoch-dynasty]: Casper FFG 论文一般用“王朝（dynasty）”一词表示时段，但也有少数例外。两者是同一个意思。

为了确保在某个时段的不同时间投票的验证者有共同的投票目标，我们让他们为检查点投票，即该时段的第一个时隙。时段 $N$ 的检查点在 $32N$ 号时隙（记住时隙和时段的编号从 0 开始）。

<a id="img_consensus_slots_epochs_checkpoints"></a>
<figure class="diagram" style="width: 90%">

![Diagram showing an epoch with a checkpoint at the first slot and blocks within each slot.](images/diagrams/consensus-slots-epochs-checkpoints.svg)

<figcaption>

一个时段被分为 32 时隙，每个时隙通常包含一个区块。某时段的第一个时隙是它的检查点。时间从左到右增加。

</figcaption>
</figure>

顺便说一句，人们常常潦草地说是对时段的最终确定，但这么说不对。Casper FFG 对检查点（即该时段第一个时隙）进行最终确定。当我们对时段 $N$ 的检查点进行了最终确定时，我们就对包括 $32N$ 号时隙在内的一切进行了最终确定。这包括时段 $N-1$ 和时段 $N$ 的第一个时隙。但我们还没有对时段 $N$ 进行最终确定——它还有 31 个未被最终确定的时隙。

我们将暂时假设每个时隙中都有一个区块。这是因为最初的 Casper FFG 检查点是基于区块高度而不是时隙编号。我们将在探讨 [Gasper](/part2/consensus/gasper/) 时放宽这一假设，以允许空时隙和检查点。

在协议中，检查点（[`Checkpoint`](/part3/containers/dependencies/#checkpoint)）对象只包含检查点的时段编号和该时段第一个时隙中的头块的哈希树根（`root`）:

```python
class Checkpoint(Container):
    epoch: Epoch
    root: Root
```

##### 合理性和最终确定性

像经典的拜占庭容错（BFT）共识协议一样，Casper FFG 通过两轮过程实现最终确定性。

在第一轮中，我将当前时段检查点 (比如说 $X$) 的视图广播给网络的其余部分，并听取它们的视图。如果绝对多数告诉我他们也支持 $X$，这就使我认可它的合理性。合理性是我对网络视图的本地判断：在这个阶段，我认为大多数网络成员认为 $X$ 会被最终确定。但我还不知道网络的其余部分是否得出了相同的结论。在敌对的条件下，可能会有足够多数的其他验证者未能就 $X$ 做出决定。我们[稍后](#conflicting-justification)会讨论这种情况。

在第二轮中，我会广播我已经从绝对多数的验证者那里得知他们支持 $X$（即我已经证明了 $X$ 的合理性），以及我从网络其他成员那里得知他们是否认为绝对多数验证者支持 $X$（即，网络中其它成员已证明的 $X$ 合理性）。 如果我听到绝对多数验证者都同意我，认为 $X$ 具有合理性，那么我就会最终确定 $X$。 最终确定性是一个全局属性：一旦一个检查点被最终确定，我就知道任何诚实的验证者都不会回滚它。 即使他们的视图中，检查点还没有被标记为最终确定，我也知道他们至少已经将其标记为具有合理性，且没有任何（非罚没性的）行为能回滚（撤销）这一合理性。

总结一下，要确保整个网络都同意某个区块不会被回滚，需要以下步骤：[^pbft-prepare-commit]

  1. 第一轮（理想情况下带来合理性）：
     1. 我将我所认为的最佳检查点告诉网络。
     2. 我从网络中听取所有其他验证者认为的最佳检查点。
     3. 如果我听到 $\frac{2}{3}$ 和我的想法一致, 我会将合理性赋予该检查点。
  2. 第二轮（理想情况下带来最终确定性）：
     1. 我将我的合理性检查点告知网络，即我从第一轮中获得的集体视图。
     2. 我从网络中听取所有其他验证者认为的集体视图，即合理性检查点。
     3. 如果我听到 $\frac{2}{3}$ 的验证者同意我的选择，我就会最终确定这个检查点。

[^pbft-prepare-commit]: 在[经典 PBFT](https://www.scs.stanford.edu/nyu/03sp/sched/bfs.pdf) 共识协议中，这两轮大致对应于 `PREPARE` 和 `COMMIT` 两阶段。PBFT 的 `PRE-PREPARE` 阶段则相当于在 Casper FFG 中广播检查点区块。

简言之，当我合理化某个检查点时，我承诺永不撤销它。当我最终确定某个检查点时，我知道所有诚实的验证者也承诺永不撤销它。

<a id="img_consensus_two_rounds"></a>
<figure class="diagram" style="width: 95%">

![A diagram illustrating voting in the two rounds.](images/diagrams/consensus-two-rounds.svg)

<figcaption>

在第一轮中，我广播我的最佳检查点并听取其他所有人的最佳检查点。理想情况下，这将带来合理性。在第二轮中，我广播我听到的每个人的最佳检查点，并听取他们的视图。理想情况下，这将带来最终确定性。

</figcaption>
</figure>

在理想条件下，每轮持续一个时段，因此需要一个时段来合理化一个检查点，另一个时段来最终确认一个检查点。在第 $N$ 个时段开始时，我们想要看到第 $N-1$ 个检查点已经被合理化，第 $N-2$ 个检查点已被最终确定。

从量化来看，在协议中，最终确认一个检查点需要 12.8 分钟，即两个时段。在 Casper FFG 中，这两轮是重叠和流水线式进行的，因此，尽管需要 12.8 分钟来完整地最终确定一个检查点，但我们可以每 6.4 分钟就最终确认一个检查点，即每个时段一次。

需要注意的是，在协议外部，可能会稍早于完整的 12.8 分钟就看到某个检查点被最终确认，假设没有长链重组的话。具体来说，在第二轮进行到大约 11 分钟后，可能已经收集到足够的票数，这相当于整个过程的 $\frac{2}{3}$。然而，在协议内部，合理性和最终确定性只在时段处理的末尾时进行。

关于命名：术语“最终确定”和“合理化”并不出现在经典的共识文献中。可以很容易理解“最终确定”，但“合理化”这个术语可能有点奇怪。据我所知，其起源在 Vlad Zamfir 的 [Casper TFG](https://github.com/vladzamfir/research/blob/master/papers/CasperTFG/CasperTFG.pdf) 协议中。在那个作品中，消息包含一个“合理性（justification）”，以支持所做的投票。该论文中并没有使用“合理化（justified）”这个词，但我怀疑来源就是这个地方。在 Casper FFG 中，我的“合理性”是指看到我与 $\frac{2}{3}$ 的验证者认可同一个检查点的证据。

##### 来源和目标、链接和冲突

在 Casper FFG 中，一次投票包含两部分：一个来源（source）检查点投票和一个目标（target）检查点投票。这些分别是认证的[数据](/part3/containers/dependencies/#attestationdata)中的 `source` 和 `target` 字段：

```python
class AttestationData(Container):
    slot: Slot
    index: CommitteeIndex
    # LMD GHOST vote
    beacon_block_root: Root
    # FFG vote
    source: Checkpoint
    target: Checkpoint
```

以一个链接（link）${s \rightarrow t}$ 的形式，来源和目标投票同时进行，其中 $s$ 是来源检查点，$t$ 是目标检查点。

我的目标投票的作用是广播我认为下一个应该被合理化的检查点。在上述术语中，这是我的第一轮投票。我的目标投票是一个软（有条件的）承诺，只要我听到 $\frac{2}{3}$ 的验证者也承诺不回滚该检查点，我就不会回滚它。

我的来源投票的作用是去广播我看到网络中 $\frac{2}{3}$ 的成员支持检查点 $s$，并且这是我所知道的最近的此类检查点。在上述术语中，这是我的第二轮投票，宣布我听到的集体视图。通过进行来源投票，我将之前的软承诺升级为硬（无条件的）承诺：永不回滚该检查点。

<a id="img_consensus_ffg_vote"></a>
<figure class="diagram" style="width: 50%">

![A diagram illustrating how the votes from the two rounds are combined into one attestation.](images/diagrams/consensus-ffg-vote.svg)

<figcaption>

Casper FFG 将来源和目标投票合并为一条信息：一个链接 ${s \rightarrow t}$ 的投票。

</figcaption>
</figure>

诚实验证者的来源投票将始终是其对链的视图中最高的合理化检查点。其目标投票将是自来源检查点以降当前时段的检查点。来源和目标检查点不需要是连续的；允许跳过检查点。但当信标链流畅顺利时，一个时段的目标投票将是下一个时段的来源投票。

<a id="img_consensus_source_target"></a>
<figure class="diagram" style="width: 50%">

![A diagram showing a valid link from source to target.](images/diagrams/consensus-source-target.svg)

<figcaption>

从已合理化的检查点到由它衍生的链上检查点的链接是有效的。这里只显示了检查点；中间的区块为了清晰性而被省略。

</figcaption>
</figure>

在一个有效链接中，来源检查点始终是目标检查点的祖先。如果不是这样，我就会自相矛盾：来源投票宣布了我永不回滚检查点 $s$ 的承诺；如果目标检查点 $t$ 不是从 $s$ 派生的，那么这个投票将回滚 $s$。然而，发布这样一个无效链接并不是可被罚没的行为[^fn-conflicting-link-slashable]。

[^fn-conflicting-link-slashable]: 在[早期版本](https://medium.com/@VitalikButerin/minimal-slashing-conditions-20f0b500fc6c)的 Casper FFG 中，链接相冲突的检查点是可被罚没的行为，但在我们今天使用的 Casper FFG 设计中，这一规定已被简化，如[Casper FFG 论文](https://arxiv.org/pdf/1710.09437.pdf)的脚注 4 所述。

<a id="img_consensus_conflict"></a>
<figure class="diagram" style="width: 50%">

![A diagram showing an invalid link from source to a conflicting target.](images/diagrams/consensus-conflict.svg)

<figcaption>

将已合理化的检查点与不是它的后代的链的检查点相链接是无效的。这两个检查点被称为相冲突（conflicting），因为它们互不是对方后代。

</figcaption>
</figure>

在 Eth2 的实现中，有一些关于及时性的具体标准，有效的 Casper FFG 投票必须满足这些标准。我们将在 [Gasper 部分](/part2/consensus/gasper/)中详细讨论这些标准，因为它们不适用于我们这里考虑的、抽象的 Casper FFG 协议。

在权衡 Casper FFG 投票时，只有那些已被包含在区块中的投票会算数。与 LMD GHOST 的分叉选择不同，我们不会考虑通过流言协议、信鸽、或其他任何方式接收到的 Casper FFG 投票。这是因为我们必须始终有一个关于最终确定性的决策的共同记录，而区块历史提供了这个共同记录。所以，当我上面说“我告诉网络”时，这是说我在广播一个将被区块提议者接收并包含在区块中的认证。当我说“我从网络中听”时，这是说我处理了包含在区块中的认证。

Also, allow me to reiterate that votes in Casper FFG are weighted according to validators' [effective balances]. A vote from a validator with a 24 ETH effective balance carries 75% of the weight of a vote from a validator with a 32 ETH effective balance. I will frequently say things like, "the votes of two-thirds of the validators"; you should understand this as, "the votes of validators managing two-thirds of the stake".

此外，请允许我重申：Casper FFG 中的投票根据验证者的[有效余额](/part2/incentives/balances/)加权。一个有效余额为 24 个以太币的验证者的投票权重是一个有效余额为 32 个以太币的验证者的投票权重的 75%。我经常会说“三分之二的验证者的投票”；你应该理解为“管理了三分之二质押的验证者的投票”。

##### 绝对多数链接

如上所述，链接（Link）是将来源和目标检查点连接起来的Casper FFG 投票对，即 ${s \rightarrow t}$。

当超过 $\frac{2}{3}$ 的验证者（按权益加权）发布相同的链接（且他们的投票被及时包含在区块中）时，链接 ${s \rightarrow t}$ 就是一个绝对多数链接（upermajority link）。

#### Casper FFG 的机制

在知道（大部分）术语和关键概念之后，我们现在可以详细了解 Casper FFG 的运作方式。它相当简单。

##### 合理性

当一个节点看到一个从已被合理化的检查点 $c_1$ 到检查点 $c_2$ 的绝对多数链接时，该节点将认为检查点 $c_2$ 将被合理化。

<a id="img_consensus_justified"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing that a supermajority link justifies a checkpoint.](images/diagrams/consensus-justified.svg)

<figcaption>

我的节点看到了一个绝对多数链接：${C_N \rightarrow C_{N+2}}$。因此我将 $C_{N+2}$ 标记为已被合理化。已被合理化的检查点有阴影覆盖，并用 “J” 标记。

</figcaption>
</figure>

合理性意味着我已经看到验证者集里超过 $\frac{2}{3}$ 的验证者的承诺：如果她们得知至少 $\frac{2}{3}$ 的验证者确认不会回滚检查点 $c_2$，她们也不会回滚检查点 $c_2$。

##### 最终确定性

当某个节点看到从已被合理化的检查点 $c_1$ 到检查点 $c_2$ 的绝对多数链接，且检查点 $c_2$ 是 $c_1$ 的直接子检查点时，该节点就将认为检查点 $c_1$ 已被最终确定。

换句话说，一个被最终确定的检查点是一个已被合理化的检查点，且其直接子检查点也已被合理化。

<a id="img_consensus_finalised"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing that when the direct child of a justified checkpoint is justified, the parent checkpoint is finalised.](images/diagrams/consensus-finalised.svg)

<figcaption>

我的节点看到了一个绝对多数链接 ${C_N \rightarrow C_{N+1}}$，因此我将 $C_{N+1}$ 标记为已被合理化。在检查点树中，由于 $C_{N+1}$ 是 $C_N$ 的直接子检查点，我也将 $C_N$ 标记为被最终确定。被最终确定的检查点被相交叉的阴影覆盖，并标记为 “F”。

</figcaption>
</figure>

最终确定性意味着我已经看到超过 $\frac{2}{3}$ 的验证者的确认，他们已经看到超过 $\frac{2}{3}$ 的验证者承诺不会撤销检查点 $c_1$。现在，检查点 $c_1$ 不能被撤销，除非根据证据得知至少 $\frac{1}{3}$ 的验证者改变了想法，并因此会被罚没。

上述最终确定检查点的规则是原始 Casper FFG 论文中描述过的那个。在实践中，我们可以轻微地泛化这一规则，而不影响安全性证明（[safety proof](#proof-of-accountable-safety)）。这种泛化称为 [k-最终确定性]（[k-finality](#k-finality)），我们稍后会讨论。

##### Casper 戒律

在那篇 Casper FFG 论文中，每个检查点都有一个被定义好的高度：如果 $c$ 是一个检查点，那么 $h(c)$ 就是该检查点的高度。检查点的高度随着它与创世区块的距离增加而单调递增。

在以太坊 2.0 的 Casper FFG 实现中，检查点高度是检查点的时段编号：$h(c) = \texttt{c.epoch}$。请记住，一个检查点由一个区块哈希和一个时段编号组成。如果这两者中任何一个出现不同，检查点就不同。

Casper FFG 对可问责安全性的证明依赖于对任何违反以下两条规则或“戒律”的验证者的罚没。

###### 不得重复投票

**戒律一**: 验证者不得发布不同的投票 ${s_1 \rightarrow t_1}$ 和 ${s_2 \rightarrow t_2}$，使得 $h(t_1) = h(t_2)$。

简单来说，验证者在任何目标时段中最多只能进行一次投票。

<a id="img_consensus_commandment_1a"></a>
<figure class="diagram" style="width: 60%">

![A diagram showing a violation of the no double vote rule by voting for the same target checkpoint from different source checkpoints.](images/diagrams/consensus-commandment-1a.svg)

<figcaption>

违反不得重复投票规则的一种方式：投票具有不同的来源检查点和相同的目标检查点：${0 \rightarrow 3}$ 和 ${1 \rightarrow 3}$。

</figcaption>
</figure>

<a id="img_consensus_commandment_1b"></a>
<figure class="diagram" style="width: 60%">

![A diagram showing a violation of the no double vote rule by voting for target checkpoints at the same height on different branches.](images/diagrams/consensus-commandment-1b.svg)

<figcaption>

违反不得重复投票规则的另一种方式：在同一时段中为不同的目标检查点投票：${0 \rightarrow 3}$ 和 ${0 \rightarrow 3'}$。

</figcaption>
</figure>

###### 无“环绕投票”

**戒律二**: 验证者不得发布不同的投票 ${s_1 \rightarrow t_1}$ 和 ${s_2 \rightarrow t_2}$ 使得 $h(s_1) < h(s_2) < h(t_2) < h(t_1)$.

也就是说，验证者不得进行使其链接环绕（surround）先前的投票链接，或被其先前投票的链接所环绕的投票。

<a id="img_consensus_commandment_2a"></a>
<figure class="diagram" style="width: 60%">

![A diagram showing a violation of the no surround vote rule on a single branch.](images/diagrams/consensus-commandment-2a.svg)

<figcaption>

一种违反“无环绕投票”规则的方式：链接 ${0 \rightarrow 3}$ 环绕链接 ${1 \rightarrow 2}$。

</figcaption>
</figure>

<a id="img_consensus_commandment_2b"></a>
<figure class="diagram" style="width: 60%">

![A diagram showing a violation of the no surround vote rule on conflicting branches.](images/diagrams/consensus-commandment-2b.svg)

<figcaption>

另一种违反“无环绕投票”规则的方式：尽管在不同的分支上，链接 ${0 \rightarrow 3}$ 再次环绕链接 ${1 \rightarrow 2}$。

</figcaption>
</figure>

##### 罚没

违反其中任何一条 Casper 戒律的验证者将面临罚没。这意味其部分或全部质押将被移除，并被驱逐出验证者集合。

通过对不良行为定价，罚没支撑了 Casper 的可问责安全保证——特别是那些可能导致冲突检查点被最终确定的行为。LMD GHOST 中[也有](/part2/consensus/lmd_ghost/#slashing-in-lmd-ghost)罚没，而[激励层章节](/part2/incentives/slashing/)会有更多关于罚没详细机制的信息。

在协议中对戒律的违反进行检测可能会有一定难度。特别是，检测环绕投票可能需要搜索验证者的大量投票历史[^fn-proto-surround-vote]。因此，我们依赖外部罚没检测服务来检测对罚没条件的违反，并将证据提交给区块提议者。网络中只需有一个可靠的此类服务即可。实际上我们有更多，但并非每个节点运营者都需要运行罚没检测器。

一旦发现违反戒律的证据，就容易在链上证明验证者违反规则。验证者签署自己发布的每个认证，因此，如果有两个相冲突的认证，那么验证他们的签名并证明验证者发布它们时违反了规则就[轻而易举](/part3/transition/block/#attester-slashings)了。

[^fn-proto-surround-vote]: 参见 Protolambda 的 [`eth2-surround`](https://github.com/protolambda/eth2-surround) GitHub 仓库，了解对找到环绕投票的所面临的挑战的分析。

[^fn-slasher-implementations]: [Lighthouse 团队](https://lighthouse-book.sigmaprime.io/slasher.html) 和 [Prysm 团队](https://docs.prylabs.network/docs/prysm-usage/slasher) 已经开发了罚没检测软件。 

在 Casper FFG 论文中的协议假设：在被证明违反罚没条件后，“验证者的全部存款将被没收”。我们在 Eth2 的实现是[它的一种变体](/part2/incentives/slashing/#the-correlation-penalty)：按照在特定期间被罚没的总质押去等比例缩放验证者被没收的质押。如果在 36 天内 $\frac{1}{3}$ 的总质押违反了惩罚条件，那么验证者的整个质押将被没收，类似于经典的 Casper FFG 协议。但如果在此期间内只有极少的其他罚没，那么几乎没有质押会被没收。这种细节在实践中并不改变 Casper FFG 的保证，至少自从 [Bellatrix 升级](/part4/history/bellatrix/) 后的信标链让 [`PROPORTIONAL_SLASHING_MULTIPLIER`](/part3/config/preset/#proportional_slashing_multiplier) 常量获得了最终值。

##### 分叉选择规则

Casper FFG 的分叉选择规则是对底层共识机制分叉选择规则的一种修改。根据 Casper FFG 论文，底层共识机制必须：

> 跟随包含了最高合理化检查点的链。

纯 [LMD GHOST](/part2/consensus/lmd_ghost/) 协议总是从链的根部，即创世区块，开始搜索头块。当被 Casper FFG 的分叉选择规则修改时，LMD GHOST 将从其已知的最高合理化检查点开始搜索头块，并忽略不是从最高已证明检查点发展而来的潜在头块。我们将在 [Gasper](/part2/consensus/gasper/) 中进一步讨论这一点。

对底层共识协议分叉选择规则的这一修改赋予了最终确定性。当一个节点在其本地视图中合理化一个检查点时，它在承诺永远不会回滚该检查点。因此，基础的链必须始终包含该检查点；所有不包含该检查点的分支必须被忽略。

请注意，这一分叉选择规则与 Casper FFG [合理的活性]([plausible liveness](#plausible-liveness)) 保证相兼容。

#### Casper FFG 的保证

Casper FFG 共识协议提供了两个保证，这两个保证类似于经典共识中的安全性和活性，但又有所不同：可问责的安全性（accountable safety）和合理的活性（plausible liveness）。

##### 可问责的安全性

经典的 PBFT 共识只能在少于三分之一的验证者是敌对的（有故障）时保证安全性。如果超过三分之一的验证者是敌对方，那么它根本无法做出任何保证。

当敌对验证者控制少于三分之一的质押时，Casper FFG 基本上提供了相同的安全性保证：已实现最终确定的检查点永远不会被回滚。此外，它进一步保证，如果相互冲突的检查点被最终确定，那么控制至少三分之一的质押以太币的验证者将被罚没。这被称为“可问责的安全性”。它是可问责的，因为我们可以精确识别哪些验证者行为不当并直接惩罚他们。

这种保证提供的额外安全性并不是传统共识协议意义上的安全性，而是一种特定的加密经济学性质的安全性：协议会极大地阻止不良行为。这通常被称为“经济最终确定性”。

###### 可问责安全性的证明

Casper FFG 的可问责安全性的证明相当直观。我将在下面勾勒出这个证明，它大致与 Casper FFG 论文中所述相似，但将其修改为适用于时段，而不是论文中使用的更抽象的“检查点高度”。 

我们将证明，如果少于 $\frac{1}{3}$ 的验证者（按质押加权）违反其中一条 [Casper 戒律](#the-casper-commandments)，则两个相互冲突的检查点不能同时被最终确定。我们的思路是：唯一能够最终确定冲突检查点的方式是存在被另一个绝对多数链接环绕的绝对多数链接，而这与本段开头的假设相矛盾，因为环绕投票违反了[戒律二](#no-surround-vote)。

我们需要注意的第一点是，在此假设下，任何时段内（在诚实节点看来）最多只有一个检查点被合理化，这直接源于不得双重投票戒律。

让我们假设有分别位于时段 $m$ 和 $n$ 中的两个相互冲突的已被最终确定的检查点 $a_m$ 和 $b_n$。由于这些检查点是冲突的，因此其中任意一个不会是另一个的后代。根据之前的观察，我们知道 $m \ne n$。为了不失去普遍性，我们假设 $m < n$，因此 $b_n$ 是两者中较高的已被最终确定的检查点。

现在，必须有一个从根检查点到 $b_n$ 的合理化检查点的连续系列，并且它们之间有绝对多数链接。也就是说，有一组 $k$ 个绝对多数链接 ${{r \rightarrow b_{i_1}},\allowbreak {b_{i_1} \rightarrow b_{i_2}},\allowbreak {b_{i_2} \rightarrow b_{i_3}},\allowbreak \ldots,\allowbreak {b_{i_{k-1}} \rightarrow b_{i_k}}}$，其中 $i_k = n$。这符合合理性的定义。因此，通向 $b_n$ 的被合理化检查点集合是 $\mathcal{B} = {r, b_{i_1},\allowbreak b_{i_2}, b_{i_3},\allowbreak \ldots,\allowbreak b_{i_{k-1}}, b_{i_k}}$。我们可以想象，从根检查点出发，沿着绝对多数链接跳跃，依次落在这些检查点上，再跳到下一个。

<a id="img_consensus_justification_chain"></a>
<figure class="diagram" style="width: 85%">

![A diagram showing a chain of justified checkpoints joined by contiguous supermajority links from the root up to a finalised block.](images/diagrams/consensus-justification-chain.svg)

<figcaption>

对于任何已被最终确定的检查点，例如 $b_{10}$，都有一条从根检查点 $r$ 到它自身的、连续的绝对多数链接组成的链。这条链接的链合理化检查点集合 $\mathcal{B} = {r, b_1,\allowbreak b_4, b_5,\allowbreak b_9, b_{10}}$。阴影中的检查点是被合理化的（并且可能是已被最终确定的）；交叉阴影中的检查点是已被最终确定的（也被标记为 “F”）。

</figcaption>
</figure>

现在考虑相互冲突的已被最终确定的检查点 $a_m$。根据最终确定性的定义，必须存在一个从 $a_m$ 到下一时段中的 $a_{m+1}$ 的绝对多数链接 ${a_m \rightarrow a_{m+1}}$。显然，$a_m$ 和 $a_{m+1}$ 都不在集合 $\mathcal{B}$ 中，因为那会使 $a_m$ 成为 $b_n$ 的祖先并且不会发生冲突。同样，集合 $\mathcal{B}$ 中不包含检查点 $b_m$ 或 $b_{m+1}$，因为在一个时段中只能有一个被合理化的检查点。

根据这些观察，不可避免地是检查点对 $(a_m, a_{m+1})$ 位于 $\mathcal{B}$ 的两个连续元素的时段之间，比如 $b_{i_{j-1}}$ 和 $b_{i_j}$。也就是说，有一个 $j$ 使得 $i_{j-1} < m < m+1 < i_j$。

最后，我们可以看到，必须有一个环绕着绝对多数链接 ${a_m \rightarrow a_{m+1}}$ 的绝对多数链接 ${b_{i_{j-1}} \rightarrow b_{i_j}}$ 。除非至少 $\frac{1}{3}$ 的验证者违反了第二条 Casper 戒律，否则环绕或被环绕的链接就不可能存在，而我们假设验证者们没有这样做。

因此，我们已经证明了（通过反证法）如果少于 $\frac{1}{3}$ 的验证者（按权重计算）违反 Casper 戒律，则两个相互冲突的检查点不会同时被最终确定。

<a id="img_consensus_conflicting_finalised"></a>
<figure class="diagram" style="width: 85%">

![A diagram showing that finalising a conflicting checkpoint must involve a surround vote.](images/diagrams/consensus-conflicting-finalised.svg)

<figcaption>

假设一个较早的、相互冲突的检查点 $a_6$ 被最终确定。最终确定性意味着一定会有一个绝对多数链接 ${a_6 \rightarrow a_7}$。在 $b$ 链上的一个绝对多数链接——在这种情况下是 ${b_5 \rightarrow b_9}$——必须跨越 ${a_6 \rightarrow a_7}$。

</figcaption>
</figure>

在这个证明中，我们依赖于最终确定性的定义，即从已经被最终确定的检查点到下一个时段的检查点之间有一个绝对多数链接。事实证明，我们可以稍微放宽这个定义，并以 $k$-最终确定性的形式计算最终确定性。我们将在[下面](#k-finality)讨论这一点。

###### 经济最终确定性

可问责安全性的证明依赖于以下两点：

1. 没有两个绝对多数链接指向位于同一高度的不同检查点，且
2. 没有其中一个环绕另一个的两个绝对多数链接。

这些条件由 [Casper 戒律](#the-casper-commandments)强制执行。由于绝对多数链接需要 $\frac{2}{3}$ 验证者（按质押）的支持，如果存在违反这些规则的两个绝对多数链接，那么一定有至少 $\frac{1}{3}$ 的验证者同时为这两个链接投票。也就是说，至少 $\frac{1}{3}$ 的验证者进行了[双重投票](#no-double-vote)或[环绕投票](#no-surround-vote)。

正如我们[已经看到的](#slashing)，任何违反戒律的验证者都将面临罚没，即移除其部分或全部质押，并将其逐出验证者集合。因此，在发生相冲突的最终确定性的情况下，我们保证至少 $\frac{1}{3}$ 的质押以太币将被罚没。

通过这种方式，罚没给对链的攻击定了价，并且给对链的成功攻击设置了一个巨大且可计算的成本[^fn-current-cost-of-attack]。正如 Vitalik [所说](https://medium.com/@VitalikButerin/minimal-slashing-conditions-20f0b500fc6c)，

> 基本上，如果一个区块被最终确定，那么这个区块就是链的一部分，要改变这一点的代价非常非常高。

[^fn-current-cost-of-attack]: 截至我写作时（2023 年 7 月），信标链上有 2160 万个以太币被质押，因此最终确定性的反转将导致至少 720 万个以太币被罚没。以当前价格计算，这相当于 137 亿美元的应对反转的经济安全性。

我们称之为“经济最终确定性”。它不是由软件强制执行的最终确定性；而是由攻击成本强制执行的最终确定性。验证者的质押是一种“良好行为保证金”，如果证明他们违反了协议规则，这些保证金会被没收。验证者拥有签署其所有消息的私钥代表的唯一身份，因此可以具体追究个别验证者的责任，并进行惩罚。

你可能会想，为什么我们甚至需要经济最终确定性这样的概念。毕竟，PBFT 能够在没有这种构造的情况下实现最终确定性。协议不能直接拒绝对相冲突检查点的最终确定吗？

区别在于，PBFT 有一个奢侈的硬性的安全性假设，即少于三分之一的敌对验证者。同样地，在 Casper FFG 中，一个拥有少于三分之一质押的敌对者无法最终确定相冲突的检查点。然而，在无许可的区块链世界中，我们必须为超过三分之一质押由敌对方掌控的情况做好防御。这个防御措施就是罚没，它为我们提供了经济最终确定性的保证：如果超过三分之一的验证者意欲作恶，我们无法阻止他们最终确定相冲突的检查点，但我们可以对这种行为设置一个巨大成本。[^fn-economic-finality-pow]

[^fn-economic-finality-pow]: 这在概念上与工作量证明中发起 51% 攻击的成本有些相似。不同的是，在 PoW 中，一次成功攻击的成本可能为零，因为攻击者可以获得所有的区块奖励，并且可以使用相同的硬件无限次重复攻击。通过罚没提供的经济最终确定性要高得多。用 Vlad Zamfir 著名的话来说，被罚没就像是“如果你参与了 51% 攻击，你的 ASIC 农场就会被烧毁”。

在一篇博客文章《关于结算的最终确定性》（[On Settlement Finality](https://blog.ethereum.org/2016/05/09/on-settlement-finality)）中，Vitalik 如此描述：

> 我们不能保证 “X 永远不会被回滚”，但我们能保证稍弱一点的说法，即“要么 X 永远不会被回滚，要么一大群验证者将自愿摧毁他们自己的数百万美元的资本”。

面对超过三分之一的攻击者和异步网络，验证者仅仅在协议内拒绝最终确定相冲突的检查点是无效的。可以最终确定相冲突检查点的攻击依赖于将诚实验证者分隔开来，使他们看不到彼此的投票，也不知道另一方最终确定的是什么。在这种强力攻击的威胁下，经济最终确定性是一种强有力的安全性保证。在相冲突的最终确定性发生时的最终补救措施是手动干预。正如 Vitalik 在同一篇文章中所指出的，“链上资产的用户社区完全可以自由地应用常识来确定哪个分叉不是攻击，并实际代表最初被一致认可为被是最终确定的交易结果。”

##### 合理的活性

仅靠 Casper FFG 并不能提供经典意义上的活性，即确保用户的交易上链。所有区块生产和链构建的责任都在于底层共识机制，在以太坊中是 LMD GHOST。

然而，在某种意义上，我们希望 Casper FFG 是活跃的：我们总会希望，只要至少有三分之二的验证者是诚实的，并且没有任何验证者被罚没，我们就能继续合理化和最终确定检查点。相反，我们绝不希望陷入无法在不罚没诚实验证者的情况下确定新检查点的僵局。这与“最终会发生某些好事”的活性[定义](/part2/consensus/preliminaries/#safety-and-liveness)相一致。

用 [Vitalik 的话说](https://medium.com/@VitalikButerin/minimal-slashing-conditions-20f0b500fc6c)：

> 合理的活性基本上意味着“算法不应该陷入‘卡住’而无法最终确定任何事情的状态”。

Casper 论文中更正式的说法：

> 只要存在扩展已被最终确定的链的子检查点，就总能添加绝对多数链接以产生新的最终确定检查点。

证明如下。将会存在一个现有的最高合理化检查点 $a$，且将会有一个高度相同或更高（不一定是 $a$ 的后代）的检查点 $b$，这是任何验证者对其进行目标投票的最高检查点。

设 $c$ 是链上第 $h(b) + 1$ 时段（即紧接 $b$ 时段）中从 $a$ 派生的一个检查点。

所有验证者都可以投票给链接 ${a \rightarrow c}$ 而不必担心被罚没，因为，（1）既然之前没有验证者投票的目标是 $h(c)$，这就不可能是双重投票。并且（2）这不可能是环绕投票，因为诚实验证者之前使用的来源不会高于 $h(a)$，也不可能是被环绕的投票，因为现有链接的目标不会高于 $h(b)$。

<a id="img_consensus_plausible_liveness"></a>
<figure class="diagram" style="width: 65%">

![A diagram showing that we can always find a checkpoint descended from the highest justified that can be finalised without breaking a commandment.](images/diagrams/consensus-plausible-liveness.svg)

<figcaption>

投票给链接 ${a \rightarrow c}$ 是安全的，因为（1）没有人之前以 $c$ 为目标投票，因为 $b$ 拥有当下最高的目标投票，且（2）它不可能环绕另一个链接，因为没有比 $a$ 更高的，可以用作来源投票的合理化检查点。

</figcaption>
</figure>

因此，我们可以合理化检查点 $c$。显然，投票给 ${c \rightarrow d}$（其中 $d$ 是 $c$ 的直接子检查点）对所有验证者来说都是安全的，因此我们可以在不违反任何规则的情况下最终确定 $c$。

这种对合理的活性的要求是 Casper FFG [分叉选择规则](#fork-choice-rule)的基础：底层共识机制必须跟随包含最高已被合理化的检查点的链。只要底层链继续被构建在最高已被合理化的检查点之上，根据这个证明，我们就能保证在不罚没任何人的情况下继续最终确定其上的检查点。

#### 练习

作为插曲和一个有趣有用的练习，让我们来思考一下 Casper FFG 可能的不同表现。

例如，以下图示中的情况是如何出现的？即，绝对多数链接不断跳过检查点，导致一串已被合理化，但没有一个被最终确认的检查点。

<a id="img_consensus_exercise_0"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing a chain of justified checkpoints with supermajority links that continually skip a checkpoint.](images/diagrams/consensus-exercise-0.svg)

<figcaption>

总是在合理化但从未最终确定。我们怎么会陷入这种情况？

</figcaption>
</figure>

答案在[下面](#answer-to-the-exercise)，但先花点时间思考一下可能导致这种后果的情形。

#### Casper FFG 杂项

##### 激励

在 LMD GHOST 中，我们看到被包含在下一个时隙的区块中的正确头块投票会得到奖励。但对于延迟或不正确的头块投票则没有惩罚。

在 Casper FFG 的实现中，奖励和惩罚要复杂一些。验证者被[充分激励](/part2/incentives/rewards/#introduction)以去做出准确和及时的 Casper FFG 投票：验证者潜在质押奖励的 22% 来自来源投票，41% 来自目标投票。此外，不准确或延迟的来源或目标投票会被施以同等额度的惩罚。

为了获得任何 Casper FFG 奖励，验证者的来源投票必须是正确的。也就是说，它必须与最终汇聚的正统链的历史一致。如果源投票不正确，那么就如同验证者根本没有投票一样，并且它会因错过来源和目标投票而受到完整的惩罚。这是因为错误的来源意味着它之前处理的是一个最终没有成为正统的分支。因此，该投票是在与共识竞争，而不是支持共识。

为了获得来源投票的奖励，来源投票必须是正确的，并且必须是及时的。如果验证者的投票在五个时隙内被包含在区块中，那么它会获得来源投票奖励，否则它会受到同等大小的惩罚。在[规范注解](/part2/incentives/rewards/#timeliness)中有一些关于五个时隙限制原因的讨论。

为了获得目标投票奖励，目标投票必须是正确的，并且也必须是及时的。如果验证者的投票在三十二个时隙内被包含在区块中，那么它会获得目标投票奖励，否则它会受到同等大小的惩罚[^fn-deneb-change-to-target-inclusion-time]。

[^fn-deneb-change-to-target-inclusion-time]: 这将在 Deneb 升级时发生[改变](https://github.com/ethereum/consensus-specs/pull/3360)。当前或上一个时段的目标投票都将是有效的，而不是在 32 个时段后失效。

对于投票的不同程度的正确性和及时性，规范注解中有一个[完整的奖惩矩阵](/part2/incentives/penalties/#penalties-rewards-table)。


##### 动态验证者集合

在上述讨论中，我们假设 Casper FFG 协议中使用的是静态验证者集合，即假定没有验证者加入或退出协议。然而，这并不完全符合实际，因为我们希望能够引入新的质押者并允许质押者退出。

Casper FFG 论文讨论了当验证者集合在每个时段发生变化时，如何维持可问责的安全性。它通过前置和后置验证者集合的概念去分析这一点。以太坊 2.0 的实现忽略了这种机制，转而通过严格限制验证者的激活和退出来解决这个问题。在每个时段中，我们允许的验证者激活和停用的数量大约占总验证者的 0.0015%（参见 [`CHURN_LIMIT_QUOTIENT`](/part3/config/configuration/#validator-cycle)）。

[Gasper 论文](https://arxiv.org/pdf/2003.03052.pdf)第 8.6 节分析了这种简化的效果。通过限制验证者的进出频率，但不考虑前置和后置验证者集合，我们在微微降低了可问责的安全性。也就是说，在最终确定一个相互冲突的检查点时，可能会有略少于三分之一的质押被罚没。具体来说，如果两个相互冲突的、被最终确定的检查点之间的验证者集合在质押数量上相差 $\varepsilon$，那么经济最终确定性（即将被罚没的最低质押数量）将变为 $\frac{2}{3} - \varepsilon$，而不是 $\frac{2}{3}$。在实践中，由于验证者集合变化的频率限制非常严格，这种差异可以忽略不计。

##### k-最终确定性（k-finality）

在最初的 Casper 论文中，要最终确定一个检查点，我们必须有一个从该检查点到其直接后代的绝对多数链接。事实证明，我们可以在不影响安全性证明有效性的情况下将其普遍化。

安全性证明所依赖的关键观察是，在跨度为绝对多数链接 ${a_m \rightarrow a_{m+1}}$，最终确定了 $a_m$ 的 $\mathcal{B}$ 的两个连续成员之间存在一个绝对多数链接。然而，如果存在一个绝对多数链接 ${a_m \rightarrow a_{m+k}}$，且 $a_m$ 到 $a_{m+k}$ 之间的所有检查点在 $a$ 分支上都被合理化了，那么 $b$ 分支上的周边链接仍然必须存在。因为，如果是这样，在第 $m$ 到第 $m+k$ 个时段之间， $\mathcal{B}$ 就不会有成员。

<a id="img_consensus_k_finality_proof"></a>
<figure class="diagram" style="width: 85%">

![A diagram showing that we can safely finalise a checkpoint with a supermajority vote than spans multiple contiguous justified checkpoints.](images/diagrams/consensus-k-finality-proof.svg)

<figcaption>

如果一个绝对多数链接从某个检查点跳过多个被证明的检查点，所有可追责安全性证明的保证仍然成立。在这里，$a_5$ 和 $a_6$ 是被证明的，所以我们可以用超多数链接 ${a_4 \rightarrow a_7}$ 安全地最终确定 $a_4$。

</figcaption>
</figure>

所以，通用的最终确定性规则是，当我们有一个绝对多数链接 ${a_m \rightarrow a_{m+k}}$，并且检查点 $a_{m+1}$, $a_{m+2}$, $\ldots$, $a_{m+k-1}$ 都被证明是合理的时，我们可以最终确定检查点 $a_m$。

这被称为 $k$-最终确定性，并在 [Gasper 论文](https://arxiv.org/pdf/2003.03052.pdf) 的第 4.5 节中被讨论。

在计算最终确定性时，考虑多少个检查点 $k$ 取决于准备记录多少内容。在以太坊 2.0 信标链上，我们采用 $2$-最终确定性：我们记录四个连续时段的合理性状态，并允许处理两个时段的目标投票（更旧的目标投票被视为无效）。

<a id="img_consensus_2_finality"></a>
<figure class="diagram" style="width: 65%">

![A diagram of the four 2-finality scenarios.](images/diagrams/consensus-2-finality.svg)

<figcaption>

2-最终确定性的四种情况。在每种情况下，绝对多数链接都会使其起始的检查点（源）最终确定，并使其结束的检查点（目标）得到合理化。情况 2 和情况 4 是经典的 $1$-最终确定性。检查点编号沿底部排列。

</figcaption>
</figure>

几乎总是，我们只会看到 $1$-最终确定性的情况，特别是情况 4。$2$-最终确定性的情况只会在许多认证被延迟或我们非常接近三分之二的参与阈值时发生。请注意，这些评估是堆叠的，因此规则 2 可以最终确定 $C_{n-2}$，然后规则 4 可以立即最终确定 $C_{n-1}$。

详细机制是在时段处理期间通过 [`weigh_justification_and_finalization()`](/part3/transition/epoch/#def_weigh_justification_and_finalization) 函数执行的[^fn-danny-k-finality-video].

[^fn-danny-k-finality-video]: Danny Ryan 在 Devcon V 的[这个视频](https://www.youtube.com/watch?v=N5DdClfLQfw&t=601s)中简要讨论了 $k$-最终确定性。

##### 为什么是三分之二？

最终确定的 $\frac{2}{3}$ 多数阈值是从哪里来的？这其实[并不明显](https://ethresear.ch/t/latest-casper-basics-tear-it-apart/151/58?u=benjaminion)，我们本可以选择一个不同的值来定义绝对多数链接。让我们把这个阈值称为 $p$：如果有 $p$ 比例的验证者集投票来最终确定一个检查点，那么它就被最终确定了。

我们试图在两个因素之间取得平衡。一方面，比例为 $1-p$ 的敌对或有故障的验证者可以阻止最终确定，这是一种活性失败。

另一方面，我们希望最大化最终确定性的可问责安全性。也就是说，为了最终确定相互冲突的检查点，有多少比例的质押必须模棱两可。这一比例是 $2p-1$。

在这些约束条件下，设置 $p = \frac{2}{3}$ 可以最大化对活性攻击的容错能力——少于三分之一的验证者不能阻止最终确定；同时最大化对安全性故障的容忍度——如果最终确定了相冲突的区块，至少三分之一的验证者将被罚没。

<a id="img_consensus_two_thirds"></a>
<figure class="diagram" style="width: 40%">

![Graph showing the tradeoff between liveness fault tolerance and safety fault tolerance.](images/diagrams/consensus-two-thirds.svg)

<figcaption>

为了同时最大化可问责的安全性和对活性故障的容忍度，最佳阈值是 $p = \frac{2}{3}$。

</figcaption>
</figure>

Vitalik 在《最小的罚没条件》（[Minimal Slashing Conditions](https://medium.com/@VitalikButerin/minimal-slashing-conditions-20f0b500fc6c#44b0)）文章的脚注 2 中对此做了一些解释。

##### 如何避免被罚没

Casper 的两条戒律非常简单，原则上避免被罚没也很容易：只需遵守规则。

至今为止，验证者被罚没的最常见原因是其秘钥同时在不同节点上运行。如果节点对网络的视图有任何分歧——例如，一个节点比另一个节点更晚看到某个检查点区块——那么验证者的每个实例都可能会为同一个时段签署不同的投票，从而违反第一条戒律。

避免这种情况的基本方法就是不要这样做——任何时候都只在一个地方运行你的私钥。客户端软件实现通常提供防御机制，例如[二重奏检测]（[doppelganger detection](https://docs.teku.consensys.net/how-to/enable-doppelganger-detection)），来防止质押者在无意中犯错。在开始签署认证前，二重奏检测会等待几个时段；如果在此期间它在链上看到来自同一密钥的其他实例的签名，它将拒绝启动。另一种方法是将对投票的签署委托给一个维护你验证者过去投票数据库并且会拒绝签署任何违反戒律的内容的中心化签署者。[Web3Signer](https://docs.web3signer.consensys.net/) 就是这样的签名服务。

在极少数情况下，即使是单个实例的验证者也可能进入可能导致可被罚没的认证的情况。例如，如果主机的时钟跳到之前的一个时间，或当链出现超过一个时段的长时间回滚时，验证者的职责可能会被重新计算。它可能发现由于其过去的职责，它已经在该时段内投票一次，而新的职责要求它在该时段再次投票，这将导致可被罚没的双重投票。

出于这些原因，所有客户端软件都包含防止罚没的机制。不同的客户端采取不同的方法，但有一个约定的通用交换格式，[EIP-3076](https://eips.ethereum.org/EIPS/eip-3076)，用于对保护罚没的数据的迁移。如果有必要，也可以用于在客户端之间迁移。Teku 对此采取了非常稳健、极简的方法。对于由 Teku 节点管理的每个验证者，它会[维护一个文本文件](https://docs.teku.consensys.net/concepts/slashing-protection#validator-slashing-protection-file)，记录该验证者至今为止的最高来源投票和最高目标投票的时段编号。要签署新的认证，其来源不得低于已存储的来源，目标必须高于已存储的目标。这足以保证 Teku 验证者的单个实例不会为同一目标时段进行双重投票，也不会进行环绕投票。

##### Casper FFG vs PBFT

正如 Vitalik [所承认的](https://web.archive.org/web/2/https://nitter.it/VitalikButerin/status/1029903234226216960#m)，Casper FFG 的技术根源是 20 世纪 80 年代和 90 年代开发的经典 BFT（Byzantine fault tolerant，拜占庭容错）共识协议。尤其是，它与 1999 年发布的 PBFT（[Practical Byzantine Fault Tolerance](https://www.scs.stanford.edu/nyu/03sp/sched/bfs.pdf)，实用拜占庭容错）算法有一些相似之处。

然而，Casper FFG 并不是 PBFT，两者之间有一些显著的区别。以下内容并非严格的比较，但涉及主要的点。

PBFT 和 Casper FFG 都基于“轮次”，并且涉及两阶段承诺过程。在 Casper FFG 中，一个完整的轮次有两个时段，但轮次相重叠或呈流水线式，所以一个轮次的 `PREPARE` 步骤（合理性）与前一个轮次的 `COMMIT` 步骤（最终确定性）相重合。这种重叠允许 Casper FFG 只使用一种消息类型（认证），该消息包含两个投票（来源和目标）。经典的 PBFT 依赖副本（验证者）广播单独的 `PREPARE` 和 `COMMIT` 消息，且其轮次严格按照顺序。

PBFT 和 Casper FFG 都在某种程度上依赖于一个领导者。在Casper FFG 中，领导者是底层共识机制的区块提议者，并且每个轮次都会更换。而在 PBFT 中，领导者被称为初选提名人，只有在其他副本认为其离线或出现故障时才会更换。PBFT 有一个完整的“视图切换”机制来处理必要时的领导者更换。

重要的是，如果超过三分之一的验证者离线，PBFT 将陷入停滞，因为它将无法执行视图切换。在这种情况下，区块生产将完全停止，这是 PBFT 及其近亲们为了安全性而牺牲活性的结果。如果超过三分之一的验证者离线，Casper FFG 也会停滞不前，因为将无法进行最终确定。然而，这并不会阻止底层的链继续前进，从而为整个系统提供活性。因为 Casper FFG 的性质是作为一个覆盖层，运行在基础的区块提议机制之上。

最终，每种协议的安全性保证是不同的。PBFT（实用拜占庭容错算法）中的安全性（最终确定性）保证是：当不到三分之一的副本出现故障时，一个轮次的输出永远不会被更改。当然，这一切依然取决于社会共识。PBFT 是一种需要许可的协议，属于权威证明（proof of authority）的范畴。如果所有被授权的参与者合谋以更新各自软件，酒可以很轻松地回滚系统状态。

Casper FFG 中的安全性增加了一层加密经济学保证：除非燃烧至少三分之一的质押，否则相冲突的检查点无法被最终确定。这种保证类型与前者具有本质上的不同，但与以太坊的权益证明协议的无需许可性质非常契合。

##### Casper 戒律是否最优？

针对这两条 [Casper 戒律](#the-casper-commandments)是否理想曾发生过一个有趣的讨论。例如，有一些情况，比如[上面](#img_consensus_commandment_2a)显示的第一个环绕投票，是无害的，但仍会导致验证者被罚没。Daniel Lubarov 在一篇[帖子](https://ethresear.ch/t/casper-ffg-leniency-tweak/2286?u=benjaminion)中讨论了另一种类似的情况，建议用“验证者必须被禁止在另一个投票的范围内投下最终确认的票”（我们目前禁止所有跨越投票）来代替第二条戒律。

类似地，Justin Drake 提出了一个[紧凑且直观的 Casper 罚没条件](https://ethresear.ch/t/a-tight-and-intuitive-casper-slashing-condition/3359?u=benjaminion)，将两条戒律统一为一条。

> 验证者不得投出一个“跳过”其最终确认投票 ${\tilde{s} \rightarrow \tilde{t}}$ 的票 ${s \rightarrow t}$，即 $h(s) \le h(\tilde{s})$ 且 $h(t) \ge h(\tilde{t})$ [并且这些目标投票相冲突]。

Jacob Eliosoff [更进一步](https://ethresear.ch/t/simplifying-casper-votes-to-remove-the-source-param-take-two/6398?u=benjaminion)建议完全去掉来源投票，并相应地重新制定罚没规则。

虽然这些建议值得思考，但我认为它们未能获得广泛支持的原因是：基于“如果没有坏，就不要动它”的原则，目前的方案已经足够好。在实现、分析和正式验证现有版本的 Casper FFG 方面已经做了大量工作，而现在做任何不会带来巨大收益（例如单时隙最终确定性）的改变，可能都不划算。

#### 相冲突的合理性

如果你想测试自己在分布式共识方面的推理能力——这并不容易——值得思考为什么我们同时需要“已合理化的”和“已最终确定的”状态。为什么我不能立即将任何获得了 $\frac{2}{3}$ 绝对多数票的检查点标记为已被最终确定？

关键在于，合理性是局部属性；最终确定性是全局属性。

“合理化”一个检查点，意味着我已经听到 $\frac{2}{3}$ 的验证者认为这个检查点是好的。但这只是我的本地视图，因为其他验证者可能听到了不同的信息——但我不知道他们是否听到。不过，作为一个诚实的验证者，我承诺永远不会撤销任何在我的本地视图中已被合理化的检查点。

“最终确定”一个检查点，意味着我已经听到 $\frac{2}{3}$ 的验证者说他们听到 $\frac{2}{3}$ 的验证者认为这个检查点没问题。我现在知道 $\frac{2}{3}$ 的验证者知道 $\frac{2}{3}$ 的验证者已经将该检查点标记为具有合理性，因此全局中绝对多数的验证者已经承诺永远不撤销它。该检查点在全局范围内是安全的，不会被撤销。

这有点令人费解，所以这样也许会更好：通过了解如果我们不这样来来回回地确认每个人都确认了我所确认的内容，而可能导致失败的方式[^fn-blue-eyes-puzzle]。

[^fn-blue-eyes-puzzle]: 如果你真的想测试自己对所有这些“我确认每个人都确认了每个人都确认”的理解——这是共识安全的核心内容——那么我强烈推荐你尝试一下[蓝眼睛谜题](https://xkcd.com/blue_eyes.html)。我记得是 Joseph Poon 在 2018 年初介绍给我这个谜题。我花了好几个小时的抓头挠腮才搞明白，但努力是值得的。解决方案是存在的，但我建议在查看答案之前先尽力挑战一下这个谜题。 

##### 简化的模型

让我们考虑一个极端的情况。假设有四个验证者，$A$、$B$、$C$ 和 $D$。他们都是诚实的，但网络可能会遭遇无限期的延迟。为了便于说明，我们将在每个区块高度设置一个检查点。我们就随便地从 $0$ 开始为时段编号，但区块 $0$ 并不是创世区块——它可以是，但那样会稍微改变接下来的描述。

###### 时段 1 - 设定

<a id="img_consensus_conflicting_justification_0"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing validator A and validators B, C and D having a common view in epoch 1.](images/diagrams/consensus-conflicting-justification-0.svg)

<figcaption>

在时段 $1$，区块 $1$ 包含足够的投票以合理化检查点 $0$。所有人都投票 ${0 \rightarrow 1}$。

</figcaption>
</figure>

请记住，只有包含在区块中的 Casper FFG 投票才会影响合理性和最终确定性。一开始，所有验证者都有一个共同的视图。他们都看到区块 $1$，并且它包含足够的投票来证明检查点 $0$ 的合理性。截至目前一切正常。四个验证者都做出相同的投票决定 $0 \rightarrow 1$：他们的来源检查点是 $0$，这刚被合理化；他们的目标检查点是 $1$，即当前时段的检查点。这是一个绝对多数投票，应当使检查点 $1$ 被所有人合理化。

###### 时段 2 - 不一致的合理化

问题出在这里。在时段 $2$，验证者 $A$ 被选中去提议，但由于某种原因，$A$ 提议的区块没有被其他人看到——可能是由于拒绝服务攻击导致的严重网络延迟。因此，验证者 $B$、$C$ 和 $D$ 没有看到它包含的、证明了检查点 $1$ 合理性的绝对多数链接，所以他们不知道其他人是否支持检查点 $1$。现在我们有了分裂的视图：验证者 $A$ 证明了检查点 $1$ 的合理性，并且不可逆地支持它；其他验证者仍然认为检查点 $0$ 是最高的合理检查点。

<a id="img_consensus_conflicting_justification_1"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing that the views of validator A and of validators B, C and D have diverged in epoch 2. Validator A has justified checkpoint 1, but the others have not.](images/diagrams/consensus-conflicting-justification-1.svg)

<figcaption>

在时段 $2$，验证者 $A$ 提议了区块 $2$。它包含了所有四个 ${0 \rightarrow 1}$ 投票，但验证者 $B$、$C$ 和 $D$ 从未看到它。验证者 $A$ 看到一个绝对多数链接 ${0 \rightarrow 1}$，所以它将检查点 $1$ 标记为合理。验证者 $B$、$C$ 和 $D$ 在当下时段没有看到投票，他们最合理的检查点仍然是 $0$。$A$ 投票 ${1 \rightarrow 2}$；$B$、$C$ 和 $D$ 投票 ${0 \rightarrow X}$，其中 $X$ 是一个空检查点。

</figcaption>
</figure>

问题的根源在于 $A$ 看到了其他人都同意，但其他人没有看到这一共识的相关证据。对于 $B$、$C$ 和 $D$ 来说，他们都只能依靠自己。除了暂时保持现状，并尝试在下一轮达成共识之外，他们别无选择。[^fn-less-contrived]

[^fn-less-contrived]：他们很可能通过广播协议看到彼此的投票。且在更现实的模型中，每个时段里有多个区块的情况下，他们很可能会在后续区块中包含这些投票。但这些并不会保证发生，而且在这个被有意简化的模型中，不可能发生这些。

现在所说的对本例来说并不重要，但值得注意的是，通过证明检查点 $1$ 的合理性，验证者 $A$ 将使检查点 $0$ 最终确定。验证者 $A$ 知道检查点 $0$ 将永远不会在全局范围内被回滚，因为它知道至少 $\frac{2}{3}$ 的验证者认为它是合理的（他们将其作为来源投票），因此无论发生什么，它们永远不会回滚它。

至于在时段 $2$ 的投票，验证者 $A$ 将投票 ${1 \rightarrow 2}$，而其他三个验证者将投票 ${0 \rightarrow X}$，其中 $X$ 表示他们认为时段 2 的检查点为空。我们尚未介绍空检查点——将在介绍 Gasper 时讨论它——目前你可以理解为对空检查点 $X$ 的投票表明时段 $2$ 的链头仍然是区块 $1$。

###### 时段 3 - 相冲突的合理性

现在假设是 $B$、$C$ 或 $D$ 中的一个提议了时段 $3$ 中的区块。它包含三个 ${0 \rightarrow X}$ 的投票，但不能包含 $A$ 的投票，因为它有着不同的来源。反过来，验证者 $A$ 将认为区块 $3$ 是无效的，因为它所包含的认证的来源不是检查点 $1$，即 $A$ 的最高合理检查点。在 $B$、$C$ 和 $D$ 的视图中，这三个投票足以形成绝对多数，因此他们适当证明了检查点 $X$ 的合理性。

<a id="img_consensus_conflicting_justification_2"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing that the views of validator A and of validators B, C and D have diverged further in epoch 3. Validator A has justified checkpoint 1, but the others have justified an empty checkpoint in epoch 2.](images/diagrams/consensus-conflicting-justification-2.svg)

<figcaption>

在时段 $3$，$B$、$C$ 或 $D$ 中的一个发布了区块 $3$。它包含了三个 ${0 \rightarrow X}$ 的投票，所以验证者 $B$、$C$ 和 $D$ 有一个绝对多数链接来证明空检查点 $X$ 的合理性。验证者 $A$ 认为区块 $3$ 是无效的。

</figcaption>
</figure>

此时，验证者 $A$ 的情况是不可恢复的。$BCD$ 链将永远不会以检查点 $1$ 作为来源以进行投票，所以验证者 $A$ 将永远不会认为他们的认证或区块是有效的，反之亦然。在 $A$ 的链上合理性无法进展，因为它只能看到 $\frac{1}{4}$ 的投票权重。唯一的补救措施是让托管 $A$ 的节点擦除其数据库并重新同步到正统链。然后验证者 $A$ 能够照常重新加入而不会有违反 Casper 戒律的风险。[^fn-extreme-and-contrived].

[^fn-extreme-and-contrived]：需要意识到这是一个非常极端且生造的例子。在实际情况中，发生类似事情的可能性非常小。

请注意，我们还没有回答自己的问题。如果验证者 $A$ 立即将区块 $1$ 标记为最终确定而不仅仅是合理呢？在这个例子中，不会产生实质性差异。区块 $1$ 最终出现在由 $B$、$C$ 和 $D$ 推动进展的正统链中，所以将其标记为最终确定是没有问题的。

##### 引入敌对行为

为了说明跳过合理性步骤是危险的，我们将引入一些敌对行为。让我们假设 $C$ 和 $D$ 是不诚实的验证者。

当 $B$、$C$ 和 $D$ 都是诚实的验证者时，如上所述，他们将在时段 $2$ 投票支持从区块 $1$ 衍生的空检查点 $X$，然后在时段 $3$ 证明其合理性。这将肯定使他们把区块 $1$ 包含在他们的正统链中，因为 Casper FFG 分叉选择现在会阻止他们构建一个相冲突的分支。

然而，如果 $B$、$C$ 和 $D$ 中的多数是不诚实的，他们可以通过不投票来使检查点 $X$ 无法合理化，并在时段 $3$ 中选择在区块 $0$ 上构建一个新分支，区块 $0$ 仍然是他们最高的合理检查点。这样做的话，他们将孤立区块 $1$。关键是，他们可以在不违反任何罚没规则的情况下这样做。

<a id="img_consensus_conflicting_justification_3"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing that dishonest validators can orphan block 1.](images/diagrams/consensus-conflicting-justification-3.svg)

<figcaption>

当 $B$、$C$ 和 $D$ 中的大多数不诚实时，他们可以不合理化检查点 $X$ 并构建一个孤立区块 $1$ 的分支。

</figcaption>
</figure>

现在我们切入正题。如果验证者 $A$ 立即将区块 $1$ 标记为最终确定，那么对于任何依赖于验证者 $A$ 的节点获取链上信息的应用程序或用户来说，这将是灾难性的。他们会看到一个被最终确定的区块，而这个区块在由 $B$、$C$ 和 $D$ 维护的后续正统链中从未出现过。也就是说，他们会看到一个被“最终确定的”区块回滚而没有任何罚没，这违反了 Casper FFG 可问责安全性的保证。[^fn-ffg-split-views]

[^fn-ffg-split-views]: 如果我们假设从一开始 $C$ 和 $D$ 就被对手控制，并且对手对 $A$ 或 $B$ 看到他们的区块有一定的控制权，那么对手很容易将 $A$ 和 $B$ 的视图分裂，使他们分别证明在不同分支上的检查点的合理性。我将留给读者自己去做这个练习——流程与上述情况相似。

请注意，我们是从敌对行为的角度来描述这个问题的。但实际上，对不诚实的假设并不是证明我们观点的必要条件。也许是网络延迟阻止了 $C$ 和 $D$ 在时段 $2$ 中的投票上链。显然，并不需要超过三分之一的不诚实验证者，过早进行最终确定会带来糟糕的结果，而这是相当灾难性的。

总之，唯一能保证安全性的方法是通过两阶段承诺。这保证了被标记为最终确定的区块将始终出现在正统链中，除非至少三分之一的验证者被罚没。

##### 总结与反思

通过这个简单模型，我们看到只有使用两阶段承诺才能实现 Casper FFG 的保证。当我们看到绝对多数链接时，不能跳过合理性直接进入最终确定。

即使在这个极度简化的信标链模型中，推理也相当困难。但这所有的确认和再确认正是分布式系统的本质。如果我们有一个可信的中央权威，一切都会容易得多。例如，在国家领导人的选举中，我们都去投票，然后中央权威统计并公布结果——这是一个单轮过程。但如果中央权威是腐败的，后果可能非常严重。两阶段承诺是让我们的协议不受腐败影响的成本。

#### Casper FFG 的历史

以太坊 2.0 权益证明共识协议的开发历史很长，Vitalik 在一个[推文串](https://web.archive.org/web/20230630135150/https://nitter.it/VitalikButerin/status/1029900695925706753)（汇总[在此](https://hackmd.io/@liangcc/BJZDR1mIX?type=view)）中做了很好的总结，Vlad Zamfir 的回忆录中也有提及。

其起源可以追溯到 2014 年 1 月，Vitalik 的《罚没者：惩罚性的权益证明算法》（[Slasher: A Punitive Proof-of-Stake Algorithm](https://blog.ethereum.org/2014/01/15/slasher-a-punitive-proof-of-stake-algorithm)）一文。虽然 Slasher 算法中的几乎所有内容都没有被使用，但它引入了惩罚违反协议规则的验证者的想法，从而解决了我们在LMD GHOST 中[讨论](/part2/consensus/lmd_ghost/#slashing-in-lmd-ghost)的无利害关系问题。罚没机制为[经济最终确定性](#economic-finality)的想法铺平了道路。

2015 年，Vitalik 正在研究打赌共识（[consensus by bet](https://blog.ethereum.org/2015/12/28/understanding-serenity-part-2-casper)），其中一个版本出现在 2016 年的 [Ethereum.org 2.0 Mauve Paper](https://docs.google.com/document/d/1maFT3cpHvwn29gLvtY4WcQiI6kRbN_nbCf3JlgR3m_8/edit#heading=h.v9zt0v8gvrdh) 中。他将打赌共识[描述](https://web.archive.org/web/20230630150818/https://nitter.it/VitalikButerin/status/1029902353703391233#m)为“一条漫长且最终无果的切线”。然而，打赌共识确实开创了在可分叉的区块生产机制上回溯性地赋予最终确定性的想法，这正是 Casper FFG 的本质。

我们今天拥有的 Casper FFG 机制真正开始成型是在 2017 年，Vitalik [回到](https://web.archive.org/web/20230630151614/https://nitter.it/VitalikButerin/status/1029903234226216960#m)经典的 PBFT 文献之后。虽然还没有使用“合理化”这一术语，而是仍然倾向于使用 PBFT 的 `PREPARE` 和 `COMMIT`；但[所涌现的设计](https://medium.com/@VitalikButerin/minimal-slashing-conditions-20f0b500fc6c)我们如今已经相当熟悉。它提供的一个真正新颖的特性是通过使用罚没条件以实现经济最终确定性。当时有四个罚没条件，后来被[减少到](https://web.archive.org/web/20230630153358/https://nitter.it/VitalikButerin/status/1029903583897051136#m)两个[^fn-four-to-two-casper]，只有[一种消息类型](https://ethresear.ch/t/casper-ffg-with-one-message-type-and-simpler-fork-choice-rule/103?u=benjaminion)。

[^fn-four-to-two-casper]: [Casper FFG 论文](https://arxiv.org/pdf/1710.09437.pdf) 中的第四个脚注解释了将消息类型从四个减少到两个的背后考量。 

在这一时期，Vitalik 的 Casper FFG 和 Vlad Zamfir 的 Casper CBC 共识协议一起成长。正如 Vitalki 在 2016 年 12 月的一篇[博文](https://blog.ethereum.org/2016/12/04/ethereum-research-update)中所述，Casper FFG 的设计要旨是“创建一个简单的权益证明协议，在尽可能不改变工作量证明的情况下去提供理想属性”。他将这种典型的实用主义方法与 Zamfir 更为纯粹的“从头开始重建共识”的愿望进行了对比。令人困惑的是，除了通用的权益证明内容之外，这两种 Casper 协议几乎没有共同之处。不过，Vitalik 偶尔还是会表示，希望以太坊[最终转向](https://www.reddit.com/r/ethereum/comments/ajc9ip/comment/eeudjjw/)类似 Casper CBC 的协议。
《友好的最终确定性小工具 Casper》（[Casper the Friendly Finality Gadget](https://arxiv.org/abs/1710.09437)）论文的第一版于 2017 年 10 月上传。该论文由 Vitalik 和 Virgil Griffith 撰写，几乎描述了我们今天使用的方案——我们将在[下一节](/part2/consensus/gasper/)中介绍针对 Gasper 的修改。

如[前](/part2/consensus/overview/#history)所述，最初的计划是将 Casper FFG 作为以太坊现有工作量证明协议的覆盖层。这项工作的进展相当顺利，[EIP-1011](https://eips.ethereum.org/EIPS/eip-1011) 测试网已于 2017 年 12 月 31 日[上线](https://web.archive.org/web/20230630135033/https://nitter.it/karl_dot_tech/status/947503029166546946)。

PoW 覆盖计划在 2018 年被放弃，转而通过运行 Gasper（LMD GHOST 加 Casper FFG）共识架构的新信标链架构直接转向完全的权益证明，这就是我们今天的架构。

Casper FFG 在信标链上的初始规范是在 [Ethresear.ch](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760?u=benjaminion) 上维护的（你可以在那里看到文档历史）。它包括 $k$-最终确定性（Casper FFG 分叉选择规则的一个略微奇怪的版本），以及我们今天使用的两条 Casper 戒律。然而，信标链现在的一个时段是 32 个时隙，而非 64 个，我们也没有实现动态的验证者集机制。

当前的 Casper FFG 规范被作为信标链状态转换函数中的[时段处理](/part3/transition/epoch/)的一部分而维护。

#### 练习的答案

以下是[上方](#exercise)练习的答案。

<details>
<summary>Answer</summary>

假设认证总是延迟整一个时段，那么在时段 $N$ 所做的投票要到时段 $N+1$ 才被能处理，请跟随下面的 Casper FFG 算法进程。

<a id="img_consensus_answer_0"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing validators voting for a supermajority link from checkpoint 0 to checkpoint 1.](images/diagrams/consensus-answer-0.svg)

<figcaption>

我们从检查点 0 被合理化开始。在时段 1 期间，每个人都像往常一样投票 ${0 \rightarrow 1}$。绝对多数链接投票显示为虚线，因为对投票的处理将延迟到下一个时段。

</figcaption>
</figure>

<a id="img_consensus_answer_1"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing validators voting for a supermajority link from checkpoint 0 to checkpoint 2.](images/diagrams/consensus-answer-1.svg)

<figcaption>

在时段 1 结束时，由于延迟，我们尚未看到任何投票，因此检查点 1 仍未被合理化。在时段 2 期间，每个人都投票 ${0 \rightarrow 2}$。

</figcaption>
</figure>

<a id="img_consensus_answer_2"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing validators voting for a supermajority link from checkpoint 1 to checkpoint 3.](images/diagrams/consensus-answer-2.svg)

<figcaption>

在时段 2 结束时，我们可以处理时段 1 的投票，从而合理化检查点 1（并最终确定检查点 0，但这无关紧要）。检查点 2 仍未被合理化，因为我们还没有看到时段 2 的投票。在时段 3 期间，每个人都投票 ${1 \rightarrow 3}$。

</figcaption>
</figure>

<a id="img_consensus_answer_3"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing validators voting for a supermajority link from checkpoint 2 to checkpoint 4.](images/diagrams/consensus-answer-3.svg)

<figcaption>

由于延迟的认证，合理化继续滞后一个时段。在时段 4 期间，每个人都投票 ${2 \rightarrow 4}$。

</figcaption>
</figure>

<a id="img_consensus_answer_4"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing a string of justified blocks with supermajority links that skip alternate checkpoints.](images/diagrams/consensus-answer-4.svg)

<figcaption>

现在，我们被永远困在这种跳蛙行为中：由于被跳过的检查点的合理化总是晚一个时段，投票也总是跳过一个检查点。

</figcaption>
</figure>

</details>

#### 另见

最初的《友好的最终确定性小工具 Casper》（[Casper the Friendly Finality Gadget](https://arxiv.org/pdf/1710.09437.pdf)）仍然是权威的参考。尽管以太坊 2.0 中的实现细节在某些方面有所不同，但根基是一样的。

那篇论文还讨论了使用“怠惰惩罚”来从灾难性崩溃中恢复（第 4.2 节）。我已经在[其他地方](/part2/incentives/inactivity/)介绍了我们如何在 Eth2 中实现怠惰惩罚。

再一次，Vitalik 关于 Casper 历史的[推文风暴](https://web.archive.org/web/20230630135150/https://nitter.it/VitalikButerin/status/1029900695925706753)（也可在[这里](https://www.trustnodes.com/2018/08/16/vitalik-buterin-tells-story-race-vlad-zamfir-implement-proof-stake-casper)和[这里](https://hackmd.io/@liangcc/BJZDR1mIX?type=view)查看）为 Casper 共识协议、CBC 和 FFG 开发的背景提供了绝佳的第一手资料。

在共识层规范中：

  - 合理化和最终确定的计算是在时段处理期间执行的，入口点是 [`process_justification_and_finalization()`](/part3/transition/epoch/#def_process_justification_and_finalization)，主要工作由 [`weigh_justification_and_finalization()`](/part3/transition/epoch/#def_weigh_justification_and_finalization) 完成，包括处理 $k$-finality。
  - 奖励和惩罚是由 [`process_rewards_and_penalties()`](/part3/transition/epoch/#def_process_rewards_and_penalties) 在时段处理期间应用的，但这只是简单地累加由 [`process_attestation()`](/part3/transition/block/#def_process_attestation) 计算的、一个区块一个区块的奖励。
  - Casper FFG 对违规行为的罚没由 [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing) 处理。

我找到的一篇关于 Casper FFG 的好文章来自于 [Juin Chiu](https://medium.com/unitychain/intro-to-casper-ffg-9ed944d98b2d)。这篇文章对 Casper FFG 与经典 PBFT 之间的关系解释的非常好。Vitalik 的《极简的罚没条件》（[Minimal slashing conditions](https://medium.com/@VitalikButerin/minimal-slashing-conditions-20f0b500fc6c)）文章也包含许多洞见（即使这些罚没条件并非极简）。

一些关于 Casper FFG 保证的正式验证工作（根据原始论文的内容，没有如 $k$-finality）在 2018 年的《Coq 证明助手对 Casper 的验证》（[Verification of Casper in the Coq Proof Assistant](https://core.ac.uk/download/pdf/161954227.pdf)）论文中有所描述。文章中有一些有用的见解，特别是澄清了可能的活性证明背后的假设。

### Gasper <!-- /part2/consensus/gasper/* -->

TODO

#### Safety and liveness in Gasper

TODO

##### Heads and tails

TODO

### Issues and Fixes <!-- /part2/consensus/issues/ -->

#### LMD GHOST

<!-- [RLMD GHOST paper](https://arxiv.org/pdf/2302.11326.pdf). -->

TODO

##### Attestation consideration delay

TODO <!-- 1 slot delay in consideration -->

##### Attestation recency

TODO. See the [Annotated Fork Choice](/part3/forkchoice/phase0/#attestation-timeliness).

##### Attestation equivocation

TODO. See the [Annotated Fork Choice](/part3/forkchoice/phase0/#on_attester_slashing).

##### Reorgs and Reversions

TODO

###### Ex ante reorgs

TODO

###### Ex post reorgs

TODO

##### Proposer boost

TODO. See the [Annotated Fork Choice](/part3/forkchoice/phase0/#proposer-boost).

#### Casper FFG

##### Casper FFG 的分叉选择可能导致长时间的重组

Casper FFG 的[分叉选择规则](/part2/consensus/casper_ffg/#fork-choice-rule)规定，底层共识协议必须遵循具有最高的已被合理化的检查点的链。这保证了 Casper FFG的[合理活性](/part2/consensus/casper_ffg/#plausible-liveness)，但在特殊情况下也可能导致长时间的重组。

2023 年 7 月 28 日，在以太坊的 Goerli 测试网就发生了类似事件。下面的解释是基于 [Potuz 的出色分析](https://web.archive.org/web/20230922104428/https://nitter.net/potuz1/status/1685736037321166848)。

如果我们查看 Goerli 测试网的[时段 192879](https://goerli.beaconcha.in/epoch/192879)，会发现它在[时隙 6172128](https://goerli.beaconcha.in/slot/6172128) 有一个初始区块，但该时段中的所有后续区块要么完全缺失，要么被孤立。分叉选择可视化工具显示，[时段 192880](https://goerli.beaconcha.in/epoch/192880) 的提议者们完全忽略了时段 192879 第一个时隙之后的所有内容，而是选择在时隙 6172128 的区块上构建新区块。

那么，到底发生了什么？

1. 异常的是，在时段 192878 结束时，检查点 192878 没有被验证，因为它没有积累足够的投票。

2. 在[时段 192879](https://goerli.beaconcha.in/slot/6172128) 开始时，时隙 6172128 的区块包含足够多能够合理化检查点 192878 的投票，但该区块发布得非常晚。

3. 由于其延迟，随后的提议者正当地忽略了时隙 6172128 的区块，而是在时隙 6172126 的区块上构建了一条链。在整个时段 192879，他们持续构建这条链。

   - 然而，这条新链上的区块没有包含足以合理化检查点 192878 的投票。根据设计，区块的认证[空间有限](/part3/config/preset/#max-operations-per-block)。由于多个时隙为空，认证空间变得拥挤。

   - They also did not have enough votes to justify Checkpoint 192879 by the end of Epoch 192879.他们也没有足够的投票以在时段 192879 结束前合理化检查点 192879。

4. 在时段 192879 结束时的时段处理后，我们有了两个分支：

   1. 第一个分支在时段 192879 中只包含一个区块，即时隙 6172128，且检查点 192878 是最高的合理检查点；

   2. 第二个分支在时段 192879 中包含多个区块，且检查点 192877 是最高的合理检查点。

5. 在时段 192880 开始时，Casper FFG 的分叉选择规则规定，由于时隙 6172128 中的区块具有最高的合理检查点，它必须成为新的头块，[时段 192879](https://goerli.beaconcha.in/epoch/192879) 中的所有后续区块必须被忽略。


<a id="img_consensus_issues_ffg_reorg"></a>
<figure class="diagram" style="width: 95%">

![A diagram illustrating the fork that led to a whole epoch of blocks being orphaned.](images/diagrams/consensus-issues-ffg-reorg.svg)

<figcaption>

在时段 192879 结束时的时段处理期间，顶部分支包含合理化检查点 192878 的投票，而底部分支没有足够的投票来合理化检查点 192878 或检查点 192879。Casper FFG 的分叉选择迫使时段 192880 的提议者在具有更高合理化检查点的分支上进行构建。因此，时隙 6172130 到 6172159 的所有区块都被孤立（重组）。大方块是检查点，圆角方块是区块。

</figcaption>
</figure>

原因不是实现中的漏洞，而是 Casper FFG 的分叉选择。如果在第 192880 个时段中，底部的分叉保持正统性，那么如果任何跟随顶部分叉的验证者直到很晚才看到底部分叉上的区块，将回被迫让其被合理化的时段从 192878 退回到 192877，这可能会导致它们在未来进行可被罚没的环绕投票。

这次重组的严重性被以下因素加剧：

  - 时段的长度：每 32 个时隙才会记录一次时段；
  - 总体的参与率接近 67% 的绝对多数门槛；
  - 一个包含关键的合理化信息的区块（在时隙 6172128）被延迟发布；
  - 几个空时隙导致区块空间的竞争，使得在时段 192879 中的重要认证被排除在较长链之外；以及
  - 认证被包含在区块中的时间窗口太紧，这应该会通过 [EIP 7045](https://eips.ethereum.org/EIPS/eip-7045) 中计划的变更得到改善。

请注意，所有这些与未实现的合理性（[unrealised justification](/part3/forkchoice/phase0/#unrealised-justification)）相关的问题无关。

这种情况在以太坊主网上发生的可能性非常低，主要是因为参与率几乎总是超过 99%，远高于绝对多数门槛，并且区块丢失的情况比在测试网上少得多。

#### Gasper

TODO

##### Unrealised Justification

TODO. See the [Annotated Fork Choice](/part3/forkchoice/phase0/#unrealised-justification).

#### Weak Subjectivity <!-- /part2/validator/weak_subjectivity/* -->

The two great problems that had to be solved in order for proof of stake to be a sound foundation for the world's economic activity were (1) the nothing at stake problem, and (2) long range attacks.

Ethereum's consensus solves the nothing at stake problem &ndash; that it is costless for a proof of stake validator to equivocate on every block proposal, building on every fork rather than picking one &ndash; with a slashing mechanism. The long range attack problem is solved by [embracing weak subjectivity](https://blog.ethereum.org/2014/11/25/proof-stake-learned-love-weak-subjectivity).

<!-- Vlad's earlier post has a good viewpoint on weak subjectvity: https://blog.ethereum.org/2015/08/01/introducing-casper-friendly-ghost -->

TODO

## The Progress of a Slot <!-- /part2/slot/* -->

### Introduction

TODO

### Proposing <!-- /part2/slot/proposing/* -->

TODO

### Attesting <!-- /part2/slot/attesting/* -->

TODO

### Aggregating <!-- /part2/slot/aggregating/* -->

TODO

### Sync Committee Participation <!-- /part2/slot/sync/* -->

TODO

## The Progress of an Epoch <!-- /part2/epoch/* -->

### Introduction

TODO

### Applying Rewards and Penalties <!-- /part2/epoch/rewards/* -->

TODO

### Justification and Finalisation <!-- /part2/epoch/finality/* -->

TODO

### Other State Updates <!-- /part2/epoch/updates/* -->

TODO

## Validator Lifecycle <!-- /part2/validator/* -->

### Introduction

TODO

## Deposits and Withdrawals <!-- /part2/deposits-withdrawals/ -->

<div class="summary">

  - 存款将以太币从执行层向共识层转移。
  - 取款将以太币从共识层向执行层转移。
  - 执行层和共识层的记账彼此完全独立。
  - 质押者向存款合约发送交易以进行质押。
  - 质押无需许可。
  - 取款是定期且自动的。
  - 可以提出部分或全部款项。

</div>

### 概述

作为一种权益证明协议，以太坊依赖将资本锁定在协议内（存款），并最终连同赚取的奖励一起取回（取款）的质押者。

被质押的资本是以太币（ETH），即以太坊的原生货币。共识层中的以太币独立存在，并与普通以太坊账户和合约中的以太币分开记账。共识层上的以太币存在于验证者账户的余额中。验证者账户的功能极度有限：余额因存款和奖励而增加，因取款和惩罚而减少。你无法在验证者账户之间进行转账或执行任何类型的交易。验证者账户的余额被作为[信标状态](/part3/containers/state/)的一部分而跟踪，并不构成常规的以太坊执行状态的一部分。需要注意的是，执行层的余额以 Wei（$10^{-18}$ ETH）为单位，而验证者的余额以 Gwei（$10^{-9}$ ETH）为单位。

它的基本架构（将在以下部分详细介绍）是，质押者通过向存款合约发送一笔以太坊交易来进行存款，该合约是执行层上的一个标准的以太坊智能合约。很重要的是质押完全无需许可。通过在一笔普通的以太坊交易中向存款合约发送 32 个以太币，任何人都可以质押并获得运行验证者的权利。

收到存款后，存款合约会发出一张收据。过一段时间，这张收据会被共识层拾取，创建一个验证者账户并记入存款金额。然后，质押者可以运行一个以太坊验证者。

如果一切顺利，验证者将获得奖励。这些奖励将定期且自动地从验证者的余额中扣除，并记入取款凭证中指定的 Eth1 账户，即取款地址。

当验证者最终表示想要退出协议，或验证者被罚没时，任何剩余余额将从验证者账户中扣除，被记入取款地址。

整个流程如下图所示。

<a id="img_deposits_withdrawals_overview"></a>
<figure class="diagram" style="width: 98%">

![A sketch of the flows of deposits and withdrawals between the execution and consensus layers.](images/diagrams/deposits-withdrawals-overview.svg)

<figcaption>

关于验证者存款和取款流动情况的草图。时间大致从上至下流动。补充存款（Top-up deposits）是可选的，但为了完整性而被展示出来。账户 1 和账户 2 可能是相同的，也可能是合约。账户 2 是取款地址。

</figcaption>
</figure>

从图表中有趣的一点是，存款合约中不会有减号：随着验证者退出和重新质押，存款合约的余额“只增不减”。当一个验证者退出然后重新质押时，存款合约的余额增加 32 以太币，而其他一切基本保持不变。如果这种情况发生 320 万次（考虑到目前有超过 50 万个质押的验证者，这并非不可想象），那么存款合约的余额将超过以太坊历史上曾经流通的以太币总量，大约为 1.2 亿。这并不重要，只是为了强调存款合约的余额应被视为已销毁，并在计算以太坊的总供应量时将其计为零。[^fn-deposit-contract-balance]

[^fn-deposit-contract-balance]: 现在引擎 API 已经可用，原则上我们可以在共识层处理收据时减少存款合约的余额，但不值得只是为了修复这一处怪异而去增加复杂性。 

更重要的是，有两种类型的存款和两种类型的取款。当共识层处理某个验证者的首次存款时（这可能不足以激活它），验证者账户便被创建。为同一验证者进行的任何后续存款都是补充存款，并且具有稍微不同的工作流程，需要的验证较少。

至于取款，部分取款是从验证者余额中将超过 32 个以太币的部分定期转移到执行层。完全取款发生在验证者退出协议并变为“可取款”状态时——此时验证人剩下的全部余额将被转移。两种取款都是自动和定期进行的。

在接下来的部分中，我们将首先看一下[进行存款](/part2/deposits-withdrawals/staking/)的操作机制，然后深入研究[存款合约](/part2/deposits-withdrawals/contract/)。最后我们将看看共识层处理[存款](/part2/deposits-withdrawals/deposit-processing/)和[取款](/part2/deposits-withdrawals/withdrawal-processing/)的机制。

下面章节中的一个恒定的主题是——当前存款和取款过程中的许多复杂性源于以太坊独特的历史。存款合约的增量默克尔树、Eth1Data 投票期、Eth1 跟随距离——这些都是因为在我们建立一个独立的信标链以进行权益证明时，执行层仍然保持着工作量证明。整个 BLS 取款凭证的事件也源于当时我们对路线图的不确定性。

另一个主题是——在合并后，我们有机会清理其中的一些复杂性。[EIP-6110](https://eips.ethereum.org/EIPS/eip-6110) 是一项将存款处理极大地流程化的提案。尽管如此，有些复杂性将永远伴随我们。

### 进行存款 <!-- /part2/deposits-withdrawals/staking/ -->

<div class="summary">

  - 初始存款会创建一个验证者记录。
  - 补充存款会增加现有的验证者余额。
  - 进行存款涉及向存款合约发送交易。
  - 以太坊启动平台提供了一个很好的界面，尽管也存在其他选择。
  - 存款命令行界面（CLI）工具等可以创建存款数据和 BLS 密钥库。

</div>

#### 引言

这不是一个操作指南，所以我只会介绍主要的工具和工作流程，以帮助理解这些概念。

以太坊基金会的质押启动平台（[Staking Launchpad](https://launchpad.ethereum.org/)）是许多个人质押者的入口。大型操作可能会使用智能合约去[批量](https://github.com/stakefish/eth2-batch-deposit)提交存款，但我们将专注于 32 个以太币的单笔存款。

启动平台将引导你使用质押存款命令行界面（[staking deposit CLI tool](https://github.com/ethereum/staking-deposit-cli)）[^fn-eth-staking-smith]。强烈建议你离线运行这个工具，比如在隔离网络的环境中或使用刚刚启动的机器。这是为了尽可能保证助记词种子短语的安全[^fn-safe-mnemonic]。

[^fn-eth-staking-smith]: [Eth-staking-smith](https://github.com/ChorusOne/eth-staking-smith) 是一个替代选择。我没有使用过它，因此不能保证其可靠性，但其来源可靠。它有一个有趣的功能，即可以使用 PBKDF2 作为密钥派生函数——见[下文](#keystores)。

[^fn-safe-mnemonic]: 在以前保护助记词比现在更重要，因为它不仅控制你的签名密钥，还控制你的取款凭证。如今，它通常只用于你的签名密钥——攻击者用它能造成的危害要比取款凭证小。 

#### Initial deposits

When making an initial deposit &ndash; that is, for a validator that does not exist yet &ndash; the staking CLI can be run interactively as follows.

```bash
./deposit new-mnemonic --execution_address '0x00....09'
```

By default, without the `--execution-address` parameter, the staking CLI will generate old-style [BLS withdrawal credentials](/part2/deposits-withdrawals/withdrawal-processing/#bls-withdrawal-credentials). You'll want to be using the new-style [Eth1 withdrawal credentials](/part2/deposits-withdrawals/withdrawal-processing/#eth1-withdrawal-credentials), so specify the address of an Ethereum account that you control here. If you don't do it now, you will need to [change it later](/part2/deposits-withdrawals/withdrawal-processing/#credential-changes) to receive your rewards and retrieve your stake.

The tool will generate a new 256 bit seed using your machine's randomness and convert it into a 24 word mnemonic phrase based on the [BIP-39 standard](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki). A single mnemonic seed phrase can be used to generate a large number of BLS12-381 [private&ndash;public key pairs](/part2/building_blocks/signatures/#key-pairs), and hence a large number of validators. Keep this mnemonic phrase very safely somewhere, and never online. You will need it if you ever need to recreate your validator keys, and when you exit your validator.

You can generate keystores for multiple validators at once, based on the same mnemonic. Just tell the deposit CLI how many. Each will need its own 32&nbsp;ETH stake, however.

After running through all the prompts and confirmations, the deposit CLI will generate some files. There will be a single `deposit_data-xxx.json` file and one `keystore-m_12381_3600_0_0_n_xxx.json` files per validator you generated.

##### Deposit data

The `deposit_data-xxx.json` file is part of the Launchpad's workflow; other tools may have different ways to do this. When you submit the deposit data file to the Launchpad, it will create a deposit transaction for each validator, which you sign with your normal Ethereum wallet, thereby sending 32&nbsp;ETH to the deposit contract.

The file contains a section like this for each validator (I've truncated some lines for convenience).

```json
{
  "pubkey": "a70d57e5fd4615bd3110a709be82be7a8b966fe881290f2738e4d8d0b38f39fe...",
  "withdrawal_credentials": "0100000000000000000000000001020304050607080900010203040506070809",
  "amount": 32000000000,
  "signature": "a6821877521df6ea65e7458fd599ef6430d23f64789cf7d89a75658eccdaf841...",
  "deposit_message_root": "047eb9f043b4cd464084c44db76ddb937e3fda11a63fde59a6149f74b8c50685",
  "deposit_data_root": "5a05c42ace9518a92c5ec950e6f58a6fd490a06b7619370d1b700d8d93b2cbbe",
  "fork_version": "00000000",
  "network_name": "mainnet",
  "deposit_cli_version": "2.5.0"
}
```

The fields are as follows.

  - `pubkey` is generated from the secret key that was generated from your mnemonic. It is the unique identity of the validator on the consensus layer. You can look it up on the [Beaconcha.in](https://beaconcha.in) explorer, for example.
  - `withdrawal_credentials` will begin with `01` if you specified the `execution_address`, and end with the 40 hexadecimal digits of the Ethereum account that withdrawals will go to. If you did not specify an `execution_address`, then `withdrawal_credentials` will begin `00` and be followed by a BLS withdrawal commitment.
  - `amount` is in Gwei. `32000000000` is 32&nbsp;ETH.
  - `signature` is a [BLS signature](/part2/building_blocks/signatures/) over the previous three fields, using your secret signing key.
  - `deposit_message_root` is the actual data that is signed with `signature`. It is the [hash tree root](/part2/building_blocks/merkleization/) of the [`DepositMessage`](/part3/containers/dependencies/#depositmessage) object. It's technically redundant as it can be recalculated easily, but the Launchpad uses it as a checksum to validate that the submitted data and signature all validate correctly.
  - `deposit_data_root` is the hash tree root of the [`DepositData`](/part3/containers/dependencies/#depositdata) object created from the first four fields above (i.e. deposit message plus its signature). This is used as a checksum by the deposit contract.
  - `fork_version` specifies which chain the deposit is for. The `fork_version` is encoded into the signature so that deposits are valid only on the intended chain. The chain's [`GENESIS_FORK_VERSION`](/part3/config/configuration/#genesis_fork_version) is always used when signing deposits.

The remaining fields are just administrative records.

##### Keystores

Also generated by the deposit CLI is a keystore file for each validator. This contains the validator's encrypted secret key. The keystore will be used by the staker's client software and needs to be installed as per the client's instructions. The keystore contents are protected by the password provided when the deposit CLI was run, which also needs to be provided to the staking client. Each client handles this differently, so consult the docs.

<details>
<summary>Example keystore</summary>

A keystore file has contents like the below. You can see that the `pubkey` matches the `pubkey` in the deposit data file, above. I won't go into the details of this, but the format is described in [ERC-2335](https://eips.ethereum.org/EIPS/eip-2335). The derivation `path` parameter is discussed in [ERC-2334](https://eips.ethereum.org/EIPS/eip-2334).

```json
{
  "crypto": {
    "kdf": {
      "function": "scrypt",
      "params": {
        "dklen": 32,
        "n": 262144,
        "r": 8,
        "p": 1,
        "salt": "d6679024b3693066eba27bbe7c2269fc62c98a1accf225c916d6eafb24abcdae"
      },
      "message": ""
    },
    "checksum": {
      "function": "sha256",
      "params": {},
      "message": "402eb9c5d6042f354bb8013ed19019b9d8cfa7deed1ed44eb2c0680615df1b13"
    },
    "cipher": {
      "function": "aes-128-ctr",
      "params": {
        "iv": "4ced4174acc07417f34106eb1cb5c685"
      },
      "message": "e7adc1ab79c2870fccd87b9cbd09830fcf0654cccca944ff3c9c26a1f6fb10b5"
    }
  },
  "description": "",
  "pubkey": "a70d57e5fd4615bd3110a709be82be7a8b966fe881290f2738e4d8d0b38f39fe..."
  "path": "m/12381/3600/0/0/0",
  "uuid": "7b25b4a7-9241-4ad8-9540-f0ed096f30cd",
  "version": 4
}
```

</details>

A key derivation function (KDF) is used to protect the secret key with a password. The KDF is designed to make it computationally infeasible to decrypt the secret key by brute-force.

The deposit CLI uses the [Scrypt KDF](https://en.wikipedia.org/wiki/Scrypt) which, by design, is slow and uses a lot of memory, about 300MB per key. This is fine for a solo-staker loading one or two keys, but can become a significant bottleneck for large staking services loading hundreds or thousands of keys at startup.

ERC-2335 keystores also support [PBKDF2](https://en.wikipedia.org/wiki/PBKDF2) as the KDF, which is much faster and less memory intensive. Depending on one's appetite for trading key security for loading speed, PBKDF2 may be preferable. The [`ethdo`](https://github.com/wealdtech/ethdo) and [`eth-staking-smith`](https://github.com/ChorusOne/eth-staking-smith) tools are able to generate keystores using PBKDF2.

#### Top-up deposits

Top-ups are deposits for validators that already exist. You might want to top-up a validator if its [effective balance](/part2/incentives/balances/) falls below 32&nbsp;ETH in order to restore it to maximum effectiveness.

The Staking Launchpad provides a [top-up](https://launchpad.ethereum.org/en/top-up) interface. You don't need access to your keystore or mnemonic to make a top-up deposit. In fact, anyone can top-up any validator at any time.

The transaction that gets sent to the deposit contract for a top-up is essentially the same as the transaction for an initial deposit, with the following differences:

  - the public key must match the public key of an existing validator,
  - the signature is not checked, and can be an "empty" dummy signature[^fn-top-up-signature], and
  - the withdrawal credentials are ignored.

[^fn-top-up-signature]: It seems that block explorers [do not know this](https://github.com/ConsenSys/teku/issues/7060) and can incorrectly mark top-up transactions as invalid.

It is possible to build up a validator's stake over time, with an initial deposit that's less than 32&nbsp;ETH, followed by one or more top-up deposits. The validator will become active when its effective balance reaches 32&nbsp;ETH. However, if you plan to do this, watch out for a tricky [edge case](/part2/incentives/balances/#an-edge-case) involving hysteresis when the final top-up is 1&nbsp;ETH.

##### See also

The [Ethereum.org website](https://ethereum.org/en/staking/solo/) has more information about solo staking, with a comparison of alternatives to the deposit CLI tool.

If you want to play around with consensus layer keys and wallets, the [`ethdo`](https://github.com/wealdtech/ethdo) tool is extremely useful. It has been [audited](https://www.wealdtech.com/articles/ethdo-audit/), and has a huge range of features as well as [handling the basics](https://medium.com/coinmonks/creating-ethereum-2-withdrawal-keys-using-ethdo-6e41b14ddd7b).

Three ERC standards have been proposed in relation to key handling for the consensus layer.

  - [ERC-2333: BLS12-381 Key Generation](https://eips.ethereum.org/EIPS/eip-2333).
  - [ERC-2334: BLS12-381 Deterministic Account Hierarchy](https://eips.ethereum.org/EIPS/eip-2334).
  - [ERC-2335: BLS12-381 Keystore](https://eips.ethereum.org/EIPS/eip-2335).

### The Deposit Contract <!-- /part2/deposits-withdrawals/contract/ -->

<div class="summary">

  - The deposit contract is the protocol's entry point for staking.
  - Anybody may permissionlessly stake 32&nbsp;ETH via the contract.
  - On receiving a valid deposit the contract emits a receipt.
  - An incremental Merkle tree maintains a Merkle root of all deposits.
  - The deposit contract cannot verify a deposit's BLS signature.
  - The balance of the deposit contract never decreases.
  - Ether sent to the deposit contract should be considered burned.

</div>

#### Overview

The deposit contract is the means by which stakers commit their Ether to the protocol in order to gain the right to run a validator.

The source code for the contract is available in the [specs repo](https://github.com/ethereum/consensus-specs/tree/dev/solidity_deposit_contract), and the verified byte code is [deployed on-chain](https://etherscan.io/address/0x00000000219ab540356cbb839cbe05303d7705fa#code).

##### Functionality

The deposit contract is a normal Ethereum smart contract running on the execution (Eth1) layer. Anyone wishing to place a stake in order to run a validator may send 32&nbsp;ETH to the deposit contract via a normal Ethereum transaction.

In addition to the Ether transferred, the deposit transaction must contain further data as follows.

First, the public key of the validator. A validator's public key is derived from its secret signing key, and is its primary identity on the consensus layer. The staker will provide the secret signing key separately to the consensus client for normal operational use.

Second, withdrawal credentials specifying which Ethereum account rewards earned will be sent to. This will also be the address that receives the validator's full balance when it eventually exits. Withdrawal credentials come in two forms, which we will discuss [later](/part2/deposits-withdrawals/withdrawal-processing/#withdrawal-credentials).

Third, a signature over the public key, the withdrawal credentials, and the deposit amount, using the normal signing key. This signature's main role is to serve as a "proof of possession" of the secret key of the validator, which side-steps a nasty [rogue public key attack](/part2/building_blocks/signatures/#proof-of-possession).

Fourth, the deposit data root, which is an [SSZ Merkleization](/part2/building_blocks/merkleization/) of all of the above data that serves as a kind of checksum that the contract can verify.

The deposit contract does some verification on these parameters. In particular, the deposit amount is subject to checks, and the deposit data root is verified. If either of these fails then the deposit will be rejected - that is, the deposit transaction will be reverted.

However, the deposit contract does not validate the signature - the EVM does not yet have the elliptic curve apparatus to do this, and it would be prohibitively expensive to do in normal bytecode. The signature will be validated later by the consensus layer, and if found to be incorrect (for new validators) the deposit will fail, and the Ether will be lost.

Once the deposit contract is as satisfied as it can be that the deposit is valid, it issues [a receipt](#deposit-receipts) (an EVM log event) containing the deposit data. This receipt will later be picked up by the consensus layer for processing.

##### Development

The original deposit contract [was written in Vyper](https://github.com/ethereum/deposit_contract), a Python-like smart contract language. Work on the contract code began in January 2018, some months before the beacon chain was conceived of: it is one of the very few carry-overs from earlier Ethereum proof of stake designs. The pre-beacon chain version, however, omitted all the of the Merkle tree apparatus as it was not required[^fn-6110-no-merkle]. Using the incremental (also called progressive) Merkle tree was [suggested by Vitalik](https://github.com/ethereum/consensus-specs/pull/490) in January 2019.

[^fn-6110-no-merkle]: With [EIP-6110](https://eips.ethereum.org/EIPS/eip-6110) we might end up going back in that direction, with the Merkle root no longer needed.

Around April 2020, work began to rewrite the deposit contract in Solidity, a more mainstream smart contract language. The stated reason in the [new repo](https://github.com/ethereum/consensus-specs/tree/dev/solidity_deposit_contract) is the following, which relates to formally verifying the contract.

> The original motivation was to run the SMTChecker and the new Yul IR generator option (`--ir`) in the compiler.

Runtime Verification's [verification work](https://github.com/runtimeverification/deposit-contract-verification/blob/master/deposit-contract-verification.pdf) cites "community concerns about the [then] current Vyper compiler" as the motivation for the rewrite. These concerns are captured in Suhabe Bugrara's initial [review of the Vyper contract](https://github.com/suhabe/eth-deposit-contract-vyper-review/blob/master/EthDepositContractVyperReview.pdf), and discussed in the Ethereum Foundation's [blog entry](https://blog.ethereum.org/2020/06/23/eth2-quick-update-no-12#solidity-deposit-contract-and-formal-verification).

The deployed deposit contract was compiled from [Solidity source code](https://etherscan.io/address/0x00000000219ab540356cbb839cbe05303d7705fa#code).

##### Verification work

Since Ethereum contracts are immutable once deployed, it was crucial that the deposit contract be correct: its balance would come to be a large fraction of all Ether. To this end, various analyses and formal verification activities were performed.

In June 2020 Runtime Verification performed a [formal verification](https://github.com/runtimeverification/deposit-contract-verification) covering two aspects.

1. Verification that the incremental Merkle tree algorithm is equivalent to a full Merkle tree construction.
2. Verification that the bytecode was correctly generated from the Solidity source code, using the KEVM verifier.

Just prior to the deployment of the contract, Franck Cassez of Consensys performed some further work as described in his paper, [Verification of the Incremental Merkle Tree Algorithm with Dafny](https://arxiv.org/pdf/2105.06009.pdf), and [GitHub repository](https://github.com/ConsenSys/deposit-sc-dafny). This goes further than Runtime Verification's work by fully mechanically verifying the incremental Merkle tree algorithm, using the Dafny formal verification language.

##### Deployment

The deposit contract was [deployed](https://etherscan.io/tx/0xe75fb554e433e03763a1560646ee22dcb74e5274b34c5ad644e7c0f619a7e1d0) on October the 14th, 2020, at 09:22:52 UTC to Ethereum address [`0x00000000219ab540356cbb839cbe05303d7705fa`](https://etherscan.io/address/0x00000000219ab540356cbb839cbe05303d7705fa).

The [deploying account](https://etherscan.io/address/0xb20a608c624ca5003905aa834de7156c68b2e1d0) was presumably generated by grinding for a key whose first transaction would deploy the contract to an address with the distinctive eight zero prefix: Ethereum contract addresses [are computed](https://ethereum.org/en/developers/docs/accounts/#contract-accounts) from the deployer's account address and nonce value. This would have taken on the order of $2^{32}$ (4.3 billion) key generation attempts.

Ignoring spam, there are only three transactions associated with the deploying account.

  - The account was funded with 1&nbsp;ETH (minus fee) via [a transfer from Tornado Cash](https://etherscan.io/tx/0x1956761ad42396786160cb4cbca845409dadc5366c46a2b4e178d63dc0f17578).
    - As a result, we have no way of identifying the deployer. Since the contract bytecode was publicly available, it could have been anybody.
  - The [deposit contract deployment](https://etherscan.io/tx/0xe75fb554e433e03763a1560646ee22dcb74e5274b34c5ad644e7c0f619a7e1d0) cost 0.31478286&nbsp;ETH.
  - The leftover ETH was transferred [to the WikiLeaks donation address](https://etherscan.io/tx/0x8aa30f7d95cd5f22dd02e59434c0e66794c6e370ed2659ea532ed6fe49f9cce5).

#### Code

The following exposition is based on the Solidity source code as [verified on Etherscan](https://etherscan.io/address/0x00000000219ab540356cbb839cbe05303d7705fa#code), which ought to match the source code in the [consensus specs repository](https://github.com/ethereum/consensus-specs/blob/v1.3.0/solidity_deposit_contract/deposit_contract.sol).

For brevity, I've omitted the interface boilerplate and some lengthy comments.

##### `DepositContract`

```solidity
contract DepositContract is IDepositContract, ERC165 {
    uint constant DEPOSIT_CONTRACT_TREE_DEPTH = 32;
    // NOTE: this also ensures `deposit_count` will fit into 64-bits
    uint constant MAX_DEPOSIT_COUNT = 2**DEPOSIT_CONTRACT_TREE_DEPTH - 1;

    bytes32[DEPOSIT_CONTRACT_TREE_DEPTH] branch;
    uint256 deposit_count;

    bytes32[DEPOSIT_CONTRACT_TREE_DEPTH] zero_hashes;

    constructor() public {
        // Compute hashes in empty sparse Merkle tree
        for (uint height = 0; height < DEPOSIT_CONTRACT_TREE_DEPTH - 1; height++)
            zero_hashes[height + 1] = sha256(abi.encodePacked(zero_hashes[height], zero_hashes[height]));
    }
```

After declaring its interfaces &ndash; we will look at ERC165 [below](#supportsinterface) &ndash; comes constants and storage.

The `DEPOSIT_CONTRACT_TREE_DEPTH` specifies the number of levels in the internal Merkle tree. With a depth of 32, it can have $2^{32}$ leaves, allowing for up to 4.3 billion deposits (`MAX_DEPOSIT_COUNT`[^fn-max-deposit-count])[^fn-eip-6110-max-deposits]. A deposit is a minimum of one ETH, so there's sufficient space for every ETH in existence to be deposited 35 times over.

[^fn-max-deposit-count]: Regarding the comment on `MAX_DEPOSIT_COUNT`, it will of course fit into 32 bits, which is definitely less than 64. The point is that `uint`s on the consensus layer are standardised at a size of 64 bits, and we don't want to overflow that.

[^fn-eip-6110-max-deposits]:  With the proposed mechanism in [EIP-6110](https://github.com/ethereum/consensus-specs/pull/3177) for deposit handling, we would no longer need the Merkle proofs and could in principle lift this limit. However, it is immutably encoded into the deposit contract, so would not be possible in practice.

The underlying data structure of the deposit contract is an incremental Merkle tree. This is a Merkle tree that supports only two operations, (1) appending a leaf, and (2) calculating the root. Constraining the data like this allows us to avoid storing the entire Merkle tree, which would be huge. Instead the contract stores only the last `branch` &ndash; a mere 32 nodes &ndash; which is all the information that's needed to calculate the Merkle root.

To gain this efficiency, we need an array of `zero_hashes`. At any given level of the tree, the zero hash is the value the node would have if all of the leaves under it were zero. Since we assign leaves sequentially, huge parts of the tree can be represented by the zero hashes.

The `constructor()` (which takes no arguments) only initialises the `zero_hashes` structure, taking advantage of the EVM's default that the uninitialised `zero_hashes[0]` storage value will be zero.

<a id="img_deposits_withdrawals_zero_hashes"></a>
<figure class="diagram" style="width: 75%">

![Diagram showing how the zero_hashes array is constructed.](images/diagrams/deposits-withdrawals-zero-hashes.svg)

<figcaption>

To construct $Z_n$, we start with $Z_0 = 0$ and define $Z_{i+1} = \text{Hash}(Z_i, Z_i)$.

</figcaption>
</figure>

##### `get_deposit_root`

```solidity
    function get_deposit_root() override external view returns (bytes32) {
        bytes32 node;
        uint size = deposit_count;
        for (uint height = 0; height < DEPOSIT_CONTRACT_TREE_DEPTH; height++) {
            if ((size & 1) == 1)
                node = sha256(abi.encodePacked(branch[height], node));
            else
                node = sha256(abi.encodePacked(node, zero_hashes[height]));
            size /= 2;
        }
        return sha256(abi.encodePacked(
            node,
            to_little_endian_64(uint64(deposit_count)),
            bytes24(0)
        ));
    }
```

Calculating the deposit root on demand saves us from having to use a storage slot to save it in. Local execution of `view` functions is free, while writing to blockchain state is very expensive.

The algorithm works as follows. In a binary Merkle tree, a node is either a left child or a right child.

  - If a node is a left child (`size & 1 == 0`), we know its sibling must be a zero hash, since the tree is incremental.
  - If a node is a right child, we take its sibling from `branch`. Thus, the important elements of `branch` are those that store the left-child nodes for the current value of `deposit_count`.

In effect, we are using the `zero_hashes`, $Z_n$, and the `branch` values, $B_n$, to summarise large parts of the tree. $Z_n$ is the root of a subtree whose $2^n$ leaves are all zero, with $Z_0 = 0$. $B_n$ is the root of a subtree, all of whose $2^n$ leaves were previously assigned, with $B_0$ being the last left-leaf that was inserted. By the time we reach the root, we will have effectively included all the leaves in the calculation.

This being an incremental Merkle tree, we know that the value of the leaf at `deposit_count` is zero: the count is zero-based, so leaf `deposit_count` has not yet been assigned; it will be the next leaf to be assigned.

To calculate the parent node, we hash together the value of its left and right children. Solidity's [`abi.encodePacked()`](https://docs.soliditylang.org/en/v0.8.11/abi-spec.html#non-standard-packed-mode) function is used to concatenate the 32 bytes of each sibling.

Note that we don't use any of the $B_n$ for $n > \log_2 i$, where $i$ is `deposit count` - any nodes we visit higher than this will be left nodes only. We make use of this when [updating `branch`](#updating_branch) after a new deposit.

###### Toy example

The beauty of the incremental Merkle tree is that we can calculate a root for a tree with up to $N$ leaves by maintaining just $\log_2 N$ values in storage, plus the `deposit_count`, with a further $\log_2 N$ constants.

<a id="img_deposits_withdrawals_deposit_root"></a>
<figure class="diagram" style="width: 75%">

![A diagram illustrating how the root of an incremental Merkle tree is calculated.](images/diagrams/deposits-withdrawals-deposit-root.svg)

<figcaption>

Finding the root of a three-level incremental Merkle tree. Five leaves have been assigned, $v_0$ to $v_4$, although we don't store their values. In the algorithm, `node` visits the dashed nodes from the leaf at the top to the root at the bottom. The $B_n$ are `branch` values maintained by `deposit()`, and the $Z_n$ are the pre-computed `zero_hashes`.

</figcaption>
</figure>

The diagram shows an incremental Merkle tree with three levels. We have filled up five of the leaves with values $v_0$ to $v_4$, but the only things that we actually store are the three $B_n$ values of `branch`, and the three $Z_n$ values of `zero_hashes`. At each level $n$ we will use either $B_n$ or $Z_n$ to calculate the parent.

The `deposit_count` is 5, so we start with `node` at the leaf labelled "5", which we know will be zero since it has not yet been assigned. This is a right child, therefore we combine it with $B_0$ as its left sibling. We know that $B_0$ will be equal to the last leaf value inserted, $v_4$. (If it were a left child, we would combine it with $Z_0 = 0$.)

Moving to level 1, `node` is now a left child, so we combine it with the level 1 zero hash, $Z_1$. We know that all the leaves descended from that $Z_1$ node are zero.

On level 2, `node` is again a right child, so we combine it with our stored value of $B_2$ to obtain the value at the root of the tree.

###### Why do we need the deposit root?

As we shall see later, each staking node separately maintains its own Merkle tree of deposits, independently of the deposit contract, which it builds using the deposit receipts from the execution layer. Why, then, do we need to put all this complex apparatus into the deposit contract in order to calculate the root?

Using the deposit root provides a self-contained way to verify that the deposit data in a block is correct. In the early stages of Eth2, it was not at all clear that all beacon chain nodes would be connected to Eth1 clients. In fact, pre-Merge, it was perfectly fine for a non-staking node not to be connected to an Eth1 client. Those nodes needed some way to be able to reject blocks with fake deposits. Putting the evidence on-chain via a Merkle proof allowed them to do so.

By means of the voting process [described below](/part2/deposits-withdrawals/deposit-processing/#eth1-voting-and-follow-distance), validators periodically import a deposit root from the contract onto the beacon chain. When a proposer includes deposits in its block, it must add a proof that the deposits are included in that deposit root. This allows every node that processes the chain to verify every deposit without having to consult the Eth1 chain.

Interestingly, post-Merge, all nodes (whether running validators or not) are required to comprise both consensus and execution clients, and execution payloads are included in beacon blocks. Therefore, nowadays, the data we need to validate deposits is on-chain as a matter of course, and we no longer strictly need to mess about with all this deposit root stuff. In fact, [EIP-6110](https://eips.ethereum.org/EIPS/eip-6110) proposes to explicitly expose validator deposits on-chain, after which the deposit contract code for maintaining the root will be redundant. Although, being immutable, it will continue to exist forever.

##### `get_deposit_count`

```solidity
    function get_deposit_count() override external view returns (bytes memory) {
        return to_little_endian_64(uint64(deposit_count));
    }
```

The only wrinkle here is the endianness transformation. The consensus layer uses little-endian format to serialise integers, whereas the EVM uses big-endian. The consensus layer calls this function to find out about new deposits, so it's convenient to get the output in the right format.

##### `deposit`

```solidity
    function deposit(
        bytes calldata pubkey,
        bytes calldata withdrawal_credentials,
        bytes calldata signature,
        bytes32 deposit_data_root
    ) override external payable {
        // Extended ABI length checks since dynamic types are used.
        require(pubkey.length == 48, "DepositContract: invalid pubkey length");
        require(withdrawal_credentials.length == 32, "DepositContract: invalid withdrawal_credentials length");
        require(signature.length == 96, "DepositContract: invalid signature length");

        // Check deposit amount
        require(msg.value >= 1 ether, "DepositContract: deposit value too low");
        require(msg.value % 1 gwei == 0, "DepositContract: deposit value not multiple of gwei");
        uint deposit_amount = msg.value / 1 gwei;
        require(deposit_amount <= type(uint64).max, "DepositContract: deposit value too high");
```

This is the business part of the contract - where stakers' deposits are made.

A deposit comprises the following items.

  - The public key of the validator: `pubkey` is the 48 byte (compressed) BLS public key derived from the staker's secret signing key.
  - The withdrawal credentials: `withdrawal_credentials` is 32 bytes of either `0x00` [BLS credentials](/part3/config/constants/#bls_withdrawal_prefix) or `0x01` [Eth1 credentials](/part3/config/constants/#eth1_address_withdrawal_prefix). Apart from their length, the withdrawal credentials are not validated anywhere in the contract, or even on the consensus layer.
  - The `signature` is a 96 Byte [BLS signature](/part2/building_blocks/signatures/). It is generated by signing the hash tree root of a [`DepositMessage`](/part3/containers/dependencies/#depositmessage) object (`public_key`, `withdrawal_credentials`, and `deposit_amount`), with the validator's signing key.
  - The `deposit_data_root` is basically a form of checksum. See below for how it is verified.
  - Finally, a `msg.value`. The message value is the amount of Ether (denominated in Wei, which are $10^{-18}$ ETH) that was sent with the transaction. This will normally be 32&nbsp;ETH for a new validator, but can be more or less. It must be,
    - at least one ETH,
    - a whole number of ETH, and
    - less than $2^{64}$ Gwei[^fn-gwei], which is 18.4 Billion ETH.

The very last condition is formally to avoid overflowing a consensus layer `uint64`, but seems kind of redundant in practice.

[^fn-gwei]: A Gwei is $10^9$ Wei, or $10^{-9}$ ETH, and is the unit of account on the consensus layer.

```solidity
        // Emit `DepositEvent` log
        bytes memory amount = to_little_endian_64(uint64(deposit_amount));
        emit DepositEvent(
            pubkey,
            withdrawal_credentials,
            amount,
            signature,
            to_little_endian_64(uint64(deposit_count))
        );
```

The contract now emits an event log (receipt). These receipts are how information about new deposits is picked up by the consensus layer. It looks a bit weird to emit the log before finishing all the checks (we have a couple more `require`s to pass yet), but if the transaction reverts, the beacon chain will also revert the event log, so no real harm is done emitting it early.

See [below](#deposit-receipts) for more detail on the receipt.

```solidity
        // Compute deposit data root (`DepositData` hash tree root)
        bytes32 pubkey_root = sha256(abi.encodePacked(pubkey, bytes16(0)));
        bytes32 signature_root = sha256(abi.encodePacked(
            sha256(abi.encodePacked(signature[:64])),
            sha256(abi.encodePacked(signature[64:], bytes32(0)))
        ));
        bytes32 node = sha256(abi.encodePacked(
            sha256(abi.encodePacked(pubkey_root, withdrawal_credentials)),
            sha256(abi.encodePacked(amount, bytes24(0), signature_root))
        ));

        // Verify computed and expected deposit data roots match
        require(node == deposit_data_root, "DepositContract: reconstructed DepositData does not match supplied deposit_data_root");
```

Here we have a "by hand" implementation of the [hash tree root](/part2/building_blocks/merkleization/) for a consensus layer [`DepositData`](/part3/containers/dependencies/#depositdata) object.

```none
class DepositData(Container):
    pubkey: BLSPubkey
    withdrawal_credentials: Bytes32
    amount: Gwei
    signature: BLSSignature  # Signing over DepositMessage
```

Using the same style as in the [Merkleization](/part2/building_blocks/merkleization/) chapter, we can illustrate the process in the following diagram. With a bit of head-scratching it's not too difficult to map it onto the mess of `sha256` calls in the code.

<a id="img_deposits_withdrawals_deposit_data_root"></a>
<figure class="diagram" style="width: 100%">

![A diagram showing how the hash tree root of a `DepositData` object is calculated from its members. ](images/diagrams/deposits-withdrawals-deposit-data-root.svg)

<figcaption>

Each box is a 32 byte chunk, possibly padded with zeros (in the cases of $S{(Pubkey)}_2$ and $S(Amount)$). Merkleization is the process of finding the hash tree root by iteratively hashing together pairs of chunks, in the form of binary trees, until the root is reached.

</figcaption>
</figure>

The only reason for doing this here is as a kind of checksum. The staker provides `deposit_data_root`, which is their independent calculation of the deposit root from the input data. The contract recalculates it to ensure that it matches the supplied data.

The `deposit_data_root` is the quantity (`node`) that will be inserted as a new leaf in the Merkle tree and forms part of the verification of a deposit on the consensus layer.

```solidity
        // Avoid overflowing the Merkle tree (and prevent edge case in computing `branch`)
        require(deposit_count < MAX_DEPOSIT_COUNT, "DepositContract: merkle tree full");

        // Add deposit data root to Merkle tree (update a single `branch` node)
        deposit_count += 1;
        uint size = deposit_count;
        for (uint height = 0; height < DEPOSIT_CONTRACT_TREE_DEPTH; height++) {
            if ((size & 1) == 1) {
                branch[height] = node;
                return;
            }
            node = sha256(abi.encodePacked(branch[height], node));
            size /= 2;
        }
        // As the loop should always end prematurely with the `return` statement,
        // this code should be unreachable. We assert `false` just to be safe.
        assert(false);
    }
```

<a id="updating_branch"></a>

Finally, we must update the Merkle tree.

A very cool feature of the incremental Merkle tree is that, not only can we continuously maintain its root by maintaining only the 32 values in `branch`, but when we insert a new leaf value, we need to update only a _single value_ of `branch`.

This is not obvious, but we can deduce it as follows. For a more formal explanation and analysis, see Franck Cassez's paper, [Verification of the Incremental Merkle Tree Algorithm with Dafny](https://arxiv.org/pdf/2105.06009).

Consider paths from adjacent leaves to the root: path $P$ from leaf $j - 1$, and path $Q$ from leaf $j$, where $j$ is `deposit_count`. Beyond some level $i$, paths $P$ and $Q$ will converge and visit the same nodes. At level $i$, path $P$ will visit a left node, having visited only right nodes previously, and path $Q$ will visit a right node, having visited only left nodes previously[^fn-binary-arithmetic].

[^fn-binary-arithmetic]: If it's not clear from thinking about paths, then consider binary numbers. If $j-1$ is `0111`, then $j$ is `1000`. The zeros represent left nodes, and the ones represent right nodes.

In short,

  - Path $P$ will comprise $(i-1)$ right nodes, followed by a left node, followed by some tail shared with $Q$.
  - Path $Q$ will comprise $(i-1)$ left nodes, followed by a right node, followed by some tail shared with $P$.

Path $Q$ is that path that will be used in the [`get_deposit_root()`](#get_deposit_root) algorithm.

Now, by construction, the $B_n$ (the `branch` values) always represent left nodes in the tree, and are needed only when path $Q$ visits a right node.

For levels greater than or equal to $i$ &ndash; where paths $P$ and $Q$ coincide &ndash; either the nodes visited are left nodes, in which case the $B_n$ value is irrelevant, or they are right nodes, in which case the $B_n$ value is unchanged since it is calculated from a sub tree whose leaves have not changed. So, no update is required to $B_n$ for $n > i$.

As for levels $n < i$, all the $Q_n$ are left nodes, and thus the $B_n$ are irrelevant. Therefore the sole $B_n$ that needs to be updated is $B_i$. The intuition is that, due to the way binary increments work, every time we need a new $B_n$, it have been be updated by the previous insertion, "just in time".

<a id="img_deposits_withdrawals_update_branch"></a>
<figure class="diagram" style="width: 100%">

![A diagram showing how `branch` is updated when a new leaf is appended.](images/diagrams/deposits-withdrawals-update-branch.svg)

<figcaption>

We've just inserted a leaf at position $j$. Next time `get_deposit_root()` is called, it will traverse path $Q$ from $j+1$, having previously traversed $P$ from $j$ . The paths converge at height $i + 1$. For $n < i$ path $Q$ is entirely left nodes, so $B_n$ is irrelevant. For $n > i$, $B_n$ is either unchanged or irrelevant. So we need only to updated $B_i$ to $B_i'$.

</figcaption>
</figure>

##### `supportsInterface`

```solidity
    function supportsInterface(bytes4 interfaceId) override external pure returns (bool) {
        return interfaceId == type(ERC165).interfaceId || interfaceId == type(IDepositContract).interfaceId;
    }
```

This is standard code based on [ERC-165](https://eips.ethereum.org/EIPS/eip-165) that allows calling applications to detect programmatically whether the contract supports a function interface based on the given function selector, `interfaceID`.

For example, according to the [Ethereum ABI](https://docs.soliditylang.org/en/develop/abi-spec.html#function-selector), the function selector for `get_deposit_count()` is `0x621fd130`. Therefore, calling `supportsInterface(0x621fd130)` will return `true`.

I don't know of any reason for implementing this for the deposit contract, but I suppose it's considered good practice to do so.

##### `to_little_endian_64`

```solidity
    function to_little_endian_64(uint64 value) internal pure returns (bytes memory ret) {
        ret = new bytes(8);
        bytes8 bytesValue = bytes8(value);
        // Byteswapping during copying to bytes.
        ret[0] = bytesValue[7];
        ret[1] = bytesValue[6];
        ret[2] = bytesValue[5];
        ret[3] = bytesValue[4];
        ret[4] = bytesValue[3];
        ret[5] = bytesValue[2];
        ret[6] = bytesValue[1];
        ret[7] = bytesValue[0];
    }
```

This is used in `get_deposit_root()`, `get_deposit_count()` and when emitting the `DepositEvent` log. All of these will be consumed by the consensus layer, which [uses little-endian](/part3/config/constants/#endianness) encoding for SSZ integers.

```solidity
}
```

And we're done.

#### Deposit Receipts

For every deposit accepted by the deposit contract it issues a receipt (also called a log or event[^fn-receipts-naming]), which is generated via an EVM `LOG1` opcode.

[^fn-receipts-naming]: Naming of these things is really messed up. I believe that Eth1 logs, events, and receipts are all the same thing. Etherscan hedges its bets by calling them "Transaction Receipt Event Logs".

The receipt has a single topic, which is the `DepositEvent` signature: `0x649bbc62d0e31342`<wbr/>`afea4e5cd82d4049`<wbr/>`e7e1ee912fc0889a`<wbr/>`a790803be39038c5`, equal to `keccak256("DepositEvent(bytes,`<wbr/>`bytes,`<wbr/>`bytes,`<wbr/>`bytes,`<wbr/>`bytes)")`.

The receipt's data is the 576 byte ABI encoding of `pubkey`, `withdrawal_credentials`, `amount`, `signature`, and `deposit_count`, converted to little-endian where required. Here's [an example](https://etherscan.io/tx/0xa41ae80276c837f3855e109c3bbba89bb6078215f86ccc4b981a4930858d3f3a#eventlog).

<details>
<summary>Example receipt data</summary>

The first column is the hexadecimal byte position of the start of the data in the second column.

```none
# Pointer to pubkey: 0x0a0
000  00000000000000000000000000000000000000000000000000000000000000a0

# Pointer to withdrawal_credentials: 0x100
020  0000000000000000000000000000000000000000000000000000000000000100

# Pointer to amount: 0x140
040  0000000000000000000000000000000000000000000000000000000000000140

# Pointer to signature: 0x180
060  0000000000000000000000000000000000000000000000000000000000000180

# Pointer to deposit_count: 0x200
080  0000000000000000000000000000000000000000000000000000000000000200

# Length of pubkey: 48 bytes
0a0  0000000000000000000000000000000000000000000000000000000000000030

# Pubkey data, padded with 16 zero bytes
0c0  b73fe99acbf91f0032ae95c3ed0d663ea246d02332373e101ff5c7ed520ce098
0e0  652de3eab056a9889bb3d05d734be21400000000000000000000000000000000

# Length of withdrawal_credentials: 32 bytes
100  0000000000000000000000000000000000000000000000000000000000000020

# Withdrawal credentials (0x01 type)
120  010000000000000000000000e637a2acbc531531700fcb7d2ed7e6d96ed8bbe8

# Length of amount: 8 bytes
140  0000000000000000000000000000000000000000000000000000000000000008

# Amount, little-endian encoded. 0x0773594000 = 32,000,000,000
160  0040597307000000000000000000000000000000000000000000000000000000

# Length of signature: 96 bytes
180  0000000000000000000000000000000000000000000000000000000000000060

# Signature data
1a0  b4a7e1546b13be69d31849b4302d870a04867b9de73a973794f8be88c25dc71f
1c0  c3440141c33cf3fbf2dea328179c89550f4e19cad118dd962b07a7c40a3aa8ac
1e0  eaded660edb6e030df48074ddfbe70b26d0e9db1c3be28afc0b47096aab7a616

# Length of deposit_count: 8 bytes
200  0000000000000000000000000000000000000000000000000000000000000008

# Deposit count, little-endian. 0x0a7b1a = 686,874
220  1a7b0a0000000000000000000000000000000000000000000000000000000000
```

</details>

A consensus client can request these receipts from its attached execution client via the standard [`eth_getLogs`](https://docs.infura.io/infura/networks/ethereum/json-rpc-methods/eth_getlogs) RPC method, filtering by the deposit contract address, block numbers, and event topic. This is how the consensus layer becomes aware of the details of new deposits.

The use of event logs here is an optimisation. The deposit contract could instead store all the Merkle tree's leaves and make them available via an `eth_call` method. However, since logs are not stored in the chain's state, only in block history, it is much cheaper to use them than it would be to store the leaves in the contract's state. However, this places a constraint on the amount of history we must keep around - we cannot now discard block history from before the deployment of the deposit contract. A newly activated consensus client needs access to the full receipt history in order to rebuild its internal view of the Merkle tree, even if it is able to checkpoint sync its beacon state. For convenience, some clients now support starting from a [deposit snapshot](https://github.com/ConsenSys/teku/pull/5954) of the Merkle tree that can be shared with other clients in much the same way as [checkpoint states](https://eth-clients.github.io/checkpoint-sync-endpoints/). This allows aggressive pruning of block history for those who want to do that.

### Deposit Processing <!-- /part2/deposits-withdrawals/deposit-processing/ -->

<div class="summary">

  - The consensus layer commits to the state of the deposit contract after an 8 hour delay, with a 2048 slot voting period.
  - The delay and voting are no longer necessary post-Merge and may be removed in future.
  - When a new deposit root is voted in, proposers must include deposits in blocks.
  - The block proposer makes an inclusion proof of the deposit against the contract's deposit root that all nodes can verify.
  - Deposits for new public keys create new validator records.
  - Deposits for existing public keys top up validators' balances.

</div>

#### Overview

The previous section on [the deposit contract](/part2/deposits-withdrawals/contract/) covered how deposits are handled on the execution layer. Now we shall look at how they get handed off to the consensus layer where the business of staking actually happens. A (valid) deposit into the execution layer deposit contract will either create a new validator on the consensus layer, or top up the balance of an existing validator.

There are two ways in which deposit information is transferred over from the execution layer to the consensus layer. One is the voting process by which the consensus layer comes to agreement on the state of the deposit contract at a particular execution block height. The other is validators directly importing deposit receipts from their attached Eth1 clients, which they will include in blocks and use to maintain their own copies of the deposit Merkle tree.

<a id="img_deposits_withdrawals_eth_calls"></a>
<figure class="diagram" style="width: 90%">

![A diagram showing how block proposers obtain and use data from the deposit contract.](images/diagrams/deposits-withdrawals-eth-calls.svg)

<figcaption>

Only block proposers directly need information from the deposit contract. They acquire this by making calls to the execution layer via the standard JSON RPC interface. Proposers rely on deposit receipts for including deposits in blocks, and for maintaining a copy of the deposit Merkle tree to make proofs for those deposits. Proposers also cast a vote for a recent state of the deposit contract.

</figcaption>
</figure>

Both of these mechanisms are somewhat legacy, post-Merge, although still in place for now. There is a proposal to overhaul the whole consensus layer deposit handling workflow at some point, in the form of [EIP-6110](https://eips.ethereum.org/EIPS/eip-6110).

#### Eth1 Voting and Follow Distance

As mentioned above, Eth1 voting is a legacy of the pre-Merge era of the consensus layer. It is the means by which the beacon chain comes to agreement on a common view of the Ethereum 1.0 chain, and in particular, a common view of the state of the deposit contract. Post-Merge, execution payloads are included in beacon blocks, and by definition all correct beacon nodes now have a common view of the Eth1 chain.

The consensus layer's common view of the deposit contract is formed by a majority vote of beacon block proposers over a repeating cycle of 2048 slots (about 6.8 hours, see [`EPOCHS_PER_ETH1_VOTING_PERIOD`](/part3/config/preset/#epochs_per_eth1_voting_period)). Each proposer includes in its block an [`Eth1Data`](/part3/containers/dependencies/#eth1data) vote as follows.

```python
class Eth1Data(Container):
    deposit_root: Root
    deposit_count: uint64
    block_hash: Hash32
```

The last field, `block_hash`, identifies a particular block on the execution chain. The `deposit_root` and `deposit_count` fields are set by calling the deposit contract's [`get_deposit_root()`](/part2/deposits-withdrawals/contract/#get_deposit_root) method at that block. The consensus client does this via normal JSON RPC [`eth_call`](https://ethereum.org/en/developers/docs/apis/json-rpc/#eth_call) invocations on the execution client.

During block processing, the beacon chain's [`process_eth1_data()`](/part3/transition/block/#def_process_eth1_data) function counts up the votes for each instance of Eth1Data seen in the current period. The first set of Eth1Data to be supported by more than 1024 validators (more than half of the period's block proposers) is adopted by immediately updating `state.eth1_data`. If no Eth1Data vote reaches the threshold during the voting period, then `state.eth1_data` is not updated. A fresh voting period begins only when the earlier one has run its full course of 2048 slots, even if new Eth1Data was voted in early.

##### Eth1 voting

Block proposers choose their Eth1 votes as described in the [honest validator guide](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#eth1-data). Here's a summary of the process. We set $S$ to be the wall clock time of the start of the current voting period. The $8\ \text{hours}$ comes from [`ETH1_FOLLOW_DISTANCE`](/part3/config/configuration/#eth1_follow_distance) (2048 Eth1 blocks) multiplied by [`SECONDS_PER_ETH1_BLOCK`](/part3/config/configuration/#seconds_per_eth1_block) (set to 14 as an approximate average value under proof of work)[^fn-28672-seconds].

[^fn-28672-seconds]: This is actually 28,672 seconds, but 8 hours is close enough for explanatory purposes. What's 128 seconds between friends?

1. First, ask the Eth1 client for all the Eth1 blocks with timestamps $t$ in the interval, $(S - 2\times 8\ \text{hours}) \le t \le (S - 8\ \text{hours})$.
2. Filter out any blocks that have a deposit count less than `state.eth1_data.deposit_count`: we've already seen these.
3. The proposer's default vote will be the Eth1Data from last block in this period, or if the list is empty (because Eth1 has stalled), then the winning vote from the previous voting period.
4. We're seeking to find agreement as quickly as possible, so an honest proposer will discard any Eth1Data that has not been voted for by other proposers already during the current period.
5. Finally, the honest proposer will cast a vote for the Eth1Data that already has the greatest support in the list that remains. If that list is empty (for example, if it is the first proposer in a voting period), it casts its default vote.

This algorithm has been refined considerably over time. Anecdotally, Eth1 data voting has been the source of significant numbers of issues on testnets over the years. It seems to be difficult to get right, probably because it is difficult to test. Getting Eth1Data voting correct is also not incentivised by the protocol. Rather, it is mildly disincentivised, since on-boarding new validators dilutes existing validators' rewards. In any case, it will be good to see the whole thing gone.

##### Eth1 follow distance

Appearing in the above is an 8 hour delay, [`ETH1_FOLLOW_DISTANCE`](/part3/config/configuration/#eth1_follow_distance) ` * ` [`SECONDS_PER_ETH1_BLOCK`](/part3/config/configuration/#seconds_per_eth1_block), before the consensus layer will even consider the state of the deposit contract.

This delay serves two functions. Under proof of work there was always a chance that blocks near the tip of the chain could be reorged out. It would be very bad for the beacon chain to include deposits that were later reverted - people might even try to "double spend" the consensus layer.

For all practical purposes, a delay of a few blocks would probably have been sufficient to counter this, since Ethereum under proof of work never suffered reversions longer than two or three blocks. Setting the follow distance as long as 8 hours is more about providing devs with enough time to respond if there were to be an incident on the Eth1 chain that might affect the deposit process, such as a chain split. In any case, this delay is now redundant as, post-Merge, the beacon chain and the execution chain move in lock-step.

The upshot of all of this is that the absolute minimum time interval between sending a deposit to the deposit contract and the consensus layer processing that deposit is around 11.4 hours: 8 hours due to the follow distance, and 3.4 hours being half of the voting period, the least required to get a majority vote. Assuming that voting is working well, the average time will be just over 17 hours. This doesn't include subsequent time waiting for the deposit to be included in a block, the validator sitting in the activation queue, etc.

For an in-depth analysis of the Eth1 follow distance and the Eth1 voting period length, see Mikhail Kalinin's Ethresear.ch article, [On the way to Eth1 finality](https://ethresear.ch/t/on-the-way-to-eth1-finality/7041?u=benjaminion). Note that both the follow distance and the voting period have been doubled in length since the article was written.

#### Deposit inclusion

Let's say that new Eth1Data has been voted in, with the `deposit_count`, $n$, replacing the previous count, $m < n$, in the beacon state. This means that subsequent block proposers have $n - m$ fresh deposits to include in blocks.

The `deposit_root` in the Eth1Data is the root of the deposit Merkle after $n$ deposits. Block proposers must construct proofs that deposits $m + 1$, $m + 2$, $\dots$, and $n$ are included in that Merkle root: proofs of inclusion in the Merkle tree.

In order to do this, each validator maintains its own deposit Merkle tree based on the deposit receipts it has downloaded from its attached Eth1 client. To construct a proof that deposit $m + 1$ is included in the tree, I need to have already built the tree that has all $n$ deposits. Then I can easily provide the [Merkle branch](https://pangea.cloud/docs/audit/merkle-trees) from leaf $m + 1$ to the known value of `deposit_root`.

Beacon block proposers must include all available deposits, in consecutive order, along with their Merkle proofs of inclusion, up to a maximum of [`MAX_DEPOSITS`](/part3/config/preset/#max-operations-per-block). If a block fails to include all available deposits in the correct order then the entire block is invalid.

The actual data that will be included in the proposer's beacon block for each deposit is a [`Deposit`](/part3/containers/operations/#deposit) object,

```python
class Deposit(Container):
    proof: Vector[Bytes32, DEPOSIT_CONTRACT_TREE_DEPTH + 1]  # Merkle path to deposit root
    data: DepositData
```

where [`DepositData`](/part3/containers/dependencies/#depositdata) is as follows,

```python
class DepositData(Container):
    pubkey: BLSPubkey
    withdrawal_credentials: Bytes32
    amount: Gwei
    signature: BLSSignature
```

Up to [`MAX_DEPOSITS`](/part3/config/preset/#max-operations-per-block) (16) of these can be included per block.

#### Deposit verification

Deposits are verified by all nodes during block processing in [`process_deposit()`](/part3/transition/block/#def_process_deposit) and [`apply_deposit()`](/part3/transition/block/#def_apply_deposit). In addition, the check that the block contains the expected number of deposits (the lesser of `MAX_DEPOSITS` and the remaining number of deposits to be processed) is done in [`process_operations()`](/part3/transition/block/#def_process_operations).

For each deposit, the first thing to be checked is its Merkle proof of inclusion. The [verification is performed](/part3/helper/predicates/#def_is_valid_merkle_branch) against the deposit root from the Eth1Data that was voted in. If a deposit passes the check, it proves that it was included in the deposit contract's tree at the same leaf position. If this check fails for any deposit, then the whole block is invalid.

When the deposit is for a new validator &ndash; that is, its public key does not already exist in the validator set &ndash; then the deposit's signature is verified. Signature verification proves that the public key belongs to a genuine, known secret key in the possession of the depositor. Importantly, a deposit with an invalid signature does not invalidate the whole block. It is just ignored and processing moves on. This is because the deposit contract was not able to validate the signature, so it is possible for invalid deposits to be present in its Merkle tree.

#### New Validators

If the public key in the deposit data does not already exist in the [validator registry](/part3/containers/state/#registry) then a new validator record is created, and the deposit amount is credited to the validator's account. The deposit amount will usually be the full 32&nbsp;ETH needed to activate a validator, but need not be. Later, at the end of the epoch, the validator's [effective balance](/part2/incentives/balances/) will be calculated - when the effective balance first becomes 32&nbsp;ETH then the validator will be queued for activation, otherwise the account will just sit there inactive until the effective balance is raised to 32&nbsp;ETH via a top-up deposit.

The validator's withdrawal credentials will also be set at this point. If they are `0x01` Eth1 withdrawal credentials, then they are permanent and cannot be changed in future. If they are `0x00` BLS withdrawal credentials then they may later be changed once to `0x01` credentials. See [the next section](/part2/deposits-withdrawals/withdrawal-processing/#withdrawal-credentials) for more on this.

#### Validator Top-ups

It is also possible to make top-up deposits for pre-existing validators. Anyone may do this for any validator. Top-up deposits have exactly the same structure as normal deposits, except that the top-up deposit's BLS signature is not checked, and the withdrawal credentials are ignored.

The minimum top-up amount is 1&nbsp;ETH. One might wish to send a top-up if a validator's effective balance has dropped below the maximum of 32&nbsp;ETH. Since most rewards are proportional to effective balance, such a validator will be under-performing. For example, with a 31&nbsp;ETH effective balance your expected rewards will be reduced by around 3%, and topping up to maintain a 32&nbsp;ETH effective balance might be worthwhile. Not many top-ups have been performed to date, but there are some [examples](https://etherscan.io/tx/0x3e68702566edee0061344eb99c484b4fac8800db082980bb6027d1dca09f5812).

As noted earlier, it is possible to build up a validator's stake over time, with an initial deposit that's less than 32&nbsp;ETH, followed by one or more top-up deposits. The validator will become active when its effective balance reaches 32&nbsp;ETH. However, if you plan to do this, watch out for a tricky [edge case](/part2/incentives/balances/#an-edge-case) involving hysteresis when the final top-up is 1&nbsp;ETH.

#### See also

Largely of historic interest now, Mikhail Kalinin's article [On the way to Eth1 finality](https://ethresear.ch/t/on-the-way-to-eth1-finality/7041?u=benjaminion) is an exemplary analysis of the deposit bridge from the Eth1 to Eth2.

The relevant spec functions and data structures for deposits are as follows.

  - The [deposit contract](/part2/deposits-withdrawals/contract/).
  - Constants [`ETH1_FOLLOW_DISTANCE`](/part3/config/configuration/#eth1_follow_distance), [`SECONDS_PER_ETH1_BLOCK`](/part3/config/configuration/#seconds_per_eth1_block), [`EPOCHS_PER_ETH1_VOTING_PERIOD`](/part3/config/preset/#epochs_per_eth1_voting_period), and [`MAX_DEPOSITS`](/part3/config/preset/#max-operations-per-block).
  - The [`Eth1Data`](/part3/containers/dependencies/#eth1data), [`DepositData`](/part3/containers/dependencies/#depositdata), and [`Deposit`](/part3/containers/operations/#deposit) containers.
  - Functions, [`process_eth1_data()`](/part3/transition/block/#def_process_eth1_data), [`process_deposit()`](/part3/transition/block/#def_process_deposit), [`is_valid_merkle_branch()`](/part3/helper/predicates/#def_is_valid_merkle_branch), and [`apply_deposit()`](/part3/transition/block/#def_apply_deposit), all part of [block processing](/part3/transition/block/).
  - Eth1 data handling in the [honest validator guide](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#eth1-data).

### Withdrawals <!-- /part2/deposits-withdrawals/withdrawal-processing/ -->

<div class="summary">

  - Consensus layer withdrawals were enabled in the Capella upgrade.
  - A validator must have Eth1 withdrawal credentials to benefit from withdrawals.
  - A one-time update from BLS credentials to Eth1 credentials is possible.
  - Withdrawals are automatic and periodic.
  - Up to 16 withdrawals per block can be processed.
  - A withdrawal might be partial (for active validators) or full (for exited validators).

</div>

#### Background

The ability to make withdrawals from the consensus layer was enabled in the [Capella upgrade](/part4/history/capella/), the first upgrade after the Merge.

Clearly, a fully-functioning proof of stake system needs ways both to stake and to unstake. However, for the first 29 months of the beacon chain's life, only staking was possible. All stakes, and all rewards earned, were locked within the consensus layer.

To have made withdrawals available pre-Merge would have needed [a bridge](https://ethresear.ch/t/two-way-bridges-between-eth1-and-eth2/6286?u=benjaminion) from the beacon chain to Ethereum's proof of work chain, perhaps via some kind of beacon chain light-client implementation on the Eth1 side. This was deemed to be a complex project that would only have delayed the Merge.

For similar reasons, we didn't enable withdrawals at the time of the Merge, either. The Merge on its own was complex and carried risk. We did what we could to simplify and de-risk it as much as we could, which included postponing withdrawals once again.

Eventually, fulfilling the core devs' soft commitment to the Ethereum community, withdrawals were successfully enabled in the first post-Merge upgrade, Capella, on April the 12th, 2023.

Two approaches were considered for enabling beacon chain Ether to be recovered on the execution chain.

The first design was for [pull withdrawals](https://github.com/ethereum/consensus-specs/pull/2759). After a validator had exited, the consensus layer would create a receipt that the staker could manually submit to the execution layer as a normal Ethereum transaction in order to recover the staked Ether and rewards. In a kind of mirroring of deposits, the consensus layer would maintain a Merkle tree of withdrawal receipts, exposing its root to the execution layer so that the withdrawal receipts could be validated when submitted there. Partial withdrawals were not really addressed in that work.

The adopted design, though, was for [push withdrawals](https://github.com/ethereum/consensus-specs/pull/2836), as described below. Push withdrawals happen automatically and do not require any action by the staker. This approach provides a better user experience, and required barely any increase to the beacon state size. It takes advantage of the post-Merge [Engine API](https://github.com/ethereum/execution-apis/blob/main/src/engine/common.md) as a bridge between the execution and consensus layers.

#### Withdrawal Credentials

When the beacon chain was conceived, it was to be only the first phase (Phase&nbsp;0) of the much larger Ethereum 2.0 project. It wasn't at all clear at that time what would happen to the existing Ethereum 1.0 chain, what kind of accounts would be implemented in Eth2.0, what kind of signature schemes would be used, and so on.

In view of the unknowns, we decided to implement withdrawal credentials as a commitment to being able to withdraw _somehow_ in the future, even though we had little idea what that might look like. The staker would keep a BLS withdrawal key, and, via the BLS withdrawal credentials, would be able to prove ownership of the validator's balance.

The beacon chain launched with only BLS withdrawal credentials, and all the early validators used these. Eth1 withdrawal credentials were [committed to](https://github.com/ethereum/consensus-specs/pull/2149) in the specs in February, 2021, only three months or so into the beacon chain's life. Since no validation is done on withdrawal credentials when a deposit is made, stakers continue to be free to use whichever they prefer. At the point of the Capella upgrade, 322,491 validators (56.9%) had BLS credentials, and 244,653 (43.1%) had Eth1 credentials[^fn-check-your-creds].

[^fn-check-your-creds]: You can check what type of credentials your validator has via its page on the [Beaconcha.in](https://beaconcha.in) explorer. Go to the "Deposits" tab. If your credentials begin `0x00` then they are BLS, if they begin `0x01` then they are Eth1.

##### BLS withdrawal credentials

BLS withdrawal credentials are often called `0x00` credentials due to their [prefix](/part3/config/constants/#withdrawal-prefixes). You can think of them as version zero credentials. A BLS withdrawal credential is the 32-byte hash of a 48-byte BLS public key, with the first byte replaced by `0x00`.

Here's [validator zero](https://beaconcha.in/validator/0#deposits)'s original BLS withdrawal credential. Note the initial `0x00` byte.

```none
0x00f50428677c60f997aadeab24aabf7fceaef491c96a52b463ae91f95611cf71
```

The idea is that the staker has a second BLS secret key, a withdrawal key, in addition to their usual signing key. The commitment from the protocol devs was that the withdrawal key could be used in future to sign a credential change message. The BLS withdrawal credential ensures that the claimed public key on that message matches the deposit that was initially made, so only the original depositor can access the stake and the rewards.

Separating the withdrawal key from the normal signing key has a number of benefits. Primarily, it separates ownership of the stake (controlled by the withdrawal key) from management of the stake (controlled by the signing key). This has allowed "non-custodial" staking services to appear, in which the staking service uses the signing key for day-to-day operations, but they have no ownership of the stake or rewards since the individual staker retains the withdrawal key. It also allows the withdrawal key to be kept offline, in cold storage, while the signing key remains online and "hot".

For ease of recovery, the BLS withdrawal key can be generated from the same mnemonic as the signing key by using a slightly different derivation path, as described in [ERC-2334](https://eips.ethereum.org/EIPS/eip-2334#validator-keys). This is what the [`staking-deposit-cli` tool](https://github.com/ethereum/staking-deposit-cli) does.

##### Eth1 withdrawal credentials

Unless you want to keep your Ether locked up on the consensus layer for some reason, everybody should now use Eth1 withdrawal credentials. These are set either when staking[^fn-cli-tool-eth1-creds], or by sending a BLS to execution change message.

[^fn-cli-tool-eth1-creds]: Take care when using the [`staking-deposit-cli`](https://github.com/ethereum/staking-deposit-cli) to make a deposit. It (silently) defaults to BLS withdrawal credentials unless you specify the `--eth1_withdrawal_address` command line parameter.

An Eth1 (execution) withdrawal credential has the [prefix](/part3/config/constants/#withdrawal-prefixes) `0x01`, followed by eleven zero bytes, followed by the 20 bytes of a normal Ethereum address. That address is where all Ether from withdrawals will be sent.

Here's [validator zero](https://beaconcha.in/validator/0#deposits)'s current Eth1 withdrawal credential. Note the initial `0x01` byte.

```none
0x0100000000000000000000000d369bb49efa5100fd3b86a9f828c55da04d2d50
```

The withdrawal address may be a normal Ethereum account (an EOA) or a smart contract. However, when it is a smart contract, no code will be executed on receiving a withdrawal payout. This differs from receiving Ether via a transfer, which can cause a fallback function to be called.

##### Credential changes

Only validators that have Eth1 withdrawal credentials are eligible for withdrawals. Validators with BLS withdrawal credentials need to send a withdrawal credential change message to update to Eth1 credentials. Until they do this, their stake and rewards remain locked on the consensus layer.

Changing withdrawal credentials is a one-time operation. Once a validator has Eth1 credentials, no further change is possible. The only way to change your withdrawal payout address once it has been set is to exit your validator and re-stake with the new credentials.

###### Making a credential change

A staker whose validator has BLS withdrawal credentials, who wishes to change to Eth1 credentials, must send a message to the beacon chain, signed with the validator's withdrawal key. The [`staking-deposit-cli`](https://github.com/ethereum/staking-deposit-cli) and the [`ethdo`](https://github.com/wealdtech/ethdo) tool are both able to generate this message. It is a straightforward process, requiring the validator's public key, the mnemonic used when staking, and the validator's existing withdrawal credentials as a checksum. It is recommended that generating the message be done offline as the BLS withdrawal key remains highly valuable to hackers until after the credential change has been completed.

Once the credential change message has been generated, it needs to be uploaded to a beacon node to be broadcast to the network. This can be done via any beacon node's [REST API](https://ethereum.github.io/beacon-APIs/#/Beacon/submitPoolBLSToExecutionChange), or via the Beaconcha.in explorer's handy signed message [submission service](https://beaconcha.in/tools/broadcast).

Some time after the message has been uploaded and broadcast, a block proposer should include the credential change message in a beacon block for processing by the consensus layer. Up to [`MAX_BLS_TO_EXECUTION_CHANGES`](/part3/config/preset/#max_bls_to_execution_changes) (16) such messages can be included per block.

For reference, a BLS to Eth1 credential change message has the [following contents](/part3/containers/operations/#blstoexecutionchange).

```python
class BLSToExecutionChange(Container):
    validator_index: ValidatorIndex
    from_bls_pubkey: BLSPubkey
    to_execution_address: ExecutionAddress
```

###### Processing a credential change

Credential change messages are handled during block processing by the [`process_bls_to_execution_change()`](/part3/transition/block/#def_process_bls_to_execution_change) function.

It checks that,

1. the validator currently has `0x00` BLS credentials,
2. the hash of the public withdrawal key (generated from the secret withdrawal key) matches the withdrawal credentials created when the deposit was made, and
3. the signature on the message verifies against the public withdrawal key provided.

Once satisfied that all is correct, the validator's withdrawal credentials are irrevocably updated to Eth1 withdrawal credentials. The validator is now eligible for automatic push withdrawals that will be made to the `to_execution_address` provided in the `BLSToExecutionChange` data.

To reiterate, changing withdrawal credentials is a one-time process. You can only change from BLS to Eth1 credentials. It is not possible to change Eth1 credentials without exiting the validator and re-staking.

#### Withdrawal processing

As mentioned above, we decided to adopt a "push" mechanism for withdrawals. Push withdrawals happen automatically, with no intervention from the staker. Up to [`MAX_WITHDRAWALS_PER_PAYLOAD`](/part3/config/preset/#max_withdrawals_per_payload) (16) consensus layer withdrawals are made per block.

<!-- Number of validators -->

Validator withdrawals are processed in a round-robin fashion. Starting from validator 0 at the Capella upgrade, with each block, the consensus layer sweeps through the validator set in validator index order until it has found 16 withdrawals to include. The next block proposer will pick up where the previous proposer left off in the validator set and scan for 16 further withdrawals, and so on. If every validator were eligible for a withdrawal, and if the beacon chain is performing perfectly, then a full sweep of 576,000 validators would take 5 days. That is, a validator could expect to receive a partial withdrawal payout every 5 days.

##### Finding withdrawals

To find the withdrawals it must include, the block proposer calls the [`get_expected_withdrawals()`](/part3/transition/block/#def_get_expected_withdrawals) function. This will return a list of up to `MAX_WITHDRAWALS_PER_PAYLOAD` [`Withdrawal`](/part3/containers/dependencies/#withdrawal) objects, each containing the following information.

```python
class Withdrawal(Container):
    index: WithdrawalIndex
    validator_index: ValidatorIndex
    address: ExecutionAddress
    amount: Gwei
```

The `index` field is the number of previous withdrawals ever made. It is populated from `state.next_withdrawal_index` and increases by one for each withdrawal. It is used only for uniquely indexing withdrawals in the execution layer. The `validator_index` is, of course, the validator whose beacon chain balance will be decreased, and whose Eth1 withdrawal address balance (the `address` field here) will be increased.

The list of withdrawals is generated deterministically. The block proposer starts from the current value of `state.next_withdrawal_validator_index` and considers validators in turn. If a validator is eligible for a withdrawal then it is added to the list, otherwise it is skipped. When either `MAX_WITHDRAWALS_PER_PAYLOAD` withdrawals have been added, or `MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP` have been considered, the list is returned. If the end of the validator registry is hit, the search wraps around again to validator 0.

To be eligible for a withdrawal, a validator must have [Eth1 withdrawal credentials](#eth1-withdrawal-credentials) set, and one of the following must also apply:

  - The validator has exited, has become withdrawable, and has a non-zero balance. Such a validator is eligible for a [full withdrawal](#full-withdrawals).
  - The validator has an [effective balance](/part2/incentives/balances/) of [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) (32&nbsp;ETH), and an actual balance higher than that. Such a validator is eligible for a [partial withdrawal](#partial-withdrawals).

Both of these may be true at once, in which case the first takes priority and a full withdrawal will be made for the validator.

Usually, a full list of 16 withdrawals will be generated. However, the search is bounded by considering a maximum of [`MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP`](/part3/config/preset/#max_validators_per_withdrawals_sweep) validators. If this limit is hit, fewer than 16 withdrawals will be generated.

The reason for [limiting the search](https://github.com/ethereum/consensus-specs/pull/3095), rather than making a full sweep through the whole validator set, is to bound the computational load on consensus nodes. Accessing the validator registry can be quite costly; an unbounded sweep could become a performance bottleneck.

There are two scenarios in which the bound might become relevant. The first is if there were long sections of the validator registry in which no validator had upgraded to Eth1 withdrawal credentials. This was more of a concern at the point of the Capella upgrade, when all early validators necessarily had BLS withdrawal credentials. More interesting is the possibility of an [inactivity leak](/part2/incentives/inactivity/), which occurs when the chain stops finalising in a timely way. During an inactivity leak, no validator receives attestation rewards, and many validators receive extra inactivity penalties. Very few balances will be increasing - only block proposers and sync committee members, perhaps. During a prolonged inactivity leak it is possible that large sections of the validator registry would be ineligible for a withdrawal, and the bound on the withdrawals sweep would be enforced.

##### Performing withdrawals

The consensus layer and the execution layer must coordinate carefully in order to process a withdrawal, which makes the full round-trip a little convoluted. The steps are as follows.

1. The beacon block proposer assembles a list of withdrawals by calling [`get_expected_withdrawals()`](/part3/transition/block/#def_get_expected_withdrawals) as detailed above.
2. The beacon block proposer sends the withdrawals list to its attached execution client via the Engine API. See [`prepare_execution_payload()`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/capella/validator.md#executionpayload) in the Honest Validator spec. The relevant Engine API data structure is [`PayloadAttributesV2`](https://github.com/ethereum/execution-apis/blob/main/src/engine/shanghai.md#payloadattributesv2).
3. The execution client returns an [execution payload](https://github.com/ethereum/execution-apis/blob/main/src/engine/shanghai.md#executionpayloadv2) that includes the list of withdrawals, along with everything else.
4. The block proposer incorporates the execution payload into its beacon block and broadcasts it to the network.
5. On receiving the block, all consensus nodes extract the withdrawals list from the [execution payload](/part3/containers/execution/#executionpayload) and call [`process_withdrawals()`](/part3/transition/block/#def_process_withdrawals) to deduct the withdrawal amounts from the validators' balances. Each node calls [`get_expected_withdrawals()`](/part3/transition/block/#def_get_expected_withdrawals) independently, and the beacon block is valid only if its withdrawals list matches.
6. The consensus node sends the execution payload to its attached execution client. The execution client will process the withdrawals alongside all the other transactions in the payload, incrementing the Eth1 withdrawal addresses as required.

The execution layer mechanics of withdrawal processing are described in [EIP-4895](https://eips.ethereum.org/EIPS/eip-4895). Withdrawal transactions are processed after all the normal transactions in the block.

As mentioned above, withdrawal transactions don't trigger smart contract processing when the Eth1 account balance is incremented. This is primarily to avoid failures (EVM transaction reversions) that would complicate the entire process. It also avoids placing an unknown load on the execution client. The benefit of this is that withdrawals are gasless and therefore free. The receiving account's balance will increase by precisely the same amount of ETH as is deducted from the validator's beacon chain balance.

##### Partial and full withdrawals

A validator might be eligible for a partial withdrawal or for a full withdrawal. Neither type is prioritised over the other; they both occur alongside each other as validators are considered during the withdrawals sweep. If a validator is eligible for both, a full withdrawal will be performed.

In both cases, there is no minimum withdrawal amount - it could be a single Gwei, the beacon chain's smallest unit of account. Withdrawals are not created for zero amounts.

###### Partial withdrawals

Partial withdrawals make up the majority of withdrawals processed. Partial withdrawals periodically skim excess balance from validators as they earn rewards.

To be eligible for a partial withdrawal, all of the following must be true. The second and third criteria are checked in the [`is_partially_withdrawable_validator()`](/part3/helper/predicates/#is_partially_withdrawable_validator) predicate.

  - The validator has [Eth1 withdrawal credentials](#eth1-withdrawal-credentials).
  - The validator's effective balance is [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) (32&nbsp;ETH).
  - The validator's actual balance exceeds `MAX_EFFECTIVE_BALANCE`.

The amount of the partial withdrawal will be the validator's balance in excess of `MAX_EFFECTIVE_BALANCE`.

The condition on the validator's effective balance eliminates an edge case where a validator has an effective balance of 31&nbsp;ETH, but an actual balance of over 32&nbsp;ETH, which can arise due to [hysteresis](/part2/incentives/balances/#hysteresis). If the condition on effective balance were not applied, it might become impossible for a validator ever to regain a full effective balance of 32 ETH (without a top-up deposit), due to its balance being continually skimmed.

###### Full withdrawals

A full withdrawal takes place after a validator has exited the validator set and subsequently become withdrawable. A validator normally becomes withdrawable about [27 hours](/part3/config/configuration/#min_validator_withdrawability_delay) after making its way through the exit queue, but a slashed validator will take [much longer](/part3/config/preset/#epochs_per_slashings_vector).

The precise criteria are in the [`is_fully_withdrawable_validator()`](/part3/helper/predicates/#is_fully_withdrawable_validator) predicate. All of the following must apply.

  - The validator has [Eth1 withdrawal credentials](#eth1-withdrawal-credentials).
  - The validator is withdrawable (the current epoch is greater than or equal to its withdrawable epoch).
  - The validator has a non-zero balance.

The amount of the full withdrawal will be the validator's entire balance.

Note that there is [no flag](https://github.com/ethereum/consensus-specs/pull/2836#discussion_r817657925) to indicate that a validator has been withdrawn. This in principle allows a top-up deposit to be made for a validator after a full withdrawal has occurred, in which case another full withdrawal will occur, and the top-up amount will be returned to the execution layer.

#### See also

The Ethereum.org pages have a [section on withdrawals](https://ethereum.org/en/staking/withdrawals/), and there is a separate [withdrawals FAQ](https://notes.ethereum.org/@launchpad/withdrawals-faq). They both have plenty of links to further resources.

The relevant spec functions and data structures for withdrawals are as follows.

  - The constants [`MAX_WITHDRAWALS_PER_PAYLOAD`](/part3/config/preset/#max_withdrawals_per_payload), [`MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP`](/part3/config/preset/#max_validators_per_withdrawals_sweep), and [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance).
  - `next_withdrawal_index`, and `next_withdrawal_validator_index` in the [beacon state](/part3/containers/state/#withdrawals).
  - The [`Withdrawal`](/part3/containers/dependencies/#withdrawal) container, and the `withdrawals` list in the [`ExecutionPayload`](/part3/containers/execution/#executionpayload) container.
  - [`get_expected_withdrawals()`](/part3/transition/block/#def_get_expected_withdrawals), [`process_withdrawals()`](/part3/transition/block/#def_process_withdrawals) in [block processing](/part3/transition/block/).
  - [`is_fully_withdrawable_validator()`](/part3/helper/predicates/#def_is_fully_withdrawable_validator), [`is_partially_withdrawable_validator()`](/part3/helper/predicates/#def_is_partially_withdrawable_validator) in [predicates](/part3/helper/predicates/).
  - Preparing the `ExecutionPayload` in the Capella [honest validator guide](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/capella/validator.md#executionpayload), and `PayloadAttributesV2` in the [Execution API](https://github.com/ethereum/execution-apis/blob/main/src/engine/shanghai.md#payloadattributesv2).
  - [EIP-4895](https://eips.ethereum.org/EIPS/eip-4895), "Beacon chain push withdrawals as operations", covers the execution layer side.

And for credential changes.

  - [Withdrawal prefixes](/part3/config/constants/#withdrawal-prefixes) in [constants](/part3/config/constants/).
  - The [`BLSToExecutionChange`](/part3/containers/operations/#blstoexecutionchange) and [`SignedBLSToExecutionChange`](/part3/containers/envelopes/#signedblstoexecutionchange) containers.
  - [`process_bls_to_execution_change()`](/part3/transition/block/#def_process_bls_to_execution_change) in [block processing](/part3/transition/block/).
  - The `submitPoolBLSToExecutionChange` method of the [Beacon API](https://ethereum.github.io/beacon-APIs/#/Beacon/submitPoolBLSToExecutionChange).

## 激励层 <!-- /part2/incentives/ -->

### 胡萝卜加大棒与突然死亡

无需许可的区块链是加密经济系统：在可能的情况下，密码学强制执行正确的行为；在无法强制执行的情况下，经济学激励正确的行为。我们所追求的正确行为大致与可用性和安全性相对应。我们希望链持续前进，并在所有合理的情况下提供可靠的、无矛盾的结果。

本章将介绍信标链中用于激励参与者的经济工具；密码学方面的内容将在其他章节涉及。概括地说，帮助我们实现这些目标的工具有：（1）奖励有助于协议的行为；（2）惩罚阻碍协议的行为；（3）惩罚看似是攻击协议的行为。

工作量证明少数吸引人的方面之一是其经济模型的简单性。矿工们通过创建被链所接受的区块以获得区块奖励，并通过在其区块中包含人们的交易以获得费用。区块奖励来自新创建（发行）的币，交易费用来自之前发行的币。协议中没有明确的处罚或惩罚。结合“最重链”的分叉选择规则，这种简单的模式被证明是非常稳健的。以太坊 1 通过对矿工的数块奖励和 EIP-1559 费用燃烧机制增加了一点复杂性，但它基本上还是很简单，相当容易推理。

相比之下，以太坊 2.0 权益证明协议采用了一系列不同的经济激励措施。在接下来的章节中，我们会将其细分为以下内容。

1. 最基本的经济要素是[质押](/part2/incentives/staking/)本身。
2. 在协议中，质押以验证者[余额](/part2/incentives/balances/)的形式表示，特别是一个被称为“有效余额”的数量，它是衡量特定验证者对协议影响的实际尺度。
3. 与工作量证明类似，协议发行新硬币以提供我们正在讨论的激励。我们将在“发行（[issuance](/part2/incentives/issuance/)）”部分探讨这一点。
4. 一系列奖励（[rewards](/part2/incentives/rewards/)）被用于激励诸如发布信标区块和及时进行认证等理想行为。
5. 惩罚（[Penalties](/part2/incentives/penalties/)）则用来抑制不可取的行为，如未能进行认证，延迟作出认证或作出不正确的认证。
6. 怠惰惩罚（[inactivity leak](/part2/incentives/inactivity/)）是信标链可能采用的一种特殊机制，在这种机制中，奖惩措施会被修改，以更严厉地惩罚不参与的行为。
7. 罚没（[Slashings](/part2/incentives/slashing/)）是对以非常具体的类似攻击的方式违反协议规则的惩罚。
8. 最后，我们以此结束：这些激励措施如何结合在一起，使信标链基础设施的[多样化](/part2/incentives/diversity/)部署成为最安全的策略。

请注意，这些部分的讨论在孤立地考虑共识层。现在是后合并时期，这已不再是全部情况。除了协议生成的奖励，以太坊质押者现在还可以从交易费用的小费和 [MEV](https://ethereum.org/en/developers/docs/mev/)（最大可提取价值）中获利。 将来，他们可能还能从再质押（[restaking](https://docs.eigenlayer.xyz/overview/readme)）中获利。所有这些都会改动协议的激励机制。例如，一个验证者为了获得约 [2000 万美元的 MEV 收入](https://collective.flashbots.net/t/post-mortem-april-3rd-2023-mev-boost-relay-incident-and-related-timing-issue/1540)，付出被[罚没](https://beaconcha.in/validator/552061) 1 个以太币的代价，尽管根据协议规则，这样做是不诚实的，但这也是经济理性的行为。这些事情的影响是许多讨论、发展和辩论的主题，值得专门去写一本书。然而，出于本书的目的，我只关注共识层的加密经济堆栈。

#### 另见

Vlad Zamfir 关于 Casper 协议开发的回忆录不仅是一本很棒的读物，也是对设计权益证明协议所面临挑战的很好介绍。他们讨论了许多设计决策的背景，这些决策最终导致了我们今天看到的协议。[第一部分](https://medium.com/@Vlad_Zamfir/the-history-of-casper-part-1-59233819c9a9), [第二部分](https://medium.com/@Vlad_Zamfir/the-history-of-casper-chapter-2-8e09b9d3b780), [第三部分](https://medium.com/@Vlad_Zamfir/the-history-of-casper-chapter-3-70fefb1182fc), [第四部分](https://medium.com/@Vlad_Zamfir/the-history-of-casper-chapter-4-3855638b5f0e), [第五部分](https://medium.com/@Vlad_Zamfir/the-history-of-casper-chapter-5-8652959cef58)。

Chorus One 的 Umberto Natale 最近发表的报告《以太坊加密经济学分析：验证者的视角》（[Analysing Ethereum Cryptoeconomics: the validator's perspective](https://docs.google.com/document/d/1r640UQOm2z-Q9nsJzqBq3BVgCtTL1_Yc7WnPp4jEBgk/edit)）也涵盖了下文的大部分信息。

### 质押 <!-- /part2/incentives/staking/ -->

<div class="summary">

  - 在权益证明中，质押提供三个功能：反女巫机制、问责机制和激励机制。
  - 32 个以太币的质押数量是在网络开销、验证者数量和最终确定时间之间的权衡。
  - 结合 Casper FFG 规则，质押提供了经济上的最终确定性：这是链安全性的可量化的衡量标准。

</div>

#### 引言
质押是以太坊 2 协议的完整参与者必须锁定的存款。质押永久存放在以太坊链上的存款合约（[deposit contract](/part2/deposits-withdrawals/contract/)）中，并反映在验证者在信标链上的记录余额中。质押使验证者有权提议区块、对区块和检查点进行认证，并参与同步委员会，以换取积累到其信标链余额中的奖励。

在以太坊 2 中，质押有三个关键作用。

首先，质押是一个反女巫机制。以太坊 2 是一个任何人都可参与的无需许可的系统。无需许可的系统必须找到一种在其参与者之间分配影响力的方法。在协议中创建身份必须付出一定的代价，否则个人可以廉价地创建大量重复身份，使链不堪重负。在工作量证明链中，参与者的影响力与其哈希能力——一种有限资源[^fn-one-cpu-one-vote]——成正比。在权益证明链中，参与者必须质押一些该链的原生货币，这也是一种有限资源。每个质押者在协议中的影响力与他们锁定的质押成正比。

[^fn-one-cpu-one-vote]: In the Bitcoin white paper, Satoshi wrote that, "Proof-of-work is essentially one-CPU-one-vote", although ASICs and mining farms have long subverted this. Proof of stake is one-stake-one-vote.

其次，质押提供了问责（accountability）。在以太坊 2 中，有害的行为是有直接成本的。特定类型的有害行为可被唯一地归因于实施这些行为的质押者，他们的质押可以通过一个叫做“罚没（[slashing](/part2/incentives/slashing/)）”的过程被减少或完全取消。这样，我们就可以根据攻击者做出有害行为的成本来量化协议的[经济安全性](#economic-finality)。

第三，质押使激励一致。质押者必然拥有他们所守护的事物的一部分，并受到激励去做好守护。

#### 质押大小

在以太坊 2 中，每个验证者的质押大小为 32 个以太币。

这是一个折中值。它既要尽可能小，以允许广泛参与，又要足够大，以避免验证者过多。简而言之，如果我们减少质押，就有可能迫使质押者在带宽更高的网络上运行更昂贵的硬件，从而增加中心化的力量。

对单体化[^fn-monolithic]的 L1 区块链中，验证者数量的主要实际限制因素是实现最终确定性所需的消息传递开销。与其他 [PBFT](https://pmg.csail.mit.edu/papers/osdi99.pdf) 风格的共识算法一样，Casper FFG 需要两轮全体对全体的通信才能实现最终确定性。也就是说，所有节点要就一个永远不会被回滚的区块达成一致。

<!-- markdownlint-disable code-block-style ul-indent -->
[^fn-monolithic]: A monolithic blockchain is one in which all nodes process all information, be it transactions or consensus-related. Pretty much all blockchains to date, including Ethereum, have been monolithic. One way to escape the scalability trilemma is to go "modular". This is the intent behind Ethereum's [rollup-centric roadmap](https://ethereum-magicians.org/t/a-rollup-centric-ethereum-roadmap/4698).

    - More on the general scalability trilemma: [Why sharding is great](https://vitalik.ca/general/2021/04/07/sharding.html) by Vitalik.
    - More on modularity: [Modular Blockchains: A Deep Dive](https://volt.capital/blog/modular-blockchains) by Alec Chen of Volt Capital.
    - Well worth a read: Polynya's entertaining (if a little grumpy), [The horrific inefficiencies of monolithic blockchains](https://polynya.mirror.xyz/3-omFNK3uU0iAaYSpFz0f9rCvrDBjx0H3XOSDGXU8hY).
<!-- markdownlint-enable code-block-style ul-indent -->

按照 Vitalik 的[记法](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Why-32-ETH-validator-sizes)，如果我们可以承受每秒 $\omega$ 条消息的网络开销，希望到达最终确定性的时间为 $f$，那么最多可以有 $n$ 个验证者的参与，其中

$$
n \le \frac{\omega f}{2}
$$

我们希望 $\omega$ 的值是小的，以便让验证者尽可能广泛地参与，包括那些速度较慢的网络中的验证者。我们希望 $f$ 尽可能短，因为较短的最终确定性时间比较长的要有用得多[^fn-finality-utility]。这些要求综合起来，就意味着对 $n$，即验证者总数的限制。

<!-- markdownlint-disable code-block-style -->
[^fn-finality-utility]: In an [unfinished paper](https://github.com/ethereum/research/blob/master/papers/casper-economics/casper_economics_basic.pdf) Vitalik attempts to quantify the "protocol utility" for different times to finality.

    > ...a blockchain with some finality time $f$ has utility roughly $-\log(f)$, or in other words increasing the finality time of a blockchain by a constant factor causes a constant loss of utility. The utility difference between 1 minute and 2 minute finality is the same as the utility difference between 1 hour and 2 hour finality.

    He goes on to make a justification for this (p.10).
<!-- markdownlint-enable code-block-style -->

这是一个经典的可扩展性三难问题（scalability trilemma）。个人而言，我并不觉得这些三角形图片很直观，但它们已经成为表示其中权衡的标准方式。

<a id="img_incentives_scalability_trilemma"></a>
<figure class="diagram" style="width: 60%">

![A version of the scalability trilemma.](images/diagrams/incentives-scalability_trilemma.svg)

<figcaption>

可扩展性三难问题的一个版本：选择任意两个。

</figcaption>
</figure>

1. 理想状况可能是具有高参与度（大 $n$）和低开销（小 $\omega$）——许多在低规格的机器上的质押者——但最终确定性将需要很长时间，因为消息交换会很慢。
2. 我们可以有非常快的最终确定性和高参与度，但需要质押者在高带宽网络上运行高规格机器才能参与。
3. 或者我们可以通过严格限制参与者数量，在相对适度的机器上实现快速的最终确定性。

目前还不清楚如何将以太坊 2 置于这样的图表中，但我们肯定更倾向于参与，而不是实现最终确定性的时间：也许 “X” 标记出了这个点。一个复杂的问题是，参与和开销不是彼此完全不相关的：我们可以降低质押来鼓励参与，但这会增加硬件和网络要求（开销），而这往往会减少能够或愿意参与的人数。[^fn-exercise-triangle]

[^fn-exercise-triangle]: Exercise for the reader: try placing some of the other monolithic L1 blockchains within the trade-off space.

具体来说，验证者数量的硬性限制是以太币供应总量除以质押大小。以 32 以太币的质押计算，目前约有 360 万验证者，这与 768 秒（两个时段）的最终确定性时间以及每秒 9375 条消息的消息开销一致[^fn-message-overhead]。这意味着每秒需要处理大量消息。不过，我们并不期望所有以太币都被质押，也许只有 10-20% 左右。此外，由于使用了 BLS 聚合签名（[BLS aggregate signatures](/part2/building_blocks/signatures/)），消息大小被高度压缩至每个验证者近似于 1 位。

[^fn-message-overhead]: Vitalik's [estimate](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Why-32-ETH-validator-sizes) of 5461 is too low since he omits the factor of two in the calculation.

考虑到当前 p2p 网络的容量，以及在两个时段内实现最终确定性，每一质押  32 个以太币已经是我们能做到的极限。据我所知，我的质押节点持续消耗大约 3.5mb/s 的上下行带宽，差不多是我家 ADSL 上行带宽的 30%。如果协议更“健谈”，很多人就不能用家庭网络去进行质押。

另一种方法可能是对同一时间内活跃的验证者数量[设置上限](https://github.com/ethereum/consensus-specs/issues/2137)，从而对信息交换数量设置上限。有了这样的方法，我们就可以探索将质押降低到 32 个以太币以下的方案，让更多的验证者参与进来，但每个验证者只能以“兼职”的方式参与。

请注意，这种分析忽略了节点（它必须处理消息）和验证者（单个节点可以托管大量验证者）之间的区别。以太坊 2 协议的一个设计目标是最大限度地减少规模经济，使个体质押者与质押池之间尽可能平等。因此，将分析应用于最分布式的情况时（即每个节点只有一个验证者）我们应该小心。

趣事：最初的“混合 Casper FFG 权益证明提案（[EIP-1011](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-1011.md)）”要求最低存款额为 1500 个以太币，因为系统设计可以处理多达约 900 个活跃验证者。虽然现在对大多数人来说， 32 个以太币都是一笔巨款，但现在也有了可接受少于 32 个以太币的去中心化质押池。

#### 经济最终确定性

要求验证者锁定质押，并引入罚没条件，这些使我们能够在某种意义上量化信标链的安全性。

我们希望阻止的主要攻击是改写信标链历史。这种攻击的成本使得信标链的安全性被参数化。在工作量证明中，这就是在一段时间内获得压倒性（51%）哈希能力的成本。有趣的是，在工作量证明中，成功的 51% 攻击基本上不需要付出任何代价，因为攻击者可以认领被重写的链上的所有区块奖励。

在以太坊的权益证明协议中，我们可以用经济最终确定性去衡量安全性。也就是说，如果攻击者想要回滚链上的一个已被最终确定的区块，成本会是什么？

这实际上很容易量化。引用 Vitalik 的《参数化 Casper》（[Parametrizing Casper](https://medium.com/@VitalikButerin/parametrizing-casper-the-decentralization-finality-time-overhead-tradeoff-3f2011672735)）：

> 如果足够多的验证者签署了认证 $H_1$ 的消息，那么状态 $H_1$ 在经济上就被最终确定了。此时有了这一特性：如果 $H_1$ 和一个与其相冲突的 $H_2$ 都被最终确定，那么就有证据证明至少 $\frac{1}{3}$ 的验证者是恶意的，并销毁他们的全部存款。

以太坊的权益证明协议具有这个属性。为了最终确定一个检查点（$H_1$），三分之二的验证者必须对其进行认证。要最终确定一个相冲突的检查点（$H_2$）也需要三分之二的验证者对其进行认证。因此，一定是至少有三分之一的验证者对这两个检查点都进行了认证。由于每个验证者都会在自己的认证上签名，因此这种情况既可以被发现，也可以被归咎：很容易就能在链上提交证据，证明这些验证者自相矛盾，而他们也会受到协议的惩罚。

如果三分之一的验证者同时被罚没，他们的全部有效余额都将被烧毁（每个人最多 32 个以太币）。在这种情况下，假设总共有 1500 万个以太币被质押，那么回滚一个已经最终确定的区块的成本是永久销毁攻击者的 500 万个以太币，并将攻击者驱逐出网络。

这时有必要引用（或转述）Vlad Zamfir 的话：如果用工作量证明去描述权益证明，“这就像是如果你参与 51% 攻击，你的 ASIC 农场就会被烧毁一样”。

想要了解关于经济最终确定性机制的更多信息，可以参阅下文“[罚没](/part2/incentives/slashing/)”部分；想要知道相关原理和依据，请参阅 “[Casper FFG](/part2/consensus/casper_ffg/)” 部分。

#### 另见

  - 《参数化 Casper：权衡去中心化、最终确定性的时间、与开销》（[Parametrizing Casper: the decentralization/finality time/overhead tradeoff](https://medium.com/@VitalikButerin/parametrizing-casper-the-decentralization-finality-time-overhead-tradeoff-3f2011672735)）呈现了对不同质押规模的权衡的早期推理。自那以后，情况有所发展，最显著的是 BLS 聚合签名的出现。
  - Vitalik 的《宁静升级的设计原理》（Serenity Design Rationale）中的“为什么验证者规模是 32 个以太币？（[Why 32&nbsp;ETH validator sizes?](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Why-32-ETH-validator-sizes)）”
  - Vitalik 关于实现单个时隙最终确定性（[single slot finality](https://notes.ethereum.org/@vbuterin/single_slot_finality)）的讨论文档从另一个角度探讨了参与、开销、以及最终确定性的权衡空间。

### 余额 <!-- /part2/incentives/balances/ -->

<div class="summary">

  - 除了实际余额外，每个验证者还维护一种有效余额（effective balance）。
  - 验证者在协议中的影响力与其有效余额成正比，对验证者的奖励和惩罚也是如此。
  - 有效余额跟踪验证者的实际余额，但在设计进行了优化，有效余额变化的频率要比实际余额低得多。
  - 验证者有效余额的上限为 32 个以太币。

</div>

#### 引言

对于每个验证者的余额，信标链会维护两份独立的记录：实际余额和有效余额。

验证者的实际余额直截了当，它是：通过存款合约而为验证者存入的资金的总和，加上累积的信标链奖励，减去累积的惩罚和取款。实际余额的变化很快，所有活跃验证者的实际余额至少每个时段会更新一次，同步委员会参与者的实际余额则会每个时隙更新一次。它的颗粒度也很细：实际余额的单位是 Gwei，即 $10^{-9}$ 个以太币。

验证器的有效余额是根据其实际余额推导出来的，其变化速度要慢得多。为了实现这一点，有效余额的单位是整数的以太币（参见 [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment)），有效余额的变化受迟滞（[hysteresis](#hysteresis)）的约束。

使用有效余额实现了两个目标，一个与经济有关，另一个纯粹是工程学。

#### 有效余额的经济学面向

有效余额的引入最初是为了表示验证者的“最大风险余额（[maximum balance at risk](https://github.com/ethereum/consensus-specs/pull/162#issuecomment-441759461)）”，上限为 32 个以太币。验证者的实际余额可能要高得多，例如，如果不小心存入了两笔资金，验证者的实际余额为 64 个以太币，但有效余额只有 32 个。我们可以设想一个协议，让每个验证者的影响力与其上不封顶的实际余额成正比，但这样会使委员会成员的组成变得复杂。相反，我们设置了有效余额的上限，如果质押者希望进行更多质押，则要求其向更多验证者账户中存款。[^fn-eip-7251]

[^fn-eip-7251]: 事实上，有一个提案提议[将最大有效余额增加到](https://notes.ethereum.org/@mikeneuder/eip-7251-faq) 2048 个以太币。

有效余额的范围迅速扩大，如今它完全代表了验证者在共识协议中的权重。

以下所有与共识相关的量都与验证者的有效余额成正比：

  - [被选为](/part3/helper/misc/#def_compute_proposer_index)信标区块提议者的概率；
  - 验证者在 LMD-GHOST [分叉选择规则](/part3/forkchoice/phase0/#get_weight)中的权重；
  - 验证者在合理化和最终确定性的[计算](/part3/transition/epoch/#def_weigh_justification_and_finalization)中的权重；以及
  - 被[纳入](/part3/helper/accessors/#def_get_next_sync_committee_indices)同步委员会的概率。

相应地，以下奖惩措施也根据有效余额进行加权：

  - 验证者的基础奖励（[base reward](/part3/transition/epoch/#def_get_base_reward)）（根据该奖励计算认证的奖励和惩罚）；
  - 因怠惰流失（inactivity leak）而导致的怠惰惩罚（[inactivity penalties](/part2/incentives/inactivity/)）；以及
  - [初始](/part2/incentives/slashing/#the-initial-penalty)罚没惩罚和[相关](/part2/incentives/slashing/#the-correlation-penalty)罚没惩罚。

不过，区块提议者的奖励并不与提议者的有效余额成比例。由于验证者被选中去提议区块的概率与其有效余额成正比，因此奖励与有效余额的比例已经得到了考虑。出于同样的原因，同步委员会的奖励也不与参与者的有效余额成比例。

#### 有效余额的工程学面向

我们只需使用验证者的实际余额作为权重（上限为 32 个以太币），就能实现上述所有功能。但如果一切以有效余额为基础，我们能获得显著的性能优势。

首先，有效余额在每个时段只[更新](/part3/transition/epoch/#def_process_effective_balance_updates)一次，这意味着我们只需计算每次增量的基础奖励（[base reward per increment](/part2/incentives/issuance/#the-base-reward-per-increment)），然后缓存整个时段的结果，而无需考虑实际余额的任何变化。

但有效余额的主要特点是，它们的变化要比这个频率小得多。要做到这一点，就必须使其具有非常高的[颗粒度](#increments)，并对任何更新都采用[迟滞机制](#hysteresis)。

对信标链状态转换的计算的一大性能挑战是生成整个状态的哈希树根。默克尔化（[Merkleization](/part2/building_blocks/merkleization/)）过程允许去缓存状态中未发生变化的部分，从而大大提高性能。

状态中的验证者记录列表是一个大型数据结构。如果我们将验证器者的实际余额存储在这些记录中，它们就会经常变化，整个数据结构至少需要每个时段被重新散列一次。

解决这个问题的[第一种方法](https://github.com/ethereum/consensus-specs/pull/317/files)就是直接将验证者的余额从验证者记录中移出，放到状态中的另一个专用列表中。这样就减少了重新散列的次数，因为在仅有验证者的余额发生变化时，整个验证者列表就不需要被重新散列。

不过，这也导致了其他方面的性能问题。需要验证者余额信息的轻客户端现在就得从状态的两个不同部分获取数据，即验证者记录和验证者余额列表。这就需要两个而不是一个默克尔证明，进而大大增加了带宽成本。

解决这个问题的办法是，在验证者记录中存储一个缓慢变化的余额版本——这意味着不需要经常重新散列——而将快速变化的实际余额存储在一个单独的列表中。这样的话，重新散列的结构要小得多。

下方引文出自为实现有效余额的一个[早期尝试](https://github.com/ethereum/consensus-specs/issues/685)：

>【有效余额是一种】“近似的余额”，可供 `validator_registry` 中的轻客户端使用，从而将每个验证者需要下载的默克尔分支数量从 3 个减少到 2 个（实际上通常是从 ~2.01 个减少到 ~1.01 个，因为在获取委员会时，active_index_roots 中的默克尔分支大多是共享的），从而显著降低轻客户端的带宽成本。

重点是，轻型客户端不需要访问单独存储在状态中的实际余额列表，而只需要访问他们反正都会下载的验证者记录。

总之，在验证者记录中添加有效余额让我们能同时实现两个性能目标：避免在状态中频繁地重新散列验证者列表，同时不增加轻客户端的工作量。

##### 增量

尽管有效余额以 Gwei 为记录单位，但它们只能是 [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment) ——即 1 以太币（ $10^9$ Gwei）——的整数倍。实际余额可以是任意数量的 Gwei。

在规范中，这个倍数被称为“增量（increment）”，在计算[基础奖励](/part3/transition/epoch/#def_get_base_reward_per_increment)以及其他奖励和惩罚时会用到。由于 1 以太币的计算很顺手，我们可以在心里将“以太币”替换为“增量”，以便更直观理解它。

如果将有效余额以增量而非 Gwei 的形式存储，可能会更清晰。这样做肯定会减少因 `EFFECTIVE_BALANCE_INCREMENT` 而进行的除法和乘法运算，以及相关的[算术溢出](https://github.com/ethereum/consensus-specs/pull/1286)风险。但现行版本是随着时间演变而来的，现在回头改变可能会带来风险和不便。

##### 迟滞

通过在计算中加入[迟滞](https://en.wikipedia.org/wiki/Hysteresis)，有效余额的变化速度必然比实际余额慢得多。

在我们的情境中，迟滞意味着，如果有效余额是 31 个以太币，实际余额必须上升到 32.25 个以太币，才能触发有效余额更新至 32 个以太币。同样，如果有效余额是 31 个以太币，那么实际余额必须下降到 30.75 个以太币才能触发有效余额更新至 30 个以太币。

下图说明了这种行为：

  - 实际余额和有效余额都从 32 个以太币开始。
  - 一开始实际余额会上升。有效余额被限制在 32 个以太币，因此不会更新。
  - 只有当实际余额下降到 31.75 个以太币以下时，有效余额才会减少到 31 个以太币。
  - 尽管实际余额上升并围绕 32 个以太币波动，但这不会触发有效余额更新，它保持在 31 个以太币。
  - 最终实际余额上升到 32.25 个以太币以上，有效余额更新为 32 个以太币。
  - 尽管实际余额再次下降，但没有低于 31.75 个以太币，因此有效余额保持在 32 个以太币。

<a id="img_hysteresis"></a>
<figure class="chart">

![A graph illustrating actual balance versus effective balance.](images/charts/hysteresis.svg)

<figcaption>

对验证者的实际金额（实线）和有效余额（虚线）之间关系的图示。点线是有效余额更新的阈值——迟滞。

</figcaption>
</figure>

The hysteresis levels are controlled by the [hysteresis parameters](/part3/config/preset/#hysteresis-parameters) in the spec:

| 名字 | 值 |
| - | - |
| `HYSTERESIS_QUOTIENT` | `uint64(4)` |
| `HYSTERESIS_DOWNWARD_MULTIPLIER` | `uint64(1)` |
| `HYSTERESIS_UPWARD_MULTIPLIER` | `uint64(5)` |

在每个时段结束时，这些参数被应用于[有效余额的更新](/part3/transition/epoch/#effective-balances-updates)。状态中的每个验证者（无论是否活跃）的有效余额都会按照以下方式更新：

  - 如果实际余额小于有效余额减去 0.25 (`=` `HYSTERESIS_DOWNWARD_MULTIPLIER` `/` `HYSTERESIS_QUOTIENT`) 增量（以太币），则将有效余额减少一个增量。
  - 如果实际余额大于有效余额加上 1.25 (`=` `HYSTERESIS_UPWARD_MULTIPLIER` `/` `HYSTERESIS_QUOTIENT`) 增量（以太币），则将有效余额增加一个增量。

迟滞功能的后果是，有效余额发生一次变化的时间间隔不会超过验证者的实际余额变化 0.5 个以太币所需的间隔——通常需要几周甚至几个月。

###### 一个边缘案例

迟滞设计引出了存款处理中一种有趣的[边缘情况](https://github.com/ethereum/consensus-specs/issues/3049)。[存款合约](https://github.com/ethereum/consensus-specs/blob/v1.3.0/solidity_deposit_contract/deposit_contract.sol)允许质押者存入大于或等于 1 个以太币的任何金额；存款不必恰好是 32 个以太币。这样就允许通过多次存款去累积质押。例如，先存 24 个以太币，再单独存 8 个，这就有了一个完整质押，并将在处理第二笔存款后激活验证者。

这种边缘情况发生在：验证者最终存款的实际余额达到 32 个以太币或更多，但由于迟滞设计，这不足以将其有效余额更新为 32 个以太币。例如，存入 31 个以太币后，验证者的实际余额和有效余额都将是 31 个以太币。再存入 1 个以太币将使验证者的实际余额达到 32 —— 技术上使该验证者有资格被激活 —— 但由于迟滞计算，其有效余额仍将是 31 个以太币。因此它不会被激活。

验证者 [418408](https://beaconcha.in/validator/b6c1531b7896e3493806a8dd72fa9c3387f4f7a2fdc565bf1e8e66becb0666f8c3938270a757703e3865619dcc34bf7c#deposits) 就是这种情况在主网上发生的一个例子。它的倒数第二次存款是 1 个以太币，这使验证者的总余额达到 32 个以太币，但直到再次存入 1 个以太币以强制更新有效余额后，该验证者才被激活。

[TODO: link to deposit contract section when written]::

###### 一点历史说明

迟滞功能的最初实现实际上[是](https://github.com/ethereum/consensus-specs/pull/1627#discussion_r387294528) `QUOTIENT = 2`, `DOWNWARD_MULTIPLIER = 0`, `UPWARD_MULTIPLIER = 3`。这意味着，如果一个实际余额为 32 个以太币的验证者遭受轻微的初始中断，它的有效余额会立即下降到 31 个以太币。要恢复到 32 个以太币的有效余额，它的实际余额需要达到 32.5 个以太币。同时由于有效余额降低，验证者的奖励会低 3.1%。这[似乎不太公平](https://github.com/ethereum/consensus-specs/issues/1609)，并激励了质押者“过度存入”以太币，以避免初始有效余额下降的风险，因此我们[改为](https://github.com/ethereum/consensus-specs/pull/1627)采用当前参数。

#### 另见

规范中：

  - 限制有效余额的预设值，[`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) 和 [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment)。
  - [控制迟滞的参数](/part3/config/preset/#hysteresis-parameters)。
  - 函数 [`process_effective_balance_updates()`](/part3/transition/epoch/#def_process_effective_balance_updates) 被用于实际计算和应用于迟滞。
  - 验证者（ [`Validator`](/part3/containers/dependencies/#validator)）对象存储有效余额。信标状态中的[注册表](/part3/containers/state/#registry)包含验证者列表以及单独的实际余额列表。

### 发行量 <!-- /part2/incentives/issuance/ -->

<div class="summary">

  - 发行量是指协议为激励参与者而新创造的以太币的数量。
  - 在理想运行状态下的信标链每个时段都会发行一定数量的以太币，新发行的以太币数量是每增量的基本奖励的倍数。
  - 总发行量与验证者数量的平方根成正比。这不是一个完全任意的选择。

</div>

#### 引言

我们可以从三个角度来看待激励验证者正确参与协议所给予的奖励。

首先是“发行量”，即协议为支付奖励而生成的新以太币的总量。其次是验证者可能在长期内预期获得的奖励。最后是任何特定验证者实际获得的奖励。

在本节中，我们将探讨发行量，而在[下一节](/part2/incentives/rewards/)中，我们将探讨奖励。这两者之间有很强的关联性，因此这一分离中间藕断丝连。

首先我们必须定义奖励的基本单位，即“每增量基础奖励（base reward per increment）”。

#### 每增量基础奖励

所有奖励都是根据“每增量基础奖励”来计算的。这反过来又是被这样[计算](/part3/transition/epoch/#def_get_base_reward_per_increment)的：

```none
Gwei(EFFECTIVE_BALANCE_INCREMENT * BASE_REWARD_FACTOR // integer_squareroot(get_total_active_balance(state)))
```

为了简洁起见，我们将每增量基础奖励称为 $b$。一个增量是一个有效余额单位，即 1 个以太币 ([`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment)), 因此活跃验证者最多有 32 个增量。

如果我们希望改变信标链上以太币的发行率，[`BASE_REWARD_FACTOR`](/part3/config/preset/#base_reward_factor) 是我们可以调整的大旋钮。截至目前，它一直被设置为 64，这带来了我们下面看到的发行图。这个值似乎运行得非常好，我们没有计划改变它。

#### 奖励来自发行量

发行量是协议为了激励其参与者而创建的新以太币数量。在计算了处罚、燃烧的交易费等之后的净发行量有时被称为通货膨胀或供应增长。

合并前，Eth1 链以区块和叔块奖励的形式发行新的以太币。自伦敦升级以来，由于 [EIP-1559](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-1559.md)，这种发行在一定程度上被交易基础费用的燃烧抵消，被燃烧的基础交易费有时甚至会超过新发行的以太币。

合并后，Eth1 链不再发行任何区块或叔块奖励。但基础费用的燃烧仍然存在。净发行量有可能变为负数——即销毁的以太币比创建的多[^fn-ultrasound-money]——至少在短期到中期内。从长远来看，Anders Elowsson 认为，由于权益证明的以太币发行和由于 EIP-1559 的以太币销毁，将出现[流通中供应量的均衡](https://ethresear.ch/t/circulating-supply-equilibrium-for-ethereum-and-minimum-viable-issuance-during-the-proof-of-stake-era/10954?u=benjaminion)。

[^fn-ultrasound-money]: You can see Ethereum's current issuance and play with various scenarios at [ultrasound.money](https://ultrasound.money/).

在以下内容中，我们假设信标链运行良好，即所有验证者都完美地履行其职责。实际上，在一个无需许可的、全球分布式的、点对点网络上，这是不可能实现的，尽管信标链在其大部分历史中一直在近似最佳状态下运行。实际的验证者奖励和净发行量肯定会因网络参与率的不同而有不同程度的降低。

#### 总体发行量

在我们假设的理想条件下，信标链被设计为每个时段发行总计 $Tb$ 的奖励。这里的 $T$ 是所有活跃验证者持有的增量总数，换句话说，就是他们所有的以太币有效余额总和。这是信标链可以生成的最大发行量（最大数量的新以太币）。如果所有 $N$ 个验证者都有最大 32 个以太币的有效余额，那么这将在每个时段总共发行 $32Nb$ Gwei。

每年有 $365.25 \times 225 = 82181.25$ 个时段，且  [`BASE_REWARD_FACTOR`](/part3/config/preset/#base_reward_factor) $= 64$,

$$
\begin{aligned}
\text{每年最大发行量} &= 82181.25 \times \frac{32 \times 64 \times N}{\sqrt{32 \times 10^9 \times N}} \text{ETH} \\
                             &= 940.8659 \sqrt{N} \\
\end{aligned}
$$

<!-- Number of validators -->

对于 50 万个验证者，这相当于每年新发行 665292 个以太币，再加上零头。相比之下，在工作量证明下，以太坊的区块和叔块奖励每年几乎达到了五百万个以太币。

我们可以将最大发行量绘制为验证者数量的函数。它只是一个缩放的平方根曲线。

<a id="img_issuance_curve"></a>
<figure class="chart">

![A graph of maximum annual protocol issuance on the beacon chain as a function of the number of active validators.](images/charts/issuance_curve.svg)

<figcaption>

信标链上每年最大协议发行量与有效验证者数量的函数关系。

</figcaption>
</figure>

#### 验证者奖励

The goal is to distribute these rewards evenly among validators (continuing to assume that things are running optimally), so that, on a long term average, each validator $i$ earns $n_{i}b$ Gwei per epoch, where $n_i$ is the number of increments it possesses, equivalently its effective balance in Ether. In these terms $T = \sum^{N-1}_{i=0}{n_i}$.

Given this, a well-performing validator with a 32&nbsp;ETH effective balance can expect to earn a long-term average of $32b$ Gwei per epoch. Of course, $b$ changes over time as the total active balance changes, but in the absence of a mass slashing event that change will be slow.

Similarly to the issuance calculation, we can calculate the expected annual percentage reward for a validator due to participating in the beacon chain protocol:

$$
\begin{aligned}
\text{APR} &= 100 \times 82181.25 \times \frac{64}{\sqrt{32 \times 10^9 \times N}} \% \\
           &= \frac{2940.21}{\sqrt{N}} \% \\
\end{aligned}
$$

<!-- Number of validators -->

For example, with 500,000 validators participating, this amounts to an expected return of 4.16% on a validator's effective balance.

Graphing this give us an inverse square root curve.

<a id="img_rewards_curve"></a>
<figure class="chart">

![A graph of the expected annual percentage rewards for stakers as a function of the number of active validators.](images/charts/rewards_curve.svg)

<figcaption>

The expected annual percentage rewards for stakers as a function of the number of active validators.

</figcaption>
</figure>

#### Inverse square root scaling

The choice to scale the per-validator expected reward with $\frac{1}{\sqrt{N}}$ is not obvious, and we can imagine different scenarios.

If we model the per-validator reward as $r \propto N^{-p}$, then some options are as follows.

1. $p = 0$: each validator earns a constant return regardless of the total number of validators. Issuance is proportional to $N$.
2. $p = \frac{1}{2}$: issuance scales like $\sqrt{N}$, the formula we are using.
3. $p = 1$: each validator's expected reward is inversely proportional to the total number of validators. Issuance is independent of the total number of validators.

Adopting a concave function is attractive as it allows an equilibrium number of validators to be discovered without constantly fiddling with parameters. Ideally, if more validators join, we want the per-validator reward to decrease to disincentivise further joiners; if validators drop out we want the per-validator reward to increase to encourage new joiners. Eventually, an equilibrium number of validators will be found that balances the staking reward against the perceived risk and opportunity cost of staking. Assuming that the protocol is not overly sensitive to the total number of validators, this seems to be a nice feature to have.

That would rule out the first, $p=0$, option. The risk with $p = 0$ is that, if the reward rate is set lower than the perceived risk, then all rational validators will exit. If we set it too high, then we end up paying for more security than we need (too many over-incentivised validators). Frequent manual tuning via hard-forks could be required to adjust the rate.

The arguments for selecting $p = \frac{1}{2}$ over $p = 1$ are quite subtle and relate to [discouragement attacks](/part2/incentives/rewards/#discouragement-attacks). With $p \ne 0$, a set of validators may act against other validators by censoring them, or performing other types of denial of service, in order to persuade them to exit the system, thus increasing the rewards for themselves. Subject to various assumptions and models, we find that we require $p \le \frac{1}{2}$ for certain kinds of attack to be unprofitable. Essentially, we don't want to increase rewards too much for validators that succeed in making other validators exit the beacon chain.

Note that since the Merge, validators' income can include a significant component from transaction priority fees and MEV. This has the effect of pushing $p$ closer to $1$, and much of the reasoning above becomes moot. Discouragement attacks in this regime are an unsolved problem.

#### See also

For more background to the $\frac{1}{\sqrt{N}}$ reward curve, see

  - [Casper: The Fixed Income Approach](https://ethresear.ch/t/casper-the-fixed-income-approach/218?u=benjaminion),
  - Vitalik's [Serenity Design Rationale](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Base-rewards), and
  - the [Discouragement Attacks](https://github.com/ethereum/research/blob/master/papers/discouragement/discouragement.pdf) paper.

Anders Elowsson's work on Ethereum's circulating supply equilibrium and minimum viable issuance takes a deeper look at the relationship between staking issuance and total Ether supply. See his [post and comments](https://ethresear.ch/t/circulating-supply-equilibrium-for-ethereum-and-minimum-viable-issuance-during-the-proof-of-stake-era/10954?u=benjaminion) on Ethresear.ch, and [ETHconomics presentation](https://www.youtube.com/watch?v=LtEMabS0Oas) at Devconnect 2022.

### Rewards <!-- /part2/incentives/rewards/ -->

<div class="summary">

  - Validators receive rewards for making attestations according to their view of the chain, proposing blocks, and participating in sync committees in varying proportions.
  - Votes that make up attestations must be both correct and timely in order to be rewarded.
  - The proposer's reward is a fixed proportion, $\frac{1}{7}$, of the total reward for all the duties it includes in its block.
  - A validator's expected long-term reward is $nb$ per epoch (number of increments times the base reward per increment), but there is significant variance due to the randomness of proposer and sync committee assignments.
  - Rewards are scaled both with a validator's effective balance and with the total participation rate of the validator set.
  - The need to defend against discouragement attacks has shaped various aspects of the protocol.

</div>

#### Introduction

In this section we will consider only rewards. We'll cover penalties in the [next](/part2/incentives/penalties/) section.

The beacon chain protocol incentivises each validator to behave well by providing rewards for three activities as follows.

1. Attesting to its view of the chain as part of the consensus protocol:
   - voting for a source checkpoint for Casper FFG;
   - voting for a target checkpoint for Casper FFG; and
   - voting for a chain head block for LMD-GHOST.
2. Proposing beacon chain blocks.
3. Signing off on blocks in the sync committees that support light clients.

The first of these, making attestations, happens regularly every epoch and accounts for the majority of a validator's total expected reward.

However, validators are selected at random to propose blocks or participate in sync committees, so there is a natural variance to the latter two rewards. Over the long run, the expected proportion of rewards earned for each activity breaks down as per the following chart.

<a id="img_incentives_weights"></a>
<figure class="diagram" style="width:50%">

![A piechart of the proportion of a validator's total reward derived from each activity.](images/diagrams/incentives-weights.svg)

<figcaption>

The proportion of a validator's total reward derived from each activity.

</figcaption>
</figure>

These proportions are set by the [incentivisation weights](/part3/config/constants/#incentivization-weights) in the spec. For convenience, I've assigned a symbol to each weight in the last column.

| Name | Value | Percentage | Symbol |
| - | - | - | - |
| `TIMELY_SOURCE_WEIGHT` | `uint64(14)` | 21.9% | $W_s$ |
| `TIMELY_TARGET_WEIGHT` | `uint64(26)` | 40.6% | $W_t$ |
| `TIMELY_HEAD_WEIGHT`   | `uint64(14)` | 21.9% | $W_h$ |
| `SYNC_REWARD_WEIGHT`   | `uint64(2)`  | 3.1%  | $W_y$ |
| `PROPOSER_WEIGHT`      | `uint64(8)`  | 12.5% | $W_p$ |
| `WEIGHT_DENOMINATOR`   | `uint64(64)` | 100%  | $W_{\Sigma}$ |

One further reward is available to block proposers for reporting violations of the slashing rules, but this ought to be very rare, and we will ignore it in this section (see [Slashing](/part2/incentives/slashing/) for more on this).

Rewards are newly created Ether that is simply added to validators' balances on the beacon chain.

#### Eligibility for rewards

There are three relevant milestones in a validator's lifecycle: its activation epoch, its exit epoch, and its withdrawable epoch. Eligibility for rewards, penalties and slashing vary based on these.

<a id="img_incentives_rewards_eligibility"></a>
<figure class="diagram" style="width:80%">

![A timeline of the eligibility of validators for rewards.](images/diagrams/incentives-rewards_eligibility.svg)

<figcaption>

Timeline of the eligibility of validators for rewards

</figcaption>
</figure>

Validators may receive rewards only between their activation and exit epochs. Note that, after submitting a voluntary exit, there may be a delay while the validator moves through the exit queue until its exit epoch is passed. The validator is expected to participate as usual during this period.

Similarly, validators receive penalties only between their activation and exit epochs. The exception to this is slashed validators. As a [special case](/part2/incentives/slashing/#other-penalties), slashed validators continue to receive penalties until they reach their withdrawable epoch, which may be long after their exit epoch.

All unslashed validators that are between their activation epoch and their withdrawable epoch are liable to being slashed.

[TODO: link to validator lifecycle when done]::

#### Rewards scale with effective balance

As described [earlier](/part2/incentives/balances/#economic-aspects-of-effective-balance), all rewards are scaled in proportion to a validator's effective balance. This reflects the fact that a validator's influence (weight) in the protocol is proportional to its effective balance.

If a validator has $n$ increments (that is, an effective balance of $n \times$ `EFFECTIVE_BALANCE_INCREMENT`, or $n$&nbsp;ETH in other words) then its expected[^fn-expected-value] income per epoch is $nb$, where $b$ is the [base reward per increment](/part2/incentives/issuance/#the-base-reward-per-increment).

[^fn-expected-value]: I'm using the word "expected" in its [technical sense](https://en.wikipedia.org/wiki/Expected_value) here. Due to [randomness](#individual-validator-rewards-vary) there is a chance that some validators earn less and a chance that some validators earn more. The averagely lucky validator can expect their rewards to average out to $nb$ Gwei per epoch over the long term.

For the regular attestations that occur every epoch, this is achieved explicitly by multiplying the base reward by the number of increments in [`get_base_reward()`](/part3/transition/epoch/#def_get_base_reward).

For the random elements &ndash; block proposals and sync committee participation &ndash; the scaling is achieved implicitly by modifying the probability that a validator is selected for duty to be proportional to $\frac{n}{T}$, where $T$ is the total number of increments of the active validator set. So, if your effective balance is 24&nbsp;ETH, then you are 25% less likely to be selected to propose a block or join a sync committee than a validator with 32&nbsp;ETH. See [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index) and [`get_next_sync_committee_indices()`](/part3/helper/accessors/#def_get_next_sync_committee_indices) for the details.

#### Attestation rewards

The largest part, 84.4%, of validators' rewards come from making attestations. Although committee and slot assignments for attesting are randomised, every active validator will be selected to make exactly one attestation each epoch.

Attestations receive rewards only if they are included in beacon chain blocks. An attestation contains three votes. Each vote is eligible for a reward subject to conditions.

| Validity | Timeliness | Reward |
| - | - | - |
| Correct source                  | Within 5 slots  | $\frac{W_s}{W_{\Sigma}}nb$ |
| Correct source and target       | Within 32 slots | $\frac{W_t}{W_{\Sigma}}nb$ |
| Correct source, target and head | Within 1 slot   | $\frac{W_h}{W_{\Sigma}}nb$ |

These are cumulative, so the maximum attestation reward per epoch (for getting all three votes correct and getting the attestation included the next block) is $\frac{W_s + W_t + W_h}{W_{\Sigma}}nb$, or $0.84375nb$.

The full matrix of possible weights for an attestation reward is as follows. In each case we need to multiply by $\frac{nb}{W_{\Sigma}}$ to get the actual reward.

<a id="rewards-table"></a>

| Timeliness | 1 slot | <= 5 slots | <= 32 slots | > 32 Slots (missing) |
| - | - | - | - | - |
| Wrong source                    | 0                 | 0           | 0     | 0 |
| Correct source                  | $W_s$             | $W_s$       | 0     | 0 |
| Correct source and target       | $W_s + W_t$       | $W_s + W_t$ | $W_t$ | 0 |
| Correct source, target and head | $W_s + W_t + W_h$ | $W_s + W_t$ | $W_t$ | 0 |

But this is not the whole picture: we will also need to account for [penalties](/part2/incentives/penalties/) for incorrect or late attestations.

The maximum total issuance per epoch across all validators is

$$
I_A = \frac{W_s + W_t + W_h}{W_{\Sigma}}Tb
$$

where, once again, $T$ is the total number of increments of active validators (the sum of their effective balances in ETH terms).

##### Correctness

"Correct" in the above means that the attestation agrees with the view of the blockchain that the current block proposer has. If the attesting validator votes for different checkpoints or head blocks then it is on a different fork and that vote is not useful to us. For instance, if the source checkpoint vote is different from what we as proposer think it ought to be, then our view of the chain's history is fundamentally different from the attester's, and so we must ignore their attestation. The attestation will instead receive rewards in blocks on the other fork, and eventually one fork or the other fork will win. To disincentivise attacks it is important that only participants in the winning chain receive rewards.

##### Timeliness

One of the changes brought in with Altair was a tightening of the timeliness requirements for attestations. Previously, there were rewards for correctness and a separate reward for timely inclusion that declined as $\frac{1}{d}$, where $d$ was the inclusion distance in slots, up to a maximum of 32 slots. This led to oddities, like it being worth waiting slightly longer to make sure to get the head vote correct since that was worth more than any loss due to lateness of inclusion, even though a late head vote is pretty much useless.

The new timeliness reward better reflect the relative importance of the votes. A head vote that is older than one slot is not useful, so it gets no reward.

Target votes are always useful, but we only want to track attestations pertaining to the current and previous epochs, so we ignore them if they are older than 32 slots. The number 32 was chosen for reasons of fairness: whichever slot in an epoch validators attest, their attestations are valid for the same length of time.[^fn-eip7045-notes]

[^fn-eip7045-notes]: [EIP-7045](https://eips.ethereum.org/EIPS/eip-7045) proposes to change this in the [Deneb upgrade](/part4/history/deneb/) to accept and reward target votes for the whole of the current and previous epochs. Danny Ryan made an insightful presentation on the reasons for changing this, and a defence of why the changes remain fair to validators, in the [PEEPanEIP 114 session](https://www.youtube.com/watch?v=Z4tMgrreCN8).

The choice of distance for including the source vote is interesting. It is chosen to be $\lfloor \sqrt{\tt SLOTS\_PER\_EPOCH} \rfloor = \lfloor \sqrt{32} \rfloor = 5$, which is the geometric mean of 1 and 32, the head and target values. It's a somewhat arbitrary choice, but is intended to put a fully correct attestation on an exponentially decreasing curve with respect to timeliness: each step down in (net) reward happens after an exponentially increasing number of slots.[^fn-five-slots]

<a id="img_reward_timeliness"></a>
<figure class="chart">

![A graph of the net reward for a completely correct attestation as it gets older plotted against an exponential curve for comparison.](images/charts/reward_timeliness.svg)

<figcaption>

It is plausible that setting the inclusion distance for correct source to 5 gives a kind of exponential reduction in reward with time. This graph shows the net reward (reward + penalty) for a completely correct attestation as it gets older plotted against an exponential curve for comparison.

</figcaption>
</figure>

<!-- markdownlint-disable code-block-style -->
[^fn-five-slots]: This is taken from a [conversation](https://discord.com/channels/595666850260713488/595701173944713277/871340571107655700) on the Ethereum R&D Discord server:

    > vbuterin:<br/>
    > The rationale for the number 5 is just that 5 is geometrically halfway in between 1 and 32<br/>
    > And so we get the closest that makes sense to a smooth curve in terms of rewarding earlier inclusion<br/>
    > ...<br/>
    > ah I mean on an exponential curve, not quadratic<br/>
    > To me exponential feels more logical<br/>
    > What's a bigger improvement in quality, 4 slot delay vs 6 slot delay, or 20 slot delay vs 23 slot delay?
<!-- markdownlint-enable code-block-style -->

##### Remarks

Note that the attester does not have full control over whether it receives rewards or not. An attester may behave perfectly, but if the next block is skipped because the proposer is offline, then it will not receive the correct head block reward. Or if the next proposer happens to be on a minority fork, the attester will again forgo rewards. Or if the next proposer's block is late and gets orphaned - subsequent proposers are supposed to pick up the orphaned attestations, but there can be considerable delays if block space is tight. There are countless failure modes outside the attester's control.

It often perplexes stakers when, to all intents and purposes, their validators seem to be working perfectly, yet they still miss out on rewards or receive penalties. But this is the nature of permissionless, global, peer-to-peer networks. It is a testament to the quality of the protocol and the various client implementations that missed rewards have been surprisingly rare on the beacon chain so far.

#### Proposer rewards for attestations

If the attestations in a block are worth a total of $R$ in rewards to the attesters, then the proposer that includes the attestations in a block receives a reward of

$$
R_{A_P} = \frac{W_p}{W_{\Sigma} - W_p}R
$$

Thus, over an epoch, the maximum total issuance due to proposer rewards in respect of attestations is

$$
I_{A_P} = \frac{W_p}{W_{\Sigma} - W_p}I_A
$$

with $I_A$ being the maximum issuance to attesters per epoch, as above.

Thus, a proposer is strongly incentivised to include high value attestations, which basically means including them quickly, and including well-packed, as correct as possible aggregates.

#### Sync committee rewards

<!-- Number of validators -->

Once every [256](/part3/config/preset/#epochs_per_sync_committee_period) epochs (27.3 hours), [512](/part3/config/preset/#sync_committee_size) validators are selected to participate in the sync committee. For any given validator this will happen rarely; with 500,000 validators, the expected interval between being chosen for sync committee duty is around 37 months. However, during the 27-hour period of participation the rewards are relatively very large.

[TODO: link to explanation of sync committees when done]::

Sync committee participants receive a reward for every slot that they correctly perform their duties. With 512 members in the committee, and 32 slots per epoch, the reward per validator per slot for correct participation is

$$
R_Y = \frac{W_y}{32 \times 512 \times W_{\Sigma}}Tb
$$

The $T$ here is the total increments of the whole active validator set, so this is a large number. The per-epoch per-validator reward is 32 times this.

The maximum issuance per epoch to sync committee members in respect of their sync contributions is then

$$
I_Y = \frac{W_y}{W_{\Sigma}}Tb
$$

#### Proposer rewards for sync committees

As with attestations, the block proposer that includes the sync committee's output receives a reward proportional to the reward of the whole committee:

$$
R_{Y_P} = 512\frac{W_p}{W_{\Sigma} - W_p}R_Y
$$

So the maximum issuance per epoch to proposers for including sync committee contributions is

$$
I_{Y_P} = \frac{W_p}{W_{\Sigma} - W_p}I_Y
$$

#### Remarks on proposer rewards

You'll note that, for both attestations and sync committees, the proposer reward for including them in a block is a fixed fraction of the validator reward. If $R$ is the validator reward for a duty, then the proposer reward is $\frac{W_p}{W_{\Sigma} - W_p}R$. In [Vitalik's words](https://github.com/ethereum/annotated-spec/blob/master/altair/beacon-chain.md#aside-proposer-rewards-in-altair), "The proposer reward for a duty is the attester reward for that duty, multiplied by the _proposer reward as a fraction of everything but the proposer reward_" (emphasis his).

This factor works out to be $\frac{8}{56} = \frac{1}{7}$ which means that $\frac{7}{8}$ of rewards go to validators performing duties and $\frac{1}{8}$ to the proposers including the evidence in blocks.

In the following charts, I have separated out the validator rewards from the proposer rewards, and we can see that they have exactly the same division among the duties. The chart on the right should probably be one seventh of the size of the one on the left for true accuracy.

<a id="img_incentives_reward_split"></a>
<figure class="diagram">

![Piecharts showing that proposer and validator rewards are allocated in the same proportions for duties.](images/diagrams/incentives-reward_split.svg)

<figcaption>

On the left, the breakdown of expected rewards for validators for performing duties. On the right, the breakdown of rewards for proposers for including evidence of those duties.

</figcaption>
</figure>

This equivalence ensures that the interests of attesters and proposers are aligned.

#### Total issuance

To check that the calculations above are consistent with our [claim](/part2/incentives/issuance/#overall-issuance) that the maximum issuance by the beacon chain per epoch is $Tb$ Gwei, let us sum up the issuance due to the four rewards: attester rewards, proposer rewards in respect of attestation inclusion, sync committee rewards, and proposer rewards in respect of sync committee inclusion. The total maximum issuance per epoch is

$$
\begin{aligned}
I &= I_A + I_{A_P} + I_Y + I_{Y_P} \\
  &= \left(1 + \frac{W_p}{W_{\Sigma} - W_p}\right)\left(I_A + I_Y\right) \\
  &= \left(1 + \frac{W_p}{W_{\Sigma} - W_p}\right)\left(\frac{W_s + W_t + W_h + W_y}{W_{\Sigma}}\right)Tb \\
  &= \left(\frac{W_{\Sigma}}{W_{\Sigma} - W_p}\right)\left(\frac{W_{\Sigma} - W_p}{W_{\Sigma}}\right)Tb \\
  &= Tb
\end{aligned}
$$

as expected.

#### Rewards in numbers

<!-- Number of validators -->

The following calculations are based on 500 thousand active validators, all performing perfectly and all with 32&nbsp;ETH of effective balance.

  - [Base reward per increment](/part2/incentives/issuance/#the-base-reward-per-increment)
    - $b = \frac{1{,}000{,}000{,}000 \times 64}{\sqrt{32{,}000{,}000{,}000 \times 500{,}000}} = 505$ Gwei
  - Value of a single attestation
    - $R_A = \frac{14 + 26 + 14}{64}32b = 13{,}635$ Gwei
  - Value of a single sync committee contribution
    - $R_Y = \frac{2}{32 \times 512 \times 64}500{,}000 \times 32b = 15{,}411$ Gwei
  - Value of a block proposal due to attestations
    - $R_{A_P} = \frac{500{,}000}{32}\frac{8}{64-8}R_A = 30{,}435{,}267$ Gwei
    - Note: this can actually be higher if the chain is not performing perfectly, as after a skip slot the proposer can include high value attestations from the missed slot.
  - Value of a block proposal due to sync committee contributions
    - $R_{Y_P} = 512\frac{8}{64-8}R_Y = 1{,}127{,}204$ Gwei

Putting it all together, the total available reward per epoch across all validators is $500{,}000R_A + 32(512R_Y + R_{A_P} + R_{Y_P}) = 8{,}080{,}000{,}000$ Gwei (to 5 significant figures)

Finally, as a check-sum, $Tb = 500{,}000 \times 32b = 8{,}080{,}000{,}000 \text{ Gwei} = 8.080 \text{ ETH}$ issued per epoch.

#### Individual validator rewards vary

Actual individual validator returns, even on an optimally running beacon chain, will vary above and below the expected amounts, since block proposals and sync committee duties are assigned randomly. This leads to variance in the rewards, with some validators earning more and some earning less. Nonetheless, an average validator over a long period can expect to earn a return in line with $nb$ per epoch.

<!-- Number of validators -->

The following chart shows the expected distribution of annual rewards for 500,000 validators, all participating perfectly, each with 32&nbsp;ETH of effective balance. The mean reward is 1.3302&nbsp;ETH/year (the 4.16% number from [earlier](/part2/incentives/issuance/#validator-rewards)), and the median 1.3123&nbsp;ETH/year, but there is a large standard deviation of 0.1037 due to the randomness of being selected to propose blocks or participate in sync committees. In fact, ten percent of validators will earn less than 1.2175&nbsp;ETH in rewards over the year, and 10% more than 1.4704&nbsp;ETH, due solely to randomness in assigning duties.

<a id="img_reward_variance"></a>
<figure class="chart">

<!-- Number of validators -->

![A bar chart of the distribution of annual reward for 500,000 validators with 32&nbsp;ETH staked.](images/charts/reward_variance.svg)

<figcaption>

Distribution of annual beacon chain rewards for 500,000 perfectly performing validators with 32&nbsp;ETH staked. The variance comes from the probabilities of different numbers block proposals or sync committee assignments. Some values are not attainable in this idealised model.

</figcaption>
</figure>

A few remarks on this.

First, the Altair upgrade did not change the expected reward per validator, but it did change the variance considerably. This is due to an increase in the block reward of a factor of four and the introduction of sync committees, with a corresponding reduction in attestation rewards. Since block proposals and sync committee participation are randomly assigned, while attestation rewards are steady, Altair greatly increased the variance in actual rewards. For an analysis of the change, see [Pintail's article](https://pintail.xyz/posts/modelling-the-impact-of-altair/).

Second, there are further sources of variation that the above analysis doesn't account for. For example, if my validator proposes a block right after a skipped slot, in which there was no block, then my block proposal could be worth up to 71.4% more than a normal block proposal. This is because I get to include attestations from the skipped slot as well as from my own slot, and benefit from the extra source and target votes (but not the extra head votes, which will be too late, or the extra sync committee inclusion).

Third (and most significantly), post-Merge, validators additionally receive the transaction priority fees from execution payloads, and potentially MEV-related income as well. These can substantially increase the percentage earnings and variance in earnings for stakers, but will not affect overall issuance on the beacon chain since they come from recycled Ether rather than new issuance.

#### Rewards scale with participation

One surprising aspect of attestation rewards not so far mentioned is that they are scaled in proportion to participation. That is, for each duty (source, target, head vote) the attester's reward is scaled by the proportion of the total stake that made the same vote.

For example, if I made a correct head vote, and validators with 75% of the total effective balance increments made the same head vote, then I would receive $0.75 \times \frac{W_h}{W_{\Sigma}}nb$ reward for that vote.

A hand-wavy reason for this is that this scaling makes it to my advantage to help other validators get their attestations included. Several aspects of the protocol are not explicitly incentivised yet are somewhat expensive, such as forwarding gossip messages and attestation aggregation duty. This scaling provides me with an implicit reward for helping out other validators by providing these services: if they perform better, then I perform better.

For a more quantitative analysis, see on [discouragement attacks](#discouragement-attacks) below.

One interesting side effect of this is that, if participation drops by 10% (due to 10% of validators being offline, say), then total issuance of rewards due to attestations will fall by 19%, in addition to a further reduction from penalties.

We can calculate the participation rate at which net issuance due to attestations turns negative. With a participation rate $p$, the reward for a fully correct attestation is $0.844nbp$, and the penalty for a missed attestation is $0.625Tb$. This gives us a net issuance of $p^2(0.844Tb) - (1-p)(0.625Tb)$. The positive root of this is $p = 56.7\%$. But since this is below the 2/3 participation rate for finalisation, the [inactivity leak](/part2/incentives/inactivity/) will kick-in before we reach this level and completely change the reward and penalty profile, so the calculation is of theoretical interest only.

Note that the proposer reward is not scaled like this &ndash; proposers are already well incentivised to include all relevant attestations &ndash; and neither are sync committee rewards. Penalties do not scale with participation, either.

#### Discouragement attacks

Quoting from Vitalik's [Discouragement Attacks paper](https://github.com/ethereum/research/blob/master/papers/discouragement/discouragement.pdf),

> A discouragement attack consists of an attacker acting maliciously inside a consensus mechanism in order to reduce other validators' revenue, even at some cost to themselves, in order to encourage the victims to drop out of the mechanism.

Attackers might do this to gain more rewards with fewer participants in the system. Or they might do it as preparation for an attack on the chain: by reducing the number of validators they decrease their own cost of attack.

The paper goes into some quantitative analysis of different kinds of discouragement attacks. I would encourage you to read it and think through these things. As per the conclusion:

> In general, this is still an active area of research, and more research on counter-strategies is desired.

Some parts of the beacon chain design that have already been influenced by a desire to avoid discouragement attacks are:

  - the [inverse square root scaling](/part2/incentives/issuance/#inverse-square-root-scaling) of validator rewards;
  - the [scaling of rewards](#rewards-scale-with-participation) with participation;
  - the zeroing of attestation rewards during an [inactivity leak](/part2/incentives/inactivity/); and
  - rate limiting of validator exits, which means that an attacker needs to sustain an attack for longer, at greater cost to achieve the same ends.

#### See also

The detailed rewards calculations are defined in the spec in these functions:

  - Validator rewards for attestations are calculated in [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas) as part of [epoch processing](/part3/transition/epoch/).
  - Proposer rewards for attestations are calculated in [`process_attestation()`](/part3/transition/block/#def_process_attestation) as part of [block processing](/part3/transition/block/#block-processing).
  - Both validator and proposer rewards for sync committee participation are calculated in [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) as part of [block processing](/part3/transition/block/#block-processing).

The discussion of the variance of rewards is based on [Pintail's analysis of Altair](https://pintail.xyz/posts/modelling-the-impact-of-altair/). The code I used to generate the stats and the chart are based on the code in that article.

Discouragement attacks are analysed in a [paper](https://github.com/ethereum/research/blob/master/papers/discouragement/discouragement.pdf) by Vitalik.

### Penalties <!-- /part2/incentives/penalties/ -->

<div class="summary">

  - Validators that do not fulfil their assigned duties are penalised by losing small amounts of stake.
  - Receiving a penalty is not the same as being slashed!
  - Break-even uptime for a validator is around 43%.

</div>

#### Introduction

Incentivisation of validators on the beacon chain is a combination of carrot and stick. Validators are rewarded for contributing to the chain's security, and penalised for failing to contribute. As we shall see, penalties are quite mild. Nonetheless, they provide good motivation for stakers to ensure that their validator deployments are running well.

It's common to hear of the penalties for being offline being referred to as "getting slashed". This is incorrect. Being [slashed](/part2/incentives/slashing/) is a severe punishment for very specific misbehaviours, and results in the validator being ejected from the protocol in addition to some or all of its stake being removed.

Penalties are subtracted from validators' balances on the beacon chain and effectively burned, so they reduce the net issuance of the beacon chain.

#### Attestation penalties

Attestations are penalised for being missing, late, or incorrect. We'll lump these together as "missed" for conciseness.

Attesters are penalised for missed Casper FFG votes, that is, missed source or target votes. But there is no penalty for a missed head vote. If a source vote is incorrect, then the target vote is missed; if the source or target vote is incorrect then the head vote is missed.

Let's update our [rewards' matrix](/part2/incentives/rewards/#rewards-table) to give the full picture of penalties and rewards for attestations. Recall that this shows the weights; we need to multiply by $\frac{nb}{W_{\Sigma}}$ to get the actual reward.

<a id="penalties-rewards-table"></a>

| Timeliness | 1 slot | <= 5 slots | <= 32 slots | > 32 Slots (missing) |
| - | - | - | - | - |
| Wrong source                    | $-W_s-W_t$    | $-W_s-W_t$ | $-W_s-W_t$ | $-W_s-W_t$ |
| Correct source only             | $W_s-W_t$     | $W_s-W_t$  | $-W_s-W_t$ | $-W_s-W_t$ |
| Correct source and target only  | $W_s+W_t$     | $W_s+W_t$  | $-W_s+W_t$ | $-W_s-W_t$ |
| Correct source, target and head | $W_s+W_t+W_h$ | $W_s+W_t$  | $-W_s+W_t$ | $-W_s-W_t$ |

For more intuition, we can put in the numbers, $W_s = 14$, $W_t = 26$, $W_h = 14$, and normalise with $W_{\Sigma} = 64$:

| Timeliness | 1 slot | <= 5 slots | <= 32 slots | > 32 Slots (missing) |
| - | - | - | - | - |
| Wrong source                    | $-0.625$ | $-0.625$ | $-0.625$ | $-0.625$ |
| Correct source only             | $-0.188$ | $-0.188$ | $-0.625$ | $-0.625$ |
| Correct source and target only  | $+0.625$ | $+0.625$ | $+0.188$ | $-0.625$ |
| Correct source, target and head | $+0.844$ | $+0.625$ | $+0.188$ | $-0.625$ |

##### Break-even uptime

Stakers sometimes worry that downtime will be very expensive. To examine this, we can estimate the break-even uptime. We'll ignore sync committee participation since that is so rare, so only attestations are relevant for the calculation.

We'll assume that, when online, the validator's performance is perfect, and that the rest of the validators are performing well (both of which are pretty good approximations to the beacon chain's actual performance over its first year).

If $p$ is the proportion of time the validator is online, then its net income is, $0.844p - 0.625(1-p) = 1.469p - 0.625$. This is positive for $p > 42.5\%$. So, if your validator is online more than 42.5% of the time, you will be earning a positive return.

A useful rule of thumb is that it takes about a day of uptime to recover from a day of downtime.

#### Sync committee penalties

The small group of validators currently on sync committee duty receive a [reward](/part2/incentives/rewards/#sync-committee-rewards) in each slot that they sign off on the correct head block (correct from the proposer's point of view).

Validators that don't participate (sign the wrong head block or don't show up at all) receive a penalty exactly equal to the reward they would have earned for being correct. And the block proposer receives nothing for the missing contribution.

Historical note: Since sync committee participation is rare for any given validator, and since rewards are significant, there were [concerns](https://github.com/ethereum/consensus-specs/issues/2448) with earlier designs that the resulting [variance in rewards](/part2/incentives/rewards/#individual-validator-rewards-vary) for validators would be quite unfair. Small stakers might prefer to join staking pools rather than solo stake in order to smooth out the variance, similarly to how proof of work mining pools have sprung up.

One [suggested approach](https://github.com/ethereum/consensus-specs/pull/2450) to reducing the variance was not to reward sync committee participation at all, but rather to raise overall reward levels for everyone and to penalise the sync committee validators if they did not participate. Ultimately the [approach adopted](https://github.com/ethereum/consensus-specs/pull/2453) was to reduce the length of sync committees (meaning lower rewards, but more often), reduce the proportion of total reward for participation, and introduce a penalty for non-participation &ndash; kind of half-way to the other proposal.

The main reasons[^fn-sync-penalties] for not adopting the former proposal, although it is elegant, seem to be around the psychology of being explicitly penalised but never explicitly rewarded. The penalty for not participating in a sync committee would be substantially bigger than the attestation reward over an epoch. In addition, participation is not entirely in the validator's own hands: it depends on the next block proposer being on the right fork. There were also concerns about changing the [clean relationship](/part2/incentives/rewards/#remarks-on-proposer-rewards) between proposer rewards and the value of the duties they include in blocks.

[^fn-sync-penalties]: The quite interesting discussion remains on the [Ethereum R&D Discord](https://discord.com/channels/595666850260713488/595701319793377299/847063172174577744).

#### Remarks on penalties

There are no explicit penalties related to block proposers.

In particular, there is no explicit penalty for failing to include deposits from the Eth1 chain, nor any direct incentive for including them. However, if a block proposer does not include deposits that the rest of the network knows about, then its block is invalid. This provides a powerful incentive to include outstanding deposits.

Also note that penalties are not scaled with participation as [rewards are](/part2/incentives/rewards/#rewards-scale-with-participation).

#### See also

The detailed penalty calculations are defined in the spec in these functions:

  - Penalties for missed attestations are calculated in [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas) as part of [epoch processing](/part3/transition/epoch/).
  - Penalties for missed sync committee participation are calculated in [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) as part of [block processing](/part3/transition/block/#block-processing).

### Inactivity leak <!-- /part2/incentives/inactivity/ -->

<div class="summary">

  - When the beacon chain is not finalising it enters a special "inactivity leak" mode.
  - Attesters receive no rewards. Non-participating validators receive increasingly large penalties based on their track records.
  - This is designed to restore finality in the event of the permanent failure of large numbers of validators.

</div>

#### Introduction

If the beacon chain hasn't finalised a checkpoint for longer than [`MIN_EPOCHS_TO_INACTIVITY_PENALTY`](/part3/config/preset/#min_epochs_to_inactivity_penalty) (4) epochs, then it enters "inactivity leak" mode[^fn-inactivity-leak-mainnet].

[^fn-inactivity-leak-mainnet]: The Ethereum mainnet had nine consecutive epochs of delayed finality from epoch 200,750 to 200,758 on the 12th of May, 2023. This was the first sufficiently long period of delayed finality on mainnet to trigger the inactivity leak.

The inactivity leak is a kind of emergency state in which rewards and penalties are modified as follows.

  - Attesters receive no attestation rewards while attestation penalties are unchanged.
  - Any validators deemed inactive have their inactivity scores raised, leading to an additional inactivity penalty that potentially grows quadratically with time. This is the inactivity leak, sometimes known as the quadratic leak.
  - Proposer and sync committee rewards are unchanged.

The idea for the inactivity leak was proposed in the original [Casper FFG paper](https://arxiv.org/abs/1710.09437). The problem it addresses is that of how to recover finality (liveness, in some sense) in the event that over one-third of validators goes offline. Finality requires a majority vote from validators representing 2/3 of the total stake.

The mechanism works as follows. When loss of finality is detected the inactivity leak gradually reduces the stakes of validators who are not making attestations until, eventually, the participating validators control 2/3 of the remaining stake. They can then begin to finalise checkpoints once again.

This inactivity penalty mechanism is designed to protect the chain long-term in the face of catastrophic events (sometimes referred to as the ability to survive World War III). The result might be that the beacon chain could permanently split into two independent chains either side of a network partition, and this is assumed to be a reasonable outcome for any problem that can't be fixed in a few weeks. In this sense, the beacon chain formally prioritises availability over consistency. (You [can't have both](https://en.wikipedia.org/wiki/CAP_theorem).)

In any case, it provides a powerful incentive for stakers to fix any issues they have and to get back online.

The reason why no validators receive attestation rewards during an inactivity leak is once again due to the possibility of [discouragement attacks](/part2/incentives/rewards/#discouragement-attacks). An attacker might deliberately drive the beacon chain into an inactivity leak, perhaps by a combination of censorship and denial of service attack on other validators. This would cause the non-participants to suffer the leak, while the attacker continues to attest normally. We need to increase the cost to the attacker in this scenario, which we do by not rewarding attestations at all during an inactivity leak.

As with penalties, the amounts subtracted from validators' beacon chain accounts due to the inactivity leak are effectively burned, reducing the overall net issuance of the beacon chain.

#### Mathematics

Let's study the effect of the leak on a single validator's balance, assuming that during the period of the inactivity leak (non-finalisation) the validator is completely offline.

At each epoch, the offline validator will be penalised an amount proportional to $tB / \alpha$, where $t$ is the number of epochs since the chain last finalised, $B$ is the validator's effective balance, and $\alpha$ is the prevailing [inactivity penalty quotient](/part3/config/preset/#rewards-and-penalties) (currently `INACTIVITY_PENALTY_QUOTIENT_BELLATRIX`).

The effective balance $B$ will remain constant for a while, by design, during which time the total amount of the penalty after $t$ epochs would be $t(t+1)B / 2\alpha$: the famous "quadratic leak". If $B$ were continuously variable, the penalty would satisfy $\frac{dB}{dt}=-\frac{tB}{\alpha}$, which can be solved to give the exponential $B(t)=B_0e^{-t^2/2\alpha}$. The actual behaviour is somewhere between these two (piecewise quadratic) since the effective balance is neither constant nor continuously variable but decreases in a step-wise fashion.

In the continuous approximation, the inactivity penalty quotient, $\alpha$, is the square of the time it takes to reduce the balance of a non-participating validator to $1 / \sqrt{e}$, or around 60.7% of its initial value. With the value of `INACTIVITY_PENALTY_QUOTIENT_BELLATRIX` at $2^{24}$, this equates to 4096 epochs, or 18.2 days.

For Phase&nbsp;0 of the beacon chain, the value of `INACTIVITY_PENALTY_QUOTIENT` [was increased](https://github.com/ethereum/consensus-specs/commit/157f7e8ef4be3675543980e68581eb4b73284763) by a factor of four from $2^{24}$ to $2^{26}$, so that validators would be penalised less severely if there were non-finalisation due to implementation problems in the early days. As it happens, there were no instances of non-finalisation during the whole eleven months of Phase&nbsp;0 of the beacon chain.

The value was decreased by one quarter in the Altair upgrade from $2^{26}$ (`INACTIVITY_PENALTY_QUOTIENT`) to $3 \cdot 2^{24}$ (`INACTIVITY_PENALTY_QUOTIENT_ALTAIR`), and to its final value of $2^{24}$ (`INACTIVITY_PENALTY_QUOTIENT_BELLATRIX`) in the [Bellatrix upgrade](/part4/history/bellatrix/). Decreasing the inactivity penalty quotient speeds up recovery of finalisation in the event of an inactivity leak.

#### Inactivity scores

During Phase&nbsp;0, the inactivity penalty was an increasing global amount applied to all validators that did not participate in an epoch, regardless of their individual track records of participation. So a validator that was able to participate for a significant fraction of the time could still be quite severely penalised due to the growth of the inactivity penalty. Vitalik gives a simplified [example](https://github.com/ethereum/consensus-specs/issues/2125#issue-737768917): "if fully [off]line validators get leaked and lose 40% of their balance, someone who has been trying hard to stay online and succeeds at 90% of their duties would still lose 4% of their balance. Arguably this is unfair." We found during the [Medalla testnet incident](https://hackmd.io/@benjaminion/wnie2_200822#Medalla-Meltdown-redux) that keeping a validator online when all around you is chaos is not easy. We don't want to punish stakers who are honestly doing their best.

To improve this, the Altair upgrade introduced individual validator inactivity scores that are stored in the state. Validators' scores are updated each epoch as follows.

  - At the end of epoch $N$, irrespective of the inactivity leak,
    - decrease a validator's score by one when it made a correct and timely target vote in epoch $N-1$, and
    - increase the validator's score by `INACTIVITY_SCORE_BIAS` (four) otherwise.
  - When _not_ in an inactivity leak,
    - decrease every validator's score by `INACTIVITY_SCORE_RECOVERY_RATE` (sixteen).

Graphically, the flow-chart looks like this.

<a id="img_incentives_inactivity_scores_flow"></a>
<figure class="diagram">

![Flowchart showing how inactivity score updates are calculated.](images/diagrams/incentives-inactivity_scores_flow.svg)

<figcaption>

How each validator's inactivity score is updated. The happy flow is right through the middle. "Active", when updating the scores at the end of epoch $N$, means having made a correct and timely target vote in epoch $N-1$.

</figcaption>
</figure>

Note that there is a floor of zero on the score.

When not in an inactivity leak validators' inactivity scores are reduced by `INACTIVITY_SCORE_RECOVERY_RATE` ` + ` `1` per epoch when they make a timely target vote, and by `INACTIVITY_SCORE_RECOVERY_RATE` ` - ` `INACTIVITY_SCORE_BIAS` when they don't. So, even for non-performing validators, scores decrease outside a leak.

When in a leak, if $p$ is the participation rate between $0$ and $1$, and $\lambda$ is `INACTIVITY_SCORE_BIAS`, then the expected score after $N$ epochs is $\max (0, N((1-p)\lambda - p))$. For $\lambda = 4$ this is $\max (0, N(4 - 5p))$. So a validator that is participating 80% of the time or more can maintain a score that is bounded near zero. With less than 80% average participation, its score will increase unboundedly.

This is nice because, if many validators are able to participate intermittently, it indicates that whatever event has befallen the chain is potentially recoverable, unlike a permanent network partition, or a super-majority network fork, for example. The inactivity leak is intended to bring finality to irrecoverable situations, so prolonging the time to finality if it's recoverable is likely a good thing.

The following graph illustrates some scenarios. We have an inactivity leak that starts at zero, and ends after 100 epochs, after which finality is recovered and we are no longer in the leak. There are five validators. Working up from the lowest line, they are:

1. Always online: correctly registering a timely target vote in every epoch. The inactivity score remains at zero.
2. 90% online: the inactivity score remains bounded near zero. From the analysis above, it is expected that anything better than 80% online will bound the score near zero.
3. 70% online: the inactivity score grows slowly over time.
4. Generally online, but offline between epochs 50 and 75: the inactivity score is zero during the initial online period; grows linearly and fairly rapidly while offline during the leak; declines slowly when back online during the leak; and declines rapidly once the leak is over.
5. Always offline: the inactivity score increases rapidly during the leak, and declines even more rapidly once the leak is over.

<a id="img_inactivity_scores"></a>
<figure class="chart">

![A graph illustrating inactivity score scenarios.](images/charts/inactivity_scores.svg)

<figcaption>

The inactivity scores of five different validator personas in an inactivity leak that starts at zero and ends at epoch 100 (labelled "End" and shown with a dashed line). The dotted lines labelled "A" and "B" mark the start and end of the offline period for the fourth validator.

</figcaption>
</figure>

#### Inactivity penalties

The inactivity penalty is applied to all validators at every epoch based on their individual inactivity scores, irrespective of whether a leak is in progress or not. When there is no leak, the scores return to zero (rapidly for active validators, less rapidly for inactive ones), so most of the time this is a no-op.

The penalty for validator $i$ is calculated as

$$
\begin{split}
s_{i}B_{i} / (\tt{INACTIVITY\_SCORE\_BIAS} \times \tt{INACTIVITY\_PENALTY\_QUOTIENT\_BELLATRIX}) \\
= \frac{s_{i}B_{i}}{4 \times 16{,}777{,}216}
\end{split}
$$

where $s_i$ is the validator's inactivity score, and $B_i$ is the validator's effective balance.

This penalty is applied at each epoch, so (for constant $B_i$) the total penalty is proportional to the area under the curve of the inactivity score, above. With the same five validator persona's we can quantify the penalties in the following graph.

1. Always online: no penalty due to the leak.
2. 90% online: negligible penalty due to the leak.
3. 70% online: the total penalty grows quadratically but slowly during the leak, and rapidly stops after the leak ends.
4. Generally online, but offline between epochs 50 and 75: a growing penalty during the leak, that rapidly stops when the leak ends.
5. Always offline: we can clearly see the quadratic nature of the penalty in the initial parabolic shape of the curve. After the end of the leak it takes around 35 epochs for the penalties to return to zero.

<a id="img_inactivity_balances"></a>
<figure class="chart">

![A graph showing the effect of the inactivity leak in five different scenarios.](images/charts/inactivity_balances.svg)

<figcaption>

The balance retained by each of the five validator personas after the inactivity leak penalty has been applied. The scenario is identical to the chart above.

</figcaption>
</figure>

We can see that the new scoring system means that some validators will continue to be penalised due to the leak even after finalisation starts again. This is [intentional](https://github.com/ethereum/consensus-specs/issues/2098). When the leak causes the beacon chain to finalise, at that point we have just two-thirds of the stake online. If we immediately stop the leak (as we used to), then the amount of stake online would remain close to two-thirds and the chain would be vulnerable to flipping in and out of finality as small numbers of validators come and go. We saw this behaviour on some of the testnets prior to launch. Continuing the leak after finalisation serves to increase the balances of participating validators to greater than two-thirds, providing a buffer that should mitigate such behaviour.

#### Ejection

It is not necessary for non-participating validators to be ejected from the active validator set in order for the inactivity leak to be effective at regaining finality. Reducing the proportion of the total stake held by those non-participating validators is sufficient.

Nonetheless, a validator will be exited when its effective balance drops to [`EJECTION_BALANCE`](/part3/config/configuration/#ejection_balance). This is taken care of in the end of epoch [registry updates](/part3/transition/epoch/#registry-updates). Note that, due to the way that effective balance is calculated, the ejection will happen when the actual balance drops below 16.75&nbsp;ETH.

We can simulate how long it would take for a completely offline validator to be ejected due solely to the inactivity leak. It will be slightly sooner in reality due to the additional penalties for missing attestations.

For a validator starting the leak period with an actual balance of 32&nbsp;ETH, the simulation shows that it would take 4686 epochs (almost 3 weeks) for it to be ejected. We can also take this as a rough upper-bound on how long it would take the beacon chain to recover finality, however many validators went offline[^fn-ejection-queue].

[^fn-ejection-queue]: This is complicated by the need for validators to be queued for exit, and the rate-limit on processing that queue. It is not possible to instantly exit validators en masse. Exiting validators remain subject to the inactivity leak while they sit in the queue, so their effective balances could drop lower than 16&nbsp;ETH.

<details>
<summary>Ejection simulation code</summary>

```python
GWEI = 10 ** 9
EJECTION_BALANCE = 16 * GWEI
MAX_EFFECTIVE_BALANCE = 32 * GWEI
HYSTERESIS_QUOTIENT = 4
INACTIVITY_SCORE_BIAS = 4
INACTIVITY_PENALTY_QUOTIENT = 2 ** 24

# Simplified hysteresis for monotonically decreasing balance
def calc_effective_balance(balance):
    return min(MAX_EFFECTIVE_BALANCE, (balance + GWEI // HYSTERESIS_QUOTIENT) // GWEI * GWEI)

epoch = 0
score = 0
balance = 32 * GWEI
effective_balance = calc_effective_balance(balance)

while effective_balance > EJECTION_BALANCE:
    balance -= effective_balance * score // (INACTIVITY_SCORE_BIAS * INACTIVITY_PENALTY_QUOTIENT)
    effective_balance = calc_effective_balance(balance)
    score += INACTIVITY_SCORE_BIAS
    epoch += 1

print(balance / GWEI)
print(effective_balance // GWEI)
print(epoch)
```

</details>

#### See also

From the spec:

  - Inactivity scores are updated during epoch processing in [`process_inactivity_updates()`](/part3/transition/epoch/#def_process_inactivity_updates).
  - Inactivity penalties are calculated in [`def_get_inactivity_penalty_deltas()`](/part3/transition/epoch/#def_get_inactivity_penalty_deltas).

For the original description of the mechanics of the inactivity leak, see the [Casper paper](https://arxiv.org/abs/1710.09437), section 4.2.

### Slashing <!-- /part2/incentives/slashing/ -->

<div class="summary">

  - Validators are slashed for breaking very specific protocol rules that could be part of an attack on the chain.
  - Slashed validators are exited from the beacon chain and receive three types of penalty.
  - Correlated penalties mean that punishment is light for isolated incidents, but severe when many validators are slashed in a short time period.
  - Block proposers receive rewards for reporting evidence of slashable offences.

</div>

#### Introduction

Slashing occurs when validators make attestations or block proposals that break very specific protocol rules. It applies to behaviour that could potentially be part of an attack on the chain. Getting slashed means losing a significant amount of stake and being ejected from the protocol. It is more "punishment" than "penalty"[^fn-slashing-punitive]. The good news is that stakers can take simple precautions to protect against ever being slashed.

[^fn-slashing-punitive]: The concept of slashing has its roots in Vitalik's [Slasher](https://blog.ethereum.org/2014/01/15/slasher-a-punitive-proof-of-stake-algorithm) algorithm from early 2014. Our current design looks quite different, but some things remain. In particular, he says that, "we are calling [it] Slasher to express its harshly punitive nature", and we retail the name "slashing" for the same reason.

The behaviours that lead to slashing are as follows.

1. Related to Casper FFG consensus,
    - making two differing attestations for the same target checkpoint, or
    - making an attestation whose source and target votes "surround" those in another attestation from the same validator.
2. Related to LMD GHOST consensus,
    - proposing more than one distinct block at the same height, or
    - attesting to different head blocks, with the same source and target checkpoints[^fn-slash-different-heads].

[TODO: Link to Casper FFG and LMD GHOST sections when done]::

[^fn-slash-different-heads]: This condition is not very obvious in the [code](/part3/helper/predicates/#is_slashable_attestation_data). It comes about because two attestations with the same source and target votes but different head votes differ from each other. They are therefore counted as conflicting votes for the same target and slashed under the first Casper FFG rule.

All of these slashable behaviours relate to "equivocation", which is when a validator contradicts something it previously advertised to the network.[^fn-avoid-slashing]

[^fn-avoid-slashing]: To avoid being slashed, simply be sure not to equivocate. Any normally operating client (in the absence of bugs) will never do so. As far as can be determined, every Ethereum slashing to date has been due to a node operator simultaneously running the same validator keys on two different nodes, perhaps as a misguided way to improve uptime. Don't do this. (Update: [one slashing](https://beaconcha.in/slot/6142320#proposer-slashings) was due to a proposer [attacking a MEV Relay](https://collective.flashbots.net/t/post-mortem-april-3rd-2023-mev-boost-relay-incident-and-related-timing-issue/1540).)

The slashing conditions related to Casper FFG underpin Ethereum&nbsp;2.0's [economic finality](/part2/incentives/staking/#economic-finality) guarantee. They effectively impose a well-determined price on reverting finality.

The slashing conditions related to LMD GHOST are designed to combat the [nothing at stake](https://ethereum.stackexchange.com/questions/2402/what-exactly-is-the-nothing-at-stake-problem) problem, and are not directly related to economic finality. They punish bad behaviour that could lead to serious issues such as the [balancing attack](https://ethresear.ch/t/a-balancing-attack-on-gasper-the-current-candidate-for-eth2s-beacon-chain/8079?u=benjaminion). Since we already had the slashing mechanism available for use with Casper FFG, it was simple enough to extend it to LMD GHOST.

As with penalties, any amount removed from validators' beacon chain accounts due to slashing is effectively burned, reducing the overall net issuance of the beacon chain.

#### The cost of being slashed

When it comes to the punishment for being slashed it does not matter which rule was broken. All slashings are dealt with in the same way.

##### The initial penalty

Slashing is triggered by the evidence of the offence being included in a beacon chain block. Once the evidence is confirmed by the network, the offending validator (or validators) is slashed.

The offender immediately has $\frac{1}{32}$ ([`MIN_SLASHING_PENALTY_QUOTIENT_BELLATRIX`](/part3/config/preset/#min_slashing_penalty_quotient)) of its effective balance deducted from its actual balance. This is a maximum of 1&nbsp;ETH due to the cap on effective balance.

This initial penalty was [introduced](https://github.com/ethereum/consensus-specs/pull/624) to make it somewhat costly for validators to self-slash for any reason[^fn-initial-penalty].

[^fn-initial-penalty]: It is not clear to me under what circumstances self-slashing would give any advantage under the beacon chain's current design. To date, the only effect of the initial penalty has been to punish small stakers for misconfiguring their staking setups (by running keys in more than one place) which seems to me unduly harsh. I have argued that it ought to be removed entirely. Nonetheless, it remains.

Along with the initial penalty, the validator is queued for exit, and has its withdrawability epoch set to around 36 days ([`EPOCHS_PER_SLASHINGS_VECTOR`](/part3/config/preset/#epochs_per_slashings_vector), which is 8192 epochs) in the future.

During Phase&nbsp;0 the initial penalty was $\frac{1}{128}$ of the offender's effective balance, and during Altair, $\frac{1}{64}$. It was raised to its full value of $\frac{1}{32}$ of the slashed validator's effective balance, a maximum of 1&nbsp;ETH, in the pre-Merge [Bellatrix upgrade](/part4/history/bellatrix/).

##### The correlation penalty

At the halfway point of its withdrawability period (18 days after being slashed) the slashed validator is due to receive a second penalty.

This second penalty is based on the total amount of stake slashed during the 18 days before and after our validator was slashed. The idea is to scale the punishment so that a one-off event posing little threat to the chain is only lightly punished, while a mass slashing event that might be the result of an attempt to finalise conflicting blocks is punished to the maximum extent possible.

To be able to calculate this, the beacon chain maintains a record of the effective balances of all validators that were slashed during the most recent 8192 epochs (about 36 days).

The correlated penalty is calculated as follows.

 1. Compute the sum of the effective balances (as they were when the validators were slashed) of all validators that were slashed in the previous 36 days. That is, for the 18 days preceding and the 18 days following our validator's slashing.
 2. Multiply this sum by [`PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX`](/part3/config/preset/#proportional_slashing_multiplier), but cap the result at `total_balance`, the total active balance of all validators.
 3. Multiply the slashed validator's effective balance by the result of #2 and then divide by the `total_balance`. This results in an amount between zero and the full effective balance of the slashed validator. That amount is subtracted from its actual balance as the penalty. Note that the effective balance could exceed the actual balance in odd corner cases, but [`decrease_balance()`](/part3/helper/mutators/#def_decrease_balance) ensures the balance does not go negative.

The slashing multiplier since Bellatrix is set to 3. With $S$ being the sum of increments in the list of slashed validators over the last 36 days, $B$ my effective balance, and $T$ the total increments, the calculation looks as follows.

$$
\text{Correlation penalty} = \min(B, \frac{3SB}{T})
$$

Interestingly, [due to](https://github.com/ethereum/consensus-specs/issues/1322) the way the integer arithmetic is constructed in [the implementation](/part3/transition/epoch/#def_process_slashings) the result of this calculation will be zero if $3SB < T$. Effectively, the penalty is rounded down to the nearest whole amount of Ether. As a consequence, when there are few slashings there is no extra correlated slashing penalty at all, which is probably a good thing.

The proportional slashing multiplier was increased gradually through the early deployment of the beacon chain. At Genesis, it was set to one (`PROPORTIONAL_SLASHING_MULTIPLIER`), at Altair it was increased to two (`PROPORTIONAL_SLASHING_MULTIPLIER_ALTAIR`), and at Bellatrix set to its final value of three (`PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX`). This was intended to punish slashed validators less harshly while we were becoming accustomed to running the beacon chain. As it happened, no correlated slashings occurred that incurred a penalty greater than zero under this mechanism.

##### Other penalties

Validators that exit normally (by sending a voluntary exit message) are expected to participate only until their exit epoch, which is normally only a couple of epochs later.

A validator that is slashed continues to receive attestation penalties until its withdrawable epoch, which is set to 8192 epochs (36 days) after the slashing, and they are unable to receive any attestation rewards during this time. They are also subject for this entire period to any [inactivity leak](/part2/incentives/inactivity/) that might be in operation. Whatever the slashed validator does, it is penalised exactly as if it is failing to participate.[^fn-slashed-validators]

[^fn-slashed-validators]: Having such a long overhang from being slashed during which validators continue to receive penalties seems like "kicking a man when he's down", especially since slashed validators are locked in for twice as long as needed to calculate the correlation penalty. Vitalik [says](https://notes.ethereum.org/@vbuterin/Sys3GLJbD#Aside-note-on-a-validators-life-cycle) that this measure "is included to prevent self-slashing from being a way to escape inactivity leaks." But validators don't need to self-slash to avoid this; they could just make a normal voluntary exit.

<!-- Number of validators -->

So, in addition to the initial slashing penalty and the correlation penalty, there is a further penalty of up to $8192\frac{14 + 26}{64}32b = 82{,}739{,}200 \text{ Gwei} = 0.0827 \text{ ETH}$, based on 500k validators, where $b$ is the [base reward per increment](/part2/incentives/issuance/#the-base-reward-per-increment). This assumes that the chain is not in an inactivity leak; the penalties will be much higher if it is.

Slashed validators are eligible to be selected to propose blocks until they reach their exit epoch, but those blocks will be considered invalid, so there is no proposer reward available to them. This is in preference to immediately recomputing the duties assignments which would break the lookahead guarantees they have. (The proposer selection algorithm could easily be modified to [skip slashed validators](https://github.com/ethereum/consensus-specs/pull/3175), but that is not how it is implemented currently.)

In an interesting edge case, however, slashed validators are eligible to be selected for sync committee duty until they reach their exit epoch and to receive the rewards for sync committee participation. The odds of this happening, though, in the absence of a mass slashing event, are pretty tiny.

#### The value of reporting a slashing

In order for the beacon chain to verify slashings and take action against the offender, the evidence needs to be included in a beacon block. To incentivise validators to make the effort there is a specific reward for the proposer of a block that includes slashings.

##### The proposer reward

At the point of the initial slashing report being included in a block, the proposer of the block receives a reward of `validator.effective_balance` / [`WHISTLEBLOWER_REWARD_QUOTIENT`](/part3/config/preset/#whistleblower_reward_quotient), which is $B / 512$ if $B$ is the effective balance of the validator being slashed.

A report of a proposer slashing violation can slash only one validator, but a report of an attestation slashing violation can simultaneously slash up to an entire committee, which might be hundreds of validators. This could be very lucrative for the proposer including the reports. A single block can contain up to 16 proposer slashing reports and up to 2 attester slashing reports.

Note that no new issuance is required to pay for this reward. The proposer reward is much less than the initial slashing applied to the validator, so the net issuance due to a slashing event is always negative.

##### The whistleblower reward

In the [code](/part3/helper/mutators/#def_slash_validator) implementing the reward for reporting slashing evidence there is provision for a "whistleblower reward", with the whistleblower receiving $\frac{7}{8}$ of the above reward and the proposer $\frac{1}{8}$.

The idea is to incentivise nodes that search for and discover evidence of slashable behaviour, which can be an intensive process.

However, this functionality is not currently used on the beacon chain, and the proposer receives both the whistleblower reward and the proposer reward, as above. The challenge is that it is too easy for a proposer just to steal a slashing report, so there's no point incentivising them separately. It's not an ideal situation, but so far there seem to be sufficient altruistic slashing detectors running on the beacon chain for slashings to be reported swiftly. There only needs to be one in practice.

This functionality may become useful in future upgrades.

#### See also

From the spec:

  - The initial slashing penalty and proposer reward are applied in [`slash_validator()`](/part3/helper/mutators/#def_slash_validator) during block processing.
  - The correlation slashing penalty is applied in [`process_slashings()`](/part3/transition/epoch/#def_process_slashings) during epoch processing.

In the Serenity Design Rationale Vitalik gives some further background on why Ethereum&nbsp;2.0 [includes proposer slashing](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Slashing). It is specifically intended to discourage stakers from simultaneously running primary and backup nodes.

### Diversity <!-- /part2/incentives/diversity/ -->

<div class="summary">

  - Beacon chain incentives strongly encourage diversity among client deployments, hosting infrastructure, and staking pools.
  - Lack of diversity puts at risk both the chain in general and all those running the majority client.
  - The greater the share of validators hosted by a single client implementation the greater the risk.
  - The beacon chain is at its most robust and fault-tolerant when no single client type manages more than one-third (33%) of validators.

</div>

#### Diversity makes us all stronger

Just as diversity in biological ecosystems makes them more resilient, and monocultures make them very fragile &ndash; yes, I've been watching David Attenborough &ndash;, so it is with Ethereum staking.

It is not unintentional that both the [inactivity leak](/part2/incentives/inactivity/) and the slashing [correlation penalty](/part2/incentives/slashing/#the-correlation-penalty) provide a strong encouragement to diversify the network as much as possible.

For example, the inactivity leak is much more likely to occur on a network in which a single client implementation runs over 33% of validators, or a single staking operator controls over 33% of validators, or over 33% of validators are deployed to the same hosting infrastructure. All these scenarios constitute single points of failure that could prevent the beacon chain from finalising and lead to a leak that penalises those running the majority (offline) client most harshly.

#### Scenarios

Let's consider some scenarios. For the sake of this exercise you are running the beacon chain client X. In each scenario you and others using client X host validators managing a certain fraction of the total stake. We will consider what happens if client X has a bug that takes it down. It might be a consensus bug or another kind of bug that takes the client off the network: we saw examples of both of these on the pre-launch testnets.

##### 1. Client X has less than one-third of the stake

When a client managing less than one-third of the total stake goes down, the consequences are minimal. The beacon chain can continue to finalise as normal. Users of client X will suffer only the normal offline penalties until the bug is fixed, though rewards will be lower across the board for the other validators. But this is not catastrophic and there is time to recover without a panic, either by fixing the bug or swapping to a different client.

_The beacon chain is at its most robust and fault-tolerant when no single client type manages more than one-third (33%) of validators._

##### 2. Client X has more than one-third of the stake

If client X goes down while managing more than one-third of the total stake, then the beacon chain will be unable to finalise and will enter the [inactivity leak](/part2/incentives/inactivity/).

In this situation no validators will receive rewards for attesting. Users of non-X clients will not lose stake, but users of client X will suffer much bigger losses than usual, due to the quadratically increasing inactivity leak. There is strong time pressure to get the issue with client X resolved either by fixing the bug or swapping to a different client.

##### 3. Client X has around half of the stake

The situation becomes potentially much worse when X hosts around half of the validators. If X were to have a consensus bug, but otherwise keep running, the beacon chain would split into two similarly sized chains. Each chain would see half its validators missing and start leaking out the stakes of those validators. Within three to four weeks each chain would have leaked out enough of the stake of the missing validators that the present validators would control two-thirds of the remaining stake, meaning that the chains could each finalise separately. It would be extremely difficult &ndash; effectively impossible &ndash; to reunite these chains ever again since they would contain conflicting finalised checkpoints. The beacon chain would be permanently partitioned.

Hopefully, 3-4 weeks is sufficient time for client X to fix its bug or for users of X to migrate to other clients. Meanwhile, users of X are suffering large inactivity penalties on the correct chain as per scenario 2.

##### 4. Client X approaches or exceeds two-thirds of the stake

A scenario in which a single client approaches[^fn-approaches-67] hosting two-thirds (66%) of the validators is potentially catastrophic. A consensus bug in that client would very quickly &ndash; possibly within 13 minutes &ndash; finalise a broken version of the chain with no chance to intervene.

That would leave the Ethereum community with a horrible dilemma.

One possible response would be to modify the other clients (and the specification) to reproduce the bug and allow them to join X's chain. The feasibility of this depends on the nature of the consensus bug. For a trivial bug it might be possible, but it would be very unfair to the non-X clients since they would suffer penalties despite having acted perfectly correctly. In any case, many types of consensus bug would make this infeasible: one way or another X's chain is broken and now incompatible with the entirety of the rest of the ecosystem.

The correct &ndash; but nuclear &ndash; option is to fix the bug in client X. Unfortunately, however, there would be no way for the stakers on the incorrect X chain to rejoin the correct chain. Any that tried to do so would be slashed, having previously finalised a checkpoint on the incorrect chain. The only reasonable strategy for (former) users of client X would be to stop validating and voluntarily exit their stakes. Exiting could take a long time due to the queuing mechanism, resulting in large penalties from the inactivity leak. Many of the affected stakers are likely to try to start validating again and would surely be slashed.

There are no good outcomes here, which is why it is critical that we never have a client with a two-thirds or more supermajority.[^fn-client-diversity-220112]

[^fn-approaches-67]: If the share is less than 67% the incorrect chain won't finalise immediately, but very soon the inactivity leak will raise the proportion above 67% on that chain, and it will then finalise.

[^fn-client-diversity-220112]: As of 2022-01-12, the Prysm client [appeared to have](https://web.archive.org/web/20230630135447/https://nitter.it/sproulM_/status/1481109509544513539) 68.1% of the validators.

#### Slashing

As for slashing, once again running a majority client could be an act of self-harm. In the unlikely event that a client implementation has a bug that leads to its validators becoming slashed en-masse, the [correlated slashing penalties](/part2/incentives/slashing/#the-correlation-penalty) would be much more severe than if the same thing happened to those running a minority client.

#### Another view

Danny Ryan has presented a slightly [different angle](https://blog.ethereum.org/2022/01/31/finalized-no-33/) on client diversity that's insightful:

> If a single client:
>
>   - Does not exceed 66.6%, a fault/bug in a single client cannot be finalized.
>   - Does not exceed 50%, a fault/bug in a single client's fork choice cannot dominate the head of the chain.
>   - Does not exceed 33.3%, a fault/bug in a single client cannot disrupt finality.

#### Epilogue

Let me emphasise that _these scenarios are far from theoretical_. It is of existential importance to the Ethereum network that stakers pay attention to the distribution of client software and avoid adding to the share of the majority client.

It is instructive to revisit the [major incident](https://hackmd.io/@benjaminion/wnie2_200822#Medalla-Meltdown-redux) that occurred on the Medalla testnet, in which an issue in the majority client caused a high degree of chaos and led to large numbers of slashings. Had that client managed a smaller proportion of the network, the consequences for everybody would have been much less severe.

#### See also

  - [Run the majority client at your own peril!](https://dankradfeist.de/ethereum/2022/03/24/run-the-majority-client-at-your-own-peril.html) by Dankrad Feist.
  - [What Happens If Beacon Chain Consensus Fails?](https://www.symphonious.net/2021/09/23/what-happens-if-beacon-chain-consensus-fails/) by Adrian Sutton.

## 基础构件 <!-- /part2/building_blocks/ -->

### 引言

在本章中，我们将探讨使以太坊 2 的协议切实可行的一些基本创新，它们是构建更高层协议的基础构件。

这些基础构件并非全新——它们都在某种程度上依赖于现有技术——但每一个基础构件在 Eth2 中的应用都有一些新颖之处。在这些进展背后，以太坊基金会研发团队的研究和洞察功不可没。

在阅读过程中，请注意这些设计选择背后的权衡取舍。迈向深入理解的大门总是在权衡之中。

有些权衡相当有趣。例如，混洗（[shuffling](/part2/building_blocks/shuffling/)）算法和状态根（[state root](/part2/building_blocks/merkleization/)）计算算法都不是我们本可以选择的最高效的算法，至少就纯粹的速度而言是如此。在这两种情况下，我们更倾向于选择能够支持轻客户端生态系统的算法，而不是对全节点来说可能更高效的算法。

我在本章中归纳的基础构件都是协议规范本身的一部分。客户端实现通常采用不属于协议规范的其它优化。我们将在“实现（[Implementation](/part2/implementation/)）”这一章中考虑其中一些优化。

以下是我挑出来的需要被特别关注的主题。

  - BLS 签名（[BLS Signatures](/part2/building_blocks/signatures/)）促成对以太坊权益证明协议全面的重新设计，并支撑以太坊 2 的规模和雄心。
  - 随机性（[Randomness](/part2/building_blocks/randomness/)）是安全性的一个重要方面，但很难在确定性系统中生成随机性。信标链通过 BLS 签名实现了这一点。
  - 混洗（[Shuffling](/part2/building_blocks/shuffling/)）通过随机性来填充委员会。但是，为了轻客户端，我们使用了“不经意（oblivious）”混洗，而不是标准的 Fisher–Yates 混洗。
  - 委员会（[Committees](/part2/building_blocks/committees/)）分担了信标链的工作量。
  - 聚合器选择（[Aggregator Selection](/part2/building_blocks/aggregator/)）会秘密选择委员会的小部分成员来完成聚合认证的工作。
  - SSZ: 简单序列化（[SSZ: Simple Serialize](/part2/building_blocks/ssz/)）是一种新颖的序列化技术，在协议中随处可见。它优雅高效。
  - 哈希树根和默克尔化（[Hash Tree Roots and Merkleization](/part2/building_blocks/merkleization/)）是 SSZ 的应用。除了其他的作用，它们还使轻客户端成为可能。
  - 广义索引和默克尔证明（待写）。
  - 同步委员会（待写）。

### BLS 签名 <!-- /part2/building_blocks/signatures/ -->

<div class="summary">

  - 权益证明协议使用数字签名来识别其参与者并使其承担责任。
  - BLS 签名可以聚合在一起，它们在大规模验证时更加高效。
  - 签名聚合允许信标链扩展到数十万名验证者。
  - 以太坊执行层（Eth1）上的交易签名保持原样。

</div>

#### 数字签名

[数字签名](https://en.wikipedia.org/wiki/Digital_signature) 在区块链技术中被广泛使用。数字签名应用于消息，以确保两点：(1) 消息没有被篡改；(2) 发送消息的人与其声称的身份一致。数字签名并不新颖，早在 1980 年代，由于[非对称加密](https://en.wikipedia.org/wiki/Public-key_cryptography)的发明，它们得到了广泛发展。然而，椭圆曲线和基于配对的密码学（pairing-based cryptography）的最新发展，对以太坊 2 的设计产生了重大影响。

每次发送以太坊交易时，你都在使用数字签名；所有以太坊用户都熟悉签名流程。但这只是交易层面的。以太坊 1 的共识协议层面完全不使用数字签名——在工作量证明下，一个区块只需要有一个正确的 `mixHash` 以证明它被正确挖出，而没人关心实际是谁挖出了这个区块，因此不需要签名。

然而，在以太坊 2 中，验证者有身份并且需要对其行为负责。为了执行 Casper FFG 规则，并能够计算 LMD GHOST 分叉选择的投票，我们需要能够准确识别进行单个证明和创建单个区块的个体验证者。

#### 数字签名的用处

数字签名的主要功能是将消息发送者与消息内容不可撤销地链接在一起。例如，这可被用来坐实一个验证者发布了相互矛盾的投票，因此会受到罚没惩罚。

在协议外，将消息与验证者绑定的能力也很有用。例如，在广播层，节点在转发消息前验证签名，以此作为防止垃圾消息的机制。

除了辨识消息发送者的常规功能外，数字签名在以太坊 2 协议中还有一些相当新颖的用途。它们用于向 [RANDAO](/part2/building_blocks/randomness/) 提供随机性，也用于[选择委员会的子集](/part2/building_blocks/aggregator/)进行聚合任务。我们将在各自的章节讨论这些用法，本章节将重点放在协议消息的签名上。

#### 背景

<!-- Number of validators -->

权益证明协议的一个特点是需要处理大量的协议消息。对于 50 万名活跃的验证者，目前的信标链设计要求每秒钟广播超过 1300 个认证消息。这是一个持续的平均值，实际操作中会有更高的峰值。这些消息不仅需要在网络上传输，每个节点还需要验证每个数字签名，这是一个大量消耗 CPU 的操作。不仅如此，还需要在区块历史中存储所有这些签名消息。这些挑战性要求通常限制了权益证明或权威证明网络中的验证者数量。纯粹基于 PBFT 的共识协议的验证者数量通常是几十而不是数千个。

2018 年初，以太坊（部分）转向权益证明的主流在研设计 [EIP-1011](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-1011.md) 估计，由于这种消息开销，该协议最多只能处理约 900 个验证者，并相应地将每个验证者的质押规模设定为巨额的 1500 个以太币。

转折点出现在 2018 年 5 月，Justin Drake 在 Ethresear.ch 论坛上发布了一篇文章《BLS 的实用签名聚合》（[Pragmatic signature aggregation with BLS](https://ethresear.ch/t/pragmatic-signature-aggregation-with-bls/2105?u=benjaminion)）。这篇文章提出使用一种新的签名方案，该方案能够在保持每个签名验证者的个人责任的同时，聚合多个数字签名。聚合提供了一种显著减少网络上广播的个体消息数量及减少验证这些消息的诚实性的成本的方法。因此，它使我们能够扩展到数十万名共识参与者。[^fn-dfinity-credit]

[^fn-dfinity-credit]: To give credit where it is due, the Dfinity blockchain researchers had published [a white paper](https://dfinity.org/pdf-viewer/pdfs/viewer?file=../library/dfinity-consensus.pdf) a few months earlier proposing the use of BLS signatures in a threshold scheme. However, their use of threshold signatures makes the chain vulnerable to liveness failures, and also requires a tricky distributed key generation protocol. Ethereum's aggregation-based approach has neither of these issues. Nonetheless, the name "beacon chain" that we still use today derives from Dfinity's "randomness beacon" described in that paper.

这种签名聚合能力是促使我们完全放弃 EIP-1011 链上 PoS 管理机制并转向我们今天的“信标链”模型的主要突破 [^fn-killing-of-hybrid-casper]。

[^fn-killing-of-hybrid-casper]: The last significant update to EIP-1011 was made on the [16th of May 2018](https://github.com/ethereum/EIPs/commit/46927c516f6dda913cbabb0beb44a3f19f02c0bb). Justin Drake's post on signature aggregation was made just [two weeks later](https://ethresear.ch/t/pragmatic-signature-aggregation-with-bls/2105?u=benjaminion).

#### BLS 数字签名

区块链世界中的数字签名通常基于椭圆曲线群。以太坊用于用户交易签名的是 [secp256k1](https://en.bitcoin.it/wiki/Secp256k1) 椭圆曲线的 [ECDSA](https://en.wikipedia.org/wiki/Elliptic_Curve_Digital_Signature_Algorithm) 签名。然而，信标链协议使用的是 [BLS12-381](https://hackmd.io/@benjaminion/bls12-381) 椭圆曲线的 [BLS](https://en.wikipedia.org/wiki/BLS_digital_signature) 签名[^fn-bls-bls]。尽管用法相似，ECDSA 和 BLS 签名在数学上相当不同，后者依赖于某些椭圆曲线的特殊性质——[配对](https://medium.com/@VitalikButerin/exploring-elliptic-curve-pairings-c73c1864e627)。尽管 ECDSA 签名比 BLS 签名[快得多](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-1.1)，但正是 BLS 签名的配对属性使我们能够聚合签名，从而使整个共识协议切实可行。

[^fn-bls-bls]: There is a curious naming collision here. The BLS trio of "BLS signatures" are Boneh, Lynn, and Shacham, whereas those of the "BLS12-381" elliptic curve are Barreto, Lynn, and Scott. Ben Lynn is the only common name between the two.

其他几个区块链协议已经或将采用 BLS12-381 曲线的 BLS 签名，在 Eth2 的实现过程中，我们一直注意遵循已有标准，并在可能的情况下参与定义这些标准。这不仅有助于互操作性，还支持通用库和工具的开发。

创建和验证 BLS 签名的高层流程相对简单。在接下来的章节中，我将用一些文字、图片和数学描述其工作原理。你尽可以跳过数学部分，这非必修也没有测试。尽管它相当优雅。

##### 组件

BLS 数字签名过程包括四个数据组件：

1. 秘钥（secret key）。协议中每个实体（在 Eth2 中就是验证者）都有一个秘钥，有时称为私钥。秘钥用于签名消息，必须保密。
2. 公钥（public key）。从私钥导出独特的公钥，但无法从公钥反向工程出秘钥。验证者的公钥代表其在协议中的身份，并为所有人所知。
3. 消息（message）。我们稍后会讨论 Eth2 协议中使用的消息类型及其构建方式。目前，消息只是一串字节。
4. 签名（signature）是签署过程的输出。消息与秘钥结合创建了签名。给定一条消息、该消息的签名以及一个公钥，我们可以验证该公钥的验证者确实签署了这条消息。换句话说，没有其他人可以签署这条消息，而且自签署以来消息没有发生更改。

在数学上，事情看起来像是这样：我们使用 BLS12-381 椭圆曲线的两个子群：定义在基字段 $F_q$ 上的 $G_1$，和定义在扩展字段 $F_{q^2}$ 上的 $G_2$。这两个子群的阶都是一个 77 位的素数 $r$，$G_1$  的生成元是 $g_1$，$G_2$ 的生成元是 $g_2$。

1. 秘钥 $sk$ 是介于 1 和 $r$ 之间的一个数字（技术上该范围包括 1，但不包括 $r$，但过小的 $sk$ 值会非常不安全）。
2. 公钥 $pk$ 是 $[sk]g_1$，其中方括号表示椭圆曲线群点的标量乘法。公钥因此是 $G_1$ 群的成员。
3. 消息 $m$ 是一个字节序列。在签名过程中，这将被映射到 $G_2$ 群中的某个点 $H(m)$。
4. 签名 $σ$ 也是 $G_2$ 群的成员，即 $[sk]H(m)$。

<a id="img_bls_key"></a>
<figure class="diagram" style="width:80%">

![Diagram showing how we will depict the various components in the diagrams below.](images/diagrams/bls-key.svg)

<figcaption>

The key to the keys. This is how we will depict the various components in the diagrams below. Variants of the same object are hatched differently. The secret key is mathematically a scalar; public keys are $G_1$ group members; message roots are mapped to $G_2$ group members; and signatures are $G_2$ group members.

</figcaption>
</figure>

##### Key pairs

A key pair is a secret key along with its public key. Together these irrefutably link each validator with its actions.

Every validator on the beacon chain has at least one key pair, the "signing key" that is used in daily operations (making attestations, producing blocks, etc.). Depending on which version of [withdrawal credentials](/part3/config/constants/#withdrawal-prefixes) the validator is using, it may also have a second BLS key pair, the "withdrawal key", that is kept offline.

The secret key is supposed to be uniformly randomly generated in the range $[1,r)$. [EIP-2333](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-2333.md) defines a standard way to do this based on the [`KeyGen`](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-2.3) method of the draft IRTF BLS signatures standard. It's not compulsory to use this method &ndash; no-one will ever know if you don't &ndash; but you'd be ill-advised not to. In practice, many stakers generate their keys with the [`eth2.0-deposit-cli`](https://github.com/ethereum/eth2.0-deposit-cli) tool created by the Ethereum Foundation. Operationally, key pairs are often stored in password-protected [EIP-2335](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-2335.md) keystore files.

The secret key, $sk$ is a 32 byte unsigned integer. The public key, $pk$, is a point on the $G_1$ curve, which is represented in-protocol in its [compressed](https://hackmd.io/@benjaminion/bls12-381#Point-compression) serialised form as a string of 48 bytes.

<a id="img_bls_setup"></a>
<figure class="diagram" style="width:50%">

![Diagram of the generation of the public key.](images/diagrams/bls-setup.svg)

<figcaption>

A validator randomly generates its secret key. Its public key is then derived from that.

</figcaption>
</figure>

##### Signing

In the beacon chain protocol the only messages that get signed are [hash tree roots](/part2/building_blocks/merkleization/) of objects: their so-called signing roots, which are 32 byte strings. The [`compute_signing_root()`](/part3/helper/misc/#compute_signing_root) function always combines the hash tree root of an object with a "domain" as described [below](#domain-separation-and-forks).

Once we have the signing root it needs to be mapped onto an elliptic curve point in the $G_2$ group. If the message's signing root is $m$, then the point is $H(m)$ where $H()$ is a function that maps bytes to $G_2$. This mapping is hard to do well, and an entire [draft standard](https://datatracker.ietf.org/doc/draft-irtf-cfrg-hash-to-curve/) exists to define the process. Thankfully, we can ignore the details completely and leave them to our cryptographic libraries[^fn-implement-h2g2].

[^fn-implement-h2g2]: Unless you have to implement the thing, as I [ended up doing](https://github.com/ConsenSys/teku/commit/e927d9be89b64fe8297b74405f37aa0e6378024) in Java.

Now that we have $H(m)$, the signing process itself is simple, being just a scalar multiplication of the $G_2$ point by the secret key:

$$
\sigma = [sk]H(m)
$$

Evidently the signature $\sigma$ is also a member of the $G_2$ group, and it serialises to a 96 byte string in compressed form.

<a id="img_bls_signing"></a>
<figure class="diagram" style="width:65%">

![Diagram of signing a message.](images/diagrams/bls-signing.svg)

<figcaption>

A validator applies its secret key to a message to generate a unique digital signature.

</figcaption>
</figure>

##### Verifying

To verify a signature we need to know the public key of the validator that signed it. Every validator's public key is stored in the beacon state and can be simply looked up via the validator's index which, by design, is always available by some means whenever it's required.

Signature verification can be treated as a black-box: we send the message, the public key, and the signature to the verifier; if after some cryptographic magic the signature matches the public key and the message then we declare it valid. Otherwise, either the signature is corrupt, the incorrect secret key was used, or the message is not what was signed.

More formally, signatures are verified using elliptic curve pairings.

With respect to the curve BLS12-381, a pairing simply takes a point $P\in G_1$, and a point $Q\in G_2$ and outputs a point from a group $G_T\subset F_{q^{12}}$. That is, for a pairing $e$, $e:G_1\times G_2\rightarrow G_T$.[^fn-pairing-multiplication]

[^fn-pairing-multiplication]: If it helps, you can loosely think of a pairing as being a way to "multiply" a point in $G_1$ by a point in $G_2$. If we were to write all the groups additively then the arithmetic would work out very nicely. However, we conventionally write $G_T$ multiplicatively, so the notation isn't quite right.

Pairings are usually denoted $e(P,Q)$ and have special properties. In particular, with $P$ and $S$ in $G_1$ and $Q$ and $R$ in $G_2$,

  - $e(P, Q + R) = e(P, Q) \cdot e(P, R)$, and
  - $e(P + S, R) = e(P, R) \cdot e(S, R)$.

(Conventionally $G_1$ and $G_2$ are written as additive groups, and $G_T$ as multiplicative, so the $\cdot$ operator is point multiplication in $G_T$.)

From this, we can deduce that all the following identities hold:

$$
e([a]P,[b]Q)={e(P,[b]Q)}^a={e(P,Q)}^{ab}={e(P,[a]Q)}^b=e([b]P,[a]Q)
$$

Armed with our pairing, verifying a signature is straightforward. The signature is valid if and only if

$$
e(g_1,\sigma)=e(pk,H(m))
$$

That is, given the message $m$, the public key $pk$, the signature $\sigma$, and the fixed public value $g_1$ (the generator of the $G_1$ group), we can verify that the message was signed by the secret key $sk$.

This identity comes directly from the properties of pairings described above.

$$
e(pk,H(m)) = e([sk]g_1,H(m)) = {e(g_1,H(m))}^{(sk)} = e(g_1,[sk]H(m)) = e(g_1,\sigma)
$$

Note that elliptic curves supporting such a pairing function are very rare. Such curves can be constructed, as [BLS12-381 was](https://hackmd.io/@benjaminion/bls12-381#History), but general elliptic curves such as the more commonly used secp256k1 curve do not support pairings and cannot be used for BLS signatures.

<a id="img_bls_verifying"></a>
<figure class="diagram" style="width:80%">

![Diagram of verifying a signature.](images/diagrams/bls-verifying.svg)

<figcaption>

To verify that a particular validator signed a particular message we use the validator's public key, the original message, and the signature. The verification operation outputs true if the signature is correct and false otherwise.

</figcaption>
</figure>

The verification will return `True` if and only if the signature corresponds both to the public key (that is, the signature and the public key were both generated from the same secret key) and to the message (that is, the message is identical to the one that was signed originally). Otherwise, it will return `False`.

#### Aggregation

So far we've looked at the basic set up of BLS signatures. In functional terms, what we've seen is very similar to any other digital signature scheme. Where the magic happens is in _aggregation_.

Aggregation means that multiple signatures over the same message &ndash; potentially thousands of signatures &ndash; can be checked with a single verification operation. Furthermore, the aggregate signature has the same size as a regular signature, 96 bytes. This is a massive gain in scalability, and it is this gain that fundamentally makes the Ethereum&nbsp;2 consensus protocol viable.

How does this work? Recall that public keys and signatures are elliptic curve points. Because of the bilinearity property of the pairing function, $e()$, it turns out that we can form linear combinations of public keys and signatures over the same message, and verification still works as expected.

This statement is a little opaque; let's go step by step.

##### Aggregating signatures

In the following we will only consider aggregation of signatures over the same message[^fn-aggregation-terminology].

[^fn-aggregation-terminology]: A note on terminology. The [original paper](https://eprint.iacr.org/2018/483.pdf) describing this scheme uses the term "multi-signature" when combining signatures over the same message, and "aggregate signature" when combining signatures over distinct messages. In Eth2 we only do the former, and just call it aggregation.

The process is conceptually very simple: we simply "add up" the signatures. The exact operations are not like the normal addition of numbers that we are familiar with, but the operation is completely analogous. Addition of points on the elliptic curve is the group operation for the $G_2$ group, and each signature is a point in this group, thus the result is also a point in the group.  An aggregated signature is mathematically indistinguishable from a non-aggregated signature, and has the same 96 byte size.

<a id="img_bls_signature_aggregation"></a>
<figure class="diagram" style="width:60%">

![Diagram showing aggregation of signatures.](images/diagrams/bls-signature_aggregation.svg)

<figcaption>

Aggregation of signatures is simply group addition in the $G_2$ group.

</figcaption>
</figure>

##### Aggregating public keys

To verify an aggregate signature, we need an aggregate public key. As long as we know exactly which validators signed the original message, this is equally easy to construct. Once again we simply "add up" the public keys of the signers. This time the addition is the group operation of the $G_1$ elliptic curve group, and the result will also be a member of the $G_1$ group, so it is mathematically indistinguishable from a non-aggregated public key, and has the same 48 byte size.

<a id="img_bls_pubkey_aggregation"></a>
<figure class="diagram" style="width:60%">

![Diagram of public key aggregation.](images/diagrams/bls-pubkey_aggregation.svg)

<figcaption>

Aggregation of public keys is simply group addition in the $G_1$ group.

</figcaption>
</figure>

##### Verifying aggregate signatures

Since aggregate signatures are indistinguishable from normal signatures, and aggregate public keys are indistinguishable from normal public keys, we can simply feed them into our normal verification algorithm.

<a id="img_bls_aggregate_verify"></a>
<figure class="diagram" style="width:70%">

![Diagram of verification of an aggregate signature.](images/diagrams/bls-aggregate_verify.svg)

<figcaption>

Verification of an aggregate signature is identical to verification of a normal signature as long as we use the corresponding aggregate public key.

</figcaption>
</figure>

This miracle is due to the bilinearity of the pairing operation. With an aggregate signature $\sigma_{agg}$ and a corresponding aggregate public key $pk_{agg}$, and common message $m$, we have the following identity, which is exactly the same as the verification identity for a single signature and public key.

$$
\begin{aligned}
e(pk_{agg},H(m)) &= e(pk_1 + pk_2 + \cdots + pk_n,H(m)) \\
                 &= e([sk_1 + sk_2 + \cdots + sk_n]g_1,H(m)) \\
                 &= {e(g_1,H(m))}^{(sk_1 + sk_2 + \cdots + sk_n)} \\
                 &= e(g_1,[sk_1 + sk_2 + \cdots + sk_n]H(m)) \\
                 &= e(g_1,\sigma_1 + \sigma_2 + \cdots + \sigma_n) \\
                 &= e(g_1,\sigma_{agg})
\end{aligned}
$$

##### Benefits of aggregation

Verification of a BLS signature is expensive (resource intensive) compared with verification of an ECDSA signature &ndash; more than an order of magnitude slower due to the pairing operation &ndash; so what benefits do we gain?

The benefits accrue when we are able to aggregate significant numbers of signatures. This is exactly what we have with beacon chain attestation committees. Ideally, all the validators in the committee sign-off on the same attestation data, so all their signatures can be aggregated. In practice, there might be differences of opinion about the chain state between committee members resulting in two or three different attestations, but even so there will be many fewer aggregates than the total number of committee members.

###### Speed benefits

To a first approximation, then, we can verify all the attestations of a whole committee &ndash; potentially hundreds &ndash; with a single signature verification operation.

This is a first approximation because we also need to account for aggregating the public keys and the signatures. But these aggregation operations involve only point additions in their respective elliptic curve groups, which are very cheap compared with the verification.

In summary:

  - We can verify a single signature with two pairings.
  - We can naively verify $N$ signatures with $2N$ pairings.
  - Or we can verify $N$ signatures via aggregation with just two pairings, $N-1$ additions in $G_1$, and $N-1$ additions in $G_2$. Each elliptic curve point addition is much, much cheaper than a pairing.

###### Space benefits

There is also a huge space saving when we aggregate signatures.

An aggregate signature has 96 bytes as all BLS signatures do. So, to a first approximation, an aggregate of $N$ signatures occupies $\frac{1}{N}$ the space of the unaggregated signatures.

Again, this is only a first approximation. The subtlety here is that, in order to construct the corresponding aggregate public key, we somehow need to keep track of which validators signed the message. We cannot assume that the whole committee participated, and we need to be careful not to include any validator more than once.

If we know in advance who the members of the committee are and how they are ordered then this tracking can be done at the marginal cost of one bit per validator: true if the validator contributed to the aggregate, false if it did not.

##### The full picture

This diagram illustrates the full flow from signing, through aggregating, to verifying. There are three validators in this case, although there could be many more, and each is signing the same message contents. Each validator has its own unique secret key and public key pair. The workflow is entirely non-interactive, and any of the actions before the verification can happen independently. Even the aggregation can be done incrementally.

<a id="img_bls_aggregation"></a>
<figure class="diagram" style="width:80%">

![Diagram showing the end-to-end aggregate signature workflow.](images/diagrams/bls-aggregation.svg)

<figcaption>

The end-to-end aggregate signature workflow. Verifying the single aggregate signature is much faster than verifying the original signatures separately.

</figcaption>
</figure>

##### Aggregation examples

Two useful examples of how aggregate signatures are used in practice are in aggregate attestations and in sync committee aggregates.

###### Aggregate attestations

Aggregate attestations are a very compact way to store and prove which validators made a particular attestation.

Within each beacon chain committee at each slot, individual validators attest to their view of the chain, as described in the [validator spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#attesting).

An [`Attestation`](/part3/containers/operations/#attestation) object looks like this:

```python
class Attestation(Container):
    aggregation_bits: Bitlist[MAX_VALIDATORS_PER_COMMITTEE]
    data: AttestationData
    signature: BLSSignature
```

When making its attestation, the validator sets a single bit in the `aggregation_bits` field to indicate which member of the committee it is. That is sufficient, in conjunction with the slot number and the committee index, to uniquely identify the attesting validator in the global validator set.

The `signature` field is the validator's [signature](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#aggregate-signature) over the `AttestationData` in the `data` field.

This attestation will later be [aggregated](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#attestation-aggregation) with other attestations from the committee that contain identical `data`. An attestation is added to an aggregate by copying over its bit from the `aggregation_bits` field and adding (in the sense of elliptic curve addition) its signature to the `signature` field. Aggregate attestations can be aggregated together in the same way, but only if their `aggregation_bits` lists are disjoint: we must not include a validator more than once. (In principle we could include individual validators multiple times, but then we'd need more than a single bit to track how many times, and the redundancy is not useful.)

This aggregate attestation will be gossiped around the network and eventually included in a block. At each step the aggregate signature will be verified.

To verify the signature, a node needs to reconstruct the list of validators in the committee, which it can do from the information in the [`AttestationData`](/part3/containers/dependencies/#attestationdata):

```python
class AttestationData(Container):
    slot: Slot
    index: CommitteeIndex
    beacon_block_root: Root
...
```

Given the reconstructed list of committee members, the validating node filters the list according to which `aggregation_bits` are set in the attestation. Now it has the indices of all the validators that contributed to this attestation. The node retrieves the public keys of those validators from the beacon state and aggregates those keys together (by elliptic curve addition).

Finally, the aggregate signature, the aggregate public key, and the signing root of the `data` are fed into the standard BLS signature verification function. If all is well this will return `True`, else the aggregate attestation is invalid.

###### Sync aggregates

[`SyncAggregate`](/part3/containers/operations/#syncaggregate)s are produced by a sync committee of 512 members.

```python
class SyncAggregate(Container):
    sync_committee_bits: Bitvector[SYNC_COMMITTEE_SIZE]
    sync_committee_signature: BLSSignature
```

The current members of the [`SyncCommittee`](/part3/containers/dependencies/#synccommittee) are stored in the beacon state in the following form:

```python
class SyncCommittee(Container):
    pubkeys: Vector[BLSPubkey, SYNC_COMMITTEE_SIZE]
    aggregate_pubkey: BLSPubkey
```

Production and aggregation of sync committee messages [differs slightly](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/validator.md#sync-committees) from attestations, but is sufficiently similar that I'll skip over it here.

The main points of interest are that the `SyncCommittee` object contains the actual public keys of all the members (possibly with duplicates), rather than validator indices. It also contains a pre-computed `aggregate_pubkey` field that is the aggregate of all the public keys in the committee.

The idea of this is to reduce the computation load for light clients, who will be the ones needing to verify the `SyncAggregate` signatures. Sync committees are expected to have high participation, with, say, 90% of the validators contributing. To verify the aggregate signature we need to aggregate the public keys of all the contributors. Starting from an empty set, that would mean 461 elliptic curve point additions (90% of 512). However, if we start from the _full_ set, `aggregate_pubkey`, then we can achieve the same thing by _subtracting_ the 10% that did not participate. That's 51 elliptic curve subtractions (which have the same cost as additions) and nine times less work.

#### Various topics

##### Domain separation and forks

Every signature that's used in the Eth2 protocol has a `domain` value mixed into the message before signing. This is taken care of by the [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root) function which both calculates the SSZ [hash tree root](/part2/building_blocks/merkleization/) of the object to be signed and mixes in the given domain.

```python
def compute_signing_root(ssz_object: SSZObject, domain: Domain) -> Root:
    return hash_tree_root(SigningData(
        object_root=hash_tree_root(ssz_object),
        domain=domain,
    ))
```

The domain, in turn, is calculated by the [`compute_domain()`](/part3/helper/misc/#def_compute_domain) function which combines one of ten [domain types](/part3/config/constants/#domain-types) with a mash-up of the [fork version](/part3/config/types/#version) and the [genesis validators root](/part3/containers/state/#genesis_validators_root).

Each of the extra quantities that's rolled into the message has a specific purpose.

  - The domain type ensures that signatures made for one purpose cannot be re-used for a different purpose. Objects of different SSZ types are not guaranteed to have unique hash tree roots, and we would rather like to be able to tell the difference between them. The ten [domain types](/part3/config/constants/#domain-types) are all the different ways signatures are used in the protocol.
  - The genesis validators root uniquely identifies this particular beacon chain, distinguishing it from any other testnet or alternative chain. This ensures that signatures from different chains are always incompatible.
  - The fork version identifies deliberate consensus upgrades to the beacon chain. Mixing the fork version into the message ensures that messages from validators that have not upgraded are invalid. They are out of consensus and have no information that is useful to us, so this provides a convenient way to ignore their messages. Alternatively, a validator may wish to operate on both sides of a contentious fork, and the fork version provides a way for them to do so safely.

The sole exception to the mixing-in of the fork version is signatures on deposits. Deposits are always valid, however the beacon chain gets upgraded.

##### Choice of groups

BLS signatures are based on two elliptic curve groups, $G_1$ and $G_2$. Elements of $G_1$ are small (48 bytes when serialised), and their group arithmetic is faster; elements of $G_2$ are large (96 bytes when serialised) and their group arithmetic is slower, perhaps three times slower.

We can choose to use either group for public keys, as long as we use the other group for signatures: the pairing function doesn't care; everything still works if we swap the groups over. The [original paper](https://eprint.iacr.org/2018/483.pdf) describing BLS aggregate signatures has public keys in $G_2$ and signatures in $G_1$, while for Ethereum&nbsp;2 we made the opposite choice.

The main reason for this is that we want public key aggregation to be as fast as possible. Signatures are verified much more often than they are aggregated &ndash; by far the main load on beacon chain clients currently is signature verification &ndash; and verification requires public key aggregation. So we choose to have our public keys in the faster $G_1$ group. This also has the benefit of reducing the size of the beacon state, since public keys are stored in validator records. If we were to use the $G_2$ group for public keys, the beacon state would be about 35% larger.

The trade-off is that protocol messages and beacon chain blocks are larger due to the larger signature size.

Fundamentally, verification of aggregate signatures is an "on-chain" activity that we wish to be as light as possible, and signature aggregation is "off-chain" so can be more heavyweight.

##### Proof of possession

There is a possible attack on the BLS signature scheme that we wish to avoid, the "rogue public key" attack.

Say your public key is $pk_1$, and I have a secret key, $sk_2$. But instead of publishing my true public key, I publish $pk'_2=[sk_2]g_1-pk_1$ (that is, my real public key plus the inverse of yours). I can sign a message $H(m)$ with my secret key to make $\sigma=[sk_2]H(m)$. I then publish this claiming that it is an aggregate signature that both you and I have signed.

Now, when verifying with my rogue public key and your actual public key, the claim checks out: it looks like you signed the message when you didn't: $e(g1,\sigma)=e(g_1,[sk_2]H(m))=e([sk_2]g_1,H(m))=e(pk_1+pk'_2,H(m))$.

One relatively simple defence against this &ndash; the one we are using in Ethereum&nbsp;2 &ndash; is to force validators to register a "proof of possession" of the secret key corresponding to their claimed public key. You see, the attacker doesn't have and cannot calculate the $sk'_2$ corresponding to $pk'_2$. The proof of possession can be done simply by getting all validators to sign their public keys on registration, that is, when they deposit their stakes in the deposit contract. If the actual signature validates with the claimed public key then all is well.

##### Threshold signatures

In addition to aggregation, the BLS scheme also supports [threshold signatures](https://alinush.github.io/2020/03/12/scalable-bls-threshold-signatures.html). This is where a secret key is divided between $N$ validators. For a predefined value of $M \le N$, if $M$ of the validators sign a message then a single joint public key of all the validators can be used to verify the signature.

Threshold signatures are not currently used within the core Ethereum&nbsp;2 protocol. However, they are useful at an infrastructure level. For example, for security and resilience it might be desirable to split a validator's secret key between multiple locations. If an attacker acquires fewer than $M$ shares then the key still remains secure; if up to  $N-M$ keystores are unavailable, the validator can still sign correctly. An operational example of this is Attestant's [Dirk](https://www.attestant.io/posts/introducing-dirk/) key manager.

Threshold signatures also find a place in Distributed Validator Technology, which I will write about in a different chapter.

[TODO - link to DVT when done]::

##### Batch verification

The bilinearity of the pairing function allows for some pretty funky optimisations. For example, Vitalik has formulated a method for [verifying a batch](https://ethresear.ch/t/fast-verification-of-multiple-bls-signatures/5407?u=benjaminion) of signatures simultaneously &ndash; such as all the signatures contained in a block &ndash; that significantly reduces the number of pairing operations required. Since this technique constitutes a client-side optimisation rather than being a fundamental part of the protocol, I shall describe it properly in the Implementation chapter.

[TODO - link to batch verification when done]::

##### Quantum security

The security (unforgeability) of BLS signatures relies on, among other things, the hardness of something called the elliptic curve discrete logarithm problem (ECDLP)[^fn-discrete-division-problem]. Basically, given the public key $[sk]g_1$ it is computationally infeasible to work out what the secret key $sk$ is.

[^fn-discrete-division-problem]: It's puzzling to me that this is called the discrete logarithm problem when we write groups additively, rather than the discrete division problem. But it's far from being the most confusing thing about elliptic curves.

The ECDLP is believed to be vulnerable to attack by [quantum computers](https://en.wikipedia.org/wiki/Elliptic-curve_cryptography#Quantum_computing_attacks), thus our signature scheme may have a limited shelf-life.

Quantum-resistant alternatives such as [zkSTARKs](https://eprint.iacr.org/2018/046.pdf) are known, but currently not as practical as the BLS scheme. The expectation is that, at some point, we will migrate to such a scheme as a drop-in replacement for BLS signatures.

In case someone overnight unveils a sufficiently capable quantum computer, [EIP-2333](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-2333.md) (which is a standard for BLS key generation in Ethereum) describes a way to generate a hierarchy of [Lamport signatures](https://en.wikipedia.org/wiki/Lamport_signature). Lamport signatures are believed to be quantum secure, but come with their own limitations. In principle, we could make an emergency switch over to these to tide us over while implementing STARKs. But this would be extremely challenging in practice.

#### BLS library functions

As a reference, the following are the BLS library functions used in the Ethereum&nbsp;2 [specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/beacon-chain.md#bls-signatures). They are named for and defined by the draft [BLS Signature Standard](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04)[^fn-ietf-irtf-0]. Function names link to the definitions in the standard. Since we use the [proof of possession](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-3.3) scheme defined in the standard, our `Sign`, `Verify`, and `AggregateVerify` functions correspond to `CoreSign`, `CoreVerify`, and `CoreAggregateVerify` respectively.

[^fn-ietf-irtf-0]: This document does not have the full force of an IETF standard. For one thing, it remains a draft (that is now expired), for another it is an IRTF document, meaning that it is from a research group rather than being on the IETF standards track. [Some context](https://mailarchive.ietf.org/arch/msg/ietf/A8MaBwNpbWf_DJoWj0sRROIml3Y/) from Brian Carpenter, former IETF chair,
    > I gather that you are referring to an issue in draft-irtf-cfrg-bls-signature-04. That is not even an IETF draft; it's an IRTF draft, apparently being discussed in an IRTF Research Group. So it is not even remotely under consideration to become an IETF standard...

  - `def` [`Sign`](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-2.6)`(privkey: int, message: Bytes) -> BLSSignature`
    - Sign a message with the validator's secret (private) key.
  - `def` [`Verify`](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-2.7)`(pubkey: BLSPubkey, message: Bytes, signature: BLSSignature) -> bool`
    - Verify a signature given the public key and the message.
  - `def` [`Aggregate`](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-2.8)`(signatures: Sequence[BLSSignature]) -> BLSSignature`
    - Aggregate a list of signatures.
  - `def` [`FastAggregateVerify`](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-3.3.4)`(pubkeys: Sequence[BLSPubkey], message: Bytes, signature: BLSSignature) - bool`
    - Verify an aggregate signature given the message and the list of public keys corresponding to the validators that contributed to the aggregate signature.
  - `def` [`AggregateVerify`](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-2.9)`(pubkeys: Sequence[BLSPubkey], messages: Sequence[Bytes], signature: BLSSignature) -> bool`
    - This is not used in the current spec but appears in the future [Proof of Custody spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/_features/custody_game/beacon-chain.md). It takes $n$ messages signed by $n$ validators and verifies their aggregate signature. The mathematics is similar to that above, but requires $n+1$ pairing operations rather than just two. But this is better than the $2n$ pairings that would be required to verify the unaggregated signatures.
  - `def` [`KeyValidate`](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-2.5)`(pubkey: BLSPubkey) -> bool`
    - Checks that a public key is valid. That is, it lies on the elliptic curve, it is not the group's identity point (corresponding to the zero secret key), and it is a member of the $G_1$ subgroup of the curve. All these checks are important to avoid certain attacks. The group membership check is quite expensive but only ever needs to be done once per public key stored in the beacon state.

The Eth2 spec also defines two further BLS utility functions, `eth_aggregate_pubkeys()` and `eth_fast_aggregate_verify()` that I describe in the [annotated spec](/part3/helper/crypto/#bls-signatures).

#### See also

The main standards that we strive to follow are the following IRTF drafts:

  - [BLS Signatures](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04)
  - [Hashing to Elliptic Curves](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-hash-to-curve-09)
  - [Pairing-Friendly Curves](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-pairing-friendly-curves-10)

[Compact Multi-Signatures for Smaller Blockchains](https://eprint.iacr.org/2018/483.pdf) (Boneh, Drijvers, Neven) is the original paper that described efficient BLS multi-signatures. And [Pragmatic signature aggregation with BLS](https://ethresear.ch/t/pragmatic-signature-aggregation-with-bls/2105?u=benjaminion) is Justin Drake's proposal to use these signatures in an Ethereum&nbsp;2 context.

For a gentle(ish) introduction to pairings, Vitalik's [Exploring Elliptic Curve Pairings](https://medium.com/@VitalikButerin/exploring-elliptic-curve-pairings-c73c1864e627) is very good. If you are looking for a very deep rabbit hole to explore, [Pairings for Beginners](https://www.craigcostello.com.au/s/PairingsForBeginners.pdf) by Craig Costello is amazing.

I've written a lengthy homage to the [BLS12-381](https://hackmd.io/@benjaminion/bls12-381) elliptic curve that also covers some BLS signature topics.

Three EIPs are intended to govern the generation and storage of keys in practice:

  - [EIP-2333](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-2333.md) provides a method for deriving a tree-hierarchy of BLS12-381 keys based on an entropy seed.
  - [EIP-2334](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-2334.md) defines a deterministic account hierarchy for specifying the purpose of keys.
  - [EIP-2335](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-2335.md) specifies a standard keystore format for storage and interchange of BLS12-381 keys.

There are several implementations of pairings on the BLS12-381 curve around, which can be used to implement the BLS signature scheme we use:

  - The [Blst](https://github.com/supranational/blst) library is the most commonly used by Eth2 client implementers.
  - The [noble-bls12-381](https://github.com/paulmillr/noble-bls12-381) library is better documented and may be more enjoyable if you want to try playing around with these things.

### 随机性 <!-- /part2/building_blocks/randomness/ -->

<div class="summary">

  - 不可预测地分配信标链职责是防御某些攻击的重要手段。
  - 信标链维护一个 RANDAO 以积累随机性。
  - 提议区块、委员会分配和同步委员会的参与等职责都是基于RANDAO 分配的，并且有一个有限的提前期（lookahead period）。
  - 通过对时段编号进行 BLS 签名，区块提议者以可验证的方式为 RANDAO 提供随机性。
  - 验证者能够在一定程度上影响 RANDAO，但在实践中这不是一个显著的问题。

</div>

#### 引言

无论是出于安全考虑还是公平考虑，随机性都是无许可区块链协议的重要组成部分。

一个完全可预测的协议可能在良性环境中良好运行。但必须假设我们的协议会受到攻击，而可预测性会给攻击者提供机会——就像犯罪惊悚片中的坏人经常利用受害者可预测的日常生活一样。

如果攻击者事先知道哪些验证者会在不同角色中发挥作用，那么该攻击者就会有很大的机会发动攻击。例如，选择性地对未来的提议者进行拒绝服务攻击，或贿赂特定委员会的成员，或为自己注册特别有利的验证者编号以便接管未来的委员会，或干脆审查交易。[^fn-initial-shuffling]

[^fn-initial-shuffling]: For a cute illustration of the perils of insufficient unpredictability, see [Issue 1446](https://github.com/ethereum/consensus-specs/issues/1446) on the specs repo: Manipulating deposit contract to gain an early majority. Hat-tip to [Paul Hauner](https://web.archive.org/web/20230630135550/https://nitter.it/paulhauner/status/1509677010448121856).

引用 Brown-Cohen 等人[论文](https://arxiv.org/abs/1809.06528)中的话[^fn-unpredictability-paper],

> 直观地说，协议具有不可预测性是件好事，因为直到某个区块即将被挖出之前，矿工们才会知道自己有资格挖出该区块。如果矿工事先知道自己何时有资格挖矿，那么许多攻击（如双花或自私挖矿）就会变得更有利可图。

[^fn-unpredictability-paper]: [Formal Barriers to Longest-Chain Proof-of-Stake Protocols](https://arxiv.org/abs/1809.06528), Jonah Brown-Cohen, Arvind Narayanan, Christos-Alexandros Psomas, and S. Matthew Weinberg (2018). Quotation is from section 3.1.

由随机性产生的不可预测性，是应对许多攻击的第一道绝佳防线。

工作量证明的不可预测性来自于开采区块的过程。只有满足[特定条件](https://ethereum.org/en/developers/docs/consensus-mechanisms/pow/)的区块才是有效的，而满足该条件的唯一方法就是试错。矿工随机猜测，测试，如果不正确就再试——这就是工作量证明中的“工作量”。只有猜测正确，区块才有效，矿工才能拓展链。在我写作这些内容时，以太坊 PoW 链的难度大约是 12.5 Peta 哈希。这意味着挖掘一个以太坊区块平均需要 $1.25 \times 10^{16}$ 次猜测。这相当于掷 21 个骰子，直到它们在同一次掷骰中所有筛子同时显示出 6 点。这几乎不可能，但在以太坊网络上，大约每 13 秒就会有人成功做到这一点。由于这个过程是统一的——没有人比其他人更擅长猜测（掷骰子）——它提供了公平性。每秒千兆哈希与每秒千兆哈希是等效的（尽管在工作量证明中还有其他不公平的来源）。由于猜测是随机的，它提供了不可预测性，从而减轻了上述攻击的影响。

以太坊权益证明协议中的随机性[^fn-pseudo-random]为区块提议者的选择带来了不可预测性，也为认证区块和签署同步数据的委员会成员的选择带来了不可预测性。

[^fn-pseudo-random]: I'm not going to distinguish the niceties of randomness and pseudo-randomness in this section. We are actually using pseudo-randomness seeded with (presumed) genuine randomness. It must be the case as it is impossible to come to consensus on genuine randomness. However, I will just call it "randomness" throughout.

在本节中，我们将探讨将随机性引入信标链的方式，它的一些用途，以及当前方案的一些问题。

#### The RANDAO

The beacon chain design has always used a RANDAO[^fn-randao-naming] mechanism to provide its in-protocol randomness. A RANDAO is simply an accumulator that incrementally gathers randomness from contributors. So, with each block, the proposer mixes in a random contribution to the existing RANDAO value.

[^fn-randao-naming]: I'm not certain where the name RANDAO comes from, but it's modelled as a DAO (decentralised autonomous organisation) that deals in randomness. The Ethereum [randao project](https://github.com/randao/randao) from 2016 may be the origin of the name.

To unpack that a little, the beacon chain maintains a RANDAO value. Every block included in the chain contains a verifiable random value provided by the validator that proposed it, its [`randao_reveal`](/part3/containers/blocks/#beaconblockbody). As each block is processed the beacon chain's RANDAO value is mixed with the `randao_reveal` from the block. Thus, over time, the RANDAO accumulates randomness from all the block proposers.

If $R_n$ is the RANDAO value after $n$ contributions, and $r_n$ is the $n$th `randao_reveal`, then the following holds. Here we are mixing in the new contribution using the `xor` function, $\oplus$. Alternatives might be to use a sum or a hash, but `xor` is simple and has useful properties.

$$
R_n = r_n \oplus R_{n-1}
$$

We can think of a RANDAO as being like a deck of cards that's passed round the table, each person shuffling it in turn: the deck gets repeatedly re-randomised. Even if one contributor's randomness is weak, the cumulative result has a high level of entropy.

<a id="img_randomness_shuffle"></a>
<figure class="diagram" style="width:80%">

![Diagram illustrating repeated shuffling of a deck of cards.](images/diagrams/randomness-shuffle.svg)

<figcaption>

We can imagine the RANDAO as a deck of cards that accumulates randomness over time as each participant shuffles the deck in turn.

</figcaption>
</figure>

Current and past RANDAO values are stored in the [beacon state](/part3/containers/state/#beaconstate) in the `randao_mixes` field. The current value is updated by [`process_randao`](/part3/transition/block/#def_process_randao) with every block that the beacon chain processes. If there is no block in a slot then the RANDAO is not updated. In addition to the RANDAO's current value, [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector) (minus one) past values of the RANDAO at the ends of epochs are also stored in the state. These can be used to recalculate past committee assignments, which allows historical attestations to be slashed even months later.

#### Source of randomness

Every [block](/part3/containers/blocks/#beaconblockbody) includes a field `randao_reveal` that is its proposer's contribution to be mixed in to the RANDAO.

This contribution needs to satisfy two properties: it should be unpredictable by any other node, yet it should be verifiable by all nodes.

"Verifiable" means that, although random (read pseudo-random), the RANDAO contribution value must not be arbitrary. The proposer must not be able to pick and choose its contribution, otherwise it will just choose a value that gives itself some sort of advantage. There must be a single valid contribution that the proposer can make in any given block, and all the other nodes must be able to verify that contribution.

##### The old: hash onions

[Early ideas](https://github.com/ethereum/consensus-specs/pull/33/files#diff-d74f72ec8cd401e342e5e5f6939647b860dd98518a6618d3a7f5256edbaf4b69R480) for verifiable randomness had each validator pre-committing to a "hash onion". Before joining the beacon chain a validator would generate a random number. When registering its initial deposit the validator would include the result of repeatedly cryptographically hashing that number a large number (thousands) of times as a commitment. Then when proposing a block the `randao_reveal` would be the pre-image of that commitment: one layer would be "peeled off the onion". Since a cryptographic hash is not invertible, only the proposer could calculate this value, but it's easily verifiable by everyone. Then the reveal gets stored as the new commitment and so on.

This scheme is viable, but has complexities and edge cases &ndash; for example, if a block gets orphaned, everybody (except the beacon chain) can now see the proposer's reveal &ndash; that make it clunky to implement in practice.

##### The new: BLS signatures

An alternative to the hash onion became available when we moved to using [BLS signatures](/part2/building_blocks/signatures/) in the protocol. With the BLS scheme every validator already has a closely guarded random value: the secret key that it uses for signing blocks and attestations. As far as anyone knows the signatures produced are uniformly random.

The signature for an agreed message nicely satisfies our two desired properties for the RANDAO contribution. It is unpredictable to the other validators since they do not know the proposer's private key, but it is easily verifiable since all validators know the proposer's public key.

The elegance and simplicity of reusing the BLS key infrastructure for the RANDAO makes it a considerable improvement on the original hash onion design.

There is a further nice benefit to using BLS signatures that may not be obvious. The [aggregation property](/part2/building_blocks/signatures/#aggregation) of the signatures allows the contribution to be derived via a multi-party computation. That is, signatures from multiple validators can be combined into a threshold signature so that they can effectively act as a single validator. We do not use this property within the core Eth2 protocol, but it enables [Distributed Validator Technology](https://docs.obol.tech/docs/int/key-concepts), which would be very difficult with the old hash onion approach.

[TODO: add link to DVT when done]::

For all these reasons, [we now use](https://github.com/ethereum/consensus-specs/pull/483) a BLS signature as the entropy contribution to the RANDAO, that is, the `randao_reveal`.

##### Where does the entropy come from?

Evidently the predominant source of randomness in the Ethereum 2 protocol is the secret keys of the validators. If every validator key is generated uniformly randomly and independently then each contributes 256 bits of entropy to the overall pool. However, keys are sometimes not independently generated[^fn-vasily]. [EIP-2333](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-2333.md) provides a way to derive multiple validator keys from a single entropy seed, and large stakers are likely to have done this. Thus, the total entropy from $N$ validator keys will be less than $N \times 256$ bits, but we don't know how much less.

[^fn-vasily]: I am indebted to Vasiliy Shapovalov for reminding me of this.

Some other sources of entropy for the RANDAO are noted in [EIP-4399](https://eips.ethereum.org/EIPS/eip-4399).

  - Missed or orphaned block proposals directly affect the RANDAO's output. Network conditions, node faults, or maintenance downtime can all lead to missed block proposals that have a degree of randomness.
  - The total number of active validators in an epoch affects the selection of proposers which in turn affects participation in the RANDAO. Thus, deposits and exits (both voluntary and forced) contribute entropy.
  - A validator's [effective balance](/part2/incentives/balances/) affects its likelihood of being selected to propose a block. Thus, changes in effective balances (perhaps due to one or more validators being offline for a period of time) add entropy.

#### Updating the RANDAO

When a validator proposes [a block](/part3/containers/blocks/#beaconblockbody), it includes a field `randao_reveal` which has `BLSSignature` type. This is the proposer's signature over the [epoch number](https://github.com/ethereum/consensus-specs/pull/498), using its normal signing secret key.

The `randao_reveal` is [computed](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#randao-reveal) by the proposer as follows, the `privkey` input being the validator's random secret key.

```python
def get_epoch_signature(state: BeaconState, block: BeaconBlock, privkey: int) -> BLSSignature:
    domain = get_domain(state, DOMAIN_RANDAO, compute_epoch_at_slot(block.slot))
    signing_root = compute_signing_root(compute_epoch_at_slot(block.slot), domain)
    return bls.Sign(privkey, signing_root)
```

When a block is processed, the `randao_reveal` is mixed into the RANDAO [like this](/part3/transition/block/#def_process_randao):

```python
def process_randao(state: BeaconState, body: BeaconBlockBody) -> None:
    epoch = get_current_epoch(state)
    # Verify RANDAO reveal
    proposer = state.validators[get_beacon_proposer_index(state)]
    signing_root = compute_signing_root(epoch, get_domain(state, DOMAIN_RANDAO))
    assert bls.Verify(proposer.pubkey, signing_root, body.randao_reveal)
    # Mix in RANDAO reveal
    mix = xor(get_randao_mix(state, epoch), hash(body.randao_reveal))
    state.randao_mixes[epoch % EPOCHS_PER_HISTORICAL_VECTOR] = mix
```

Two things are going on in the processing of the `randao_reveal` signature.

First, the signature is verified using the proposer's public key before being mixed in. This means that the proposer has almost no choice about what it contributes to the RANDAO: it either contributes a single verifiable value &ndash; the correct signature over the epoch number &ndash; or it withholds its block and contributes nothing. (Equivalently, a block with an incorrect reveal is invalid.)

Second, the hash of the signature is mixed in to the beacon state's RANDAO using an `xor` operation. We apply the hash operation to reduce the length of the RANDAO accumulator from about 762 bits &ndash; the length of a compressed BLS signature, an inconvenient number of bits to work with &ndash; to 256 bits. The uniformity of the output of hash functions is also better established than that of BLS signatures.

<a id="img_randomness_reveal"></a>
<figure class="diagram" style="width:80%">

![Diagram illustrating updating the RANDAO.](images/diagrams/randomness-reveal.svg)

<figcaption>

What's really happening when the RANDAO is shuffled. The signature over the epoch number is the RANDAO reveal that the proposer includes in its block. This is hashed then mixed in to the existing RANDAO with an `xor` operation.

</figcaption>
</figure>

<a id="randao_xor"></a>
We could have mixed in the reveal by hashing it directly with the RANDAO accumulator, however we choose to mix it in via an `xor` operation. The combination of using the epoch number as the signed quantity and using `xor` to mix it in leads to a subtle, albeit tiny, [improvement](https://github.com/ethereum/consensus-specs/pull/496#issuecomment-457449830) in attack-resistance of the RANDAO. Justin Drake explains in his [notes](https://notes.ethereum.org/@JustinDrake/rkPjB1_xr):

> Using `xor` in `process_randao` is (slightly) more secure than using `hash`. To illustrate why, imagine an attacker can grind randomness in the current epoch such that two of his validators are the last proposers, in a different order, in two resulting samplings of the next epochs. The commutativity of `xor` makes those two samplings equivalent, hence reducing the attacker's grinding opportunity for the next epoch versus `hash` (which is not commutative). The strict security improvement may simplify the derivation of RANDAO security formal lower bounds.

We will see [shortly](#randao-biasability) that it can be advantageous to an attacker to have control of the last slots of an epoch. Justin's [point](https://github.com/ethereum/consensus-specs/pull/496#issuecomment-457546253) is that, under the current scheme, the attacker having validators $V_0, V_1$ in the two last slots of an epoch is equivalent to it having $V_1, V_0$ with respect to the `randao_reveal`s. This fractionally reduces an attacker's choices when it comes to influencing the RANDAO. If we used `hash` rather than `xor`, or if we signed over the slot number rather than the epoch number, these orderings would result in different outcomes from each other, giving an attacker more choice and therefore more power.

#### Lookahead

We started this section with a discussion of unpredictability. Ideally, it should not be possible to predict the duties for any block proposer or committee member until the moment they become active. However, in practice, proposers and committee members need a little advance notice of their duties to allow them to join the right p2p network subnets and do whatever other preparation they need to do.

The RANDAO seed at the end of epoch $N$ is used to compute validator duties for the whole of epoch $N+2$. This interval is controlled by [`MIN_SEED_LOOKAHEAD`](/part3/config/preset/#min_seed_lookahead) via the [`get_seed()`](/part3/helper/accessors/#def_get_seed) function. Thus, validators have at least one full epoch to prepare themselves for any duties, but no more than two.

Under normal circumstances, then, an attacker is not able to predict the duty assignments more than two epochs in advance. However, if an attacker has a large proportion of the stake or is, for example, able to mount a DoS attack against block proposers for a while, then it might be possible for the attacker to predict the output of the RANDAO further ahead than `MIN_SEED_LOOKAHEAD` would normally allow. The attacker might then use this foreknowledge to strategically exit validators or make deposits[^fn-instant-activations] in order to gain control of a committee, or a large number of block proposal slots.

[^fn-instant-activations]: In the current protocol you'd need to predict the RANDAO for around 16 hours ahead for deposits to be useful in manipulating it, due to [`ETH1_FOLLOW_DISTANCE`](/part3/config/configuration/#eth1_follow_distance) and [`EPOCHS_PER_ETH1_VOTING_PERIOD`](/part3/config/preset/#epochs_per_eth1_voting_period). However, at some point post-Merge, it may become possible to onboard deposits more-or-less immediately.

It's certainly not an easy attack. Nonetheless, it is easy to defend against, so we might as well do so.

To prevent this, we assume a maximum feasible lookahead that an attacker might achieve, [`MAX_SEED_LOOKAHEAD`](/part3/config/preset/#max_seed_lookahead) and delay all activations and exits by this amount, which allows time for new randomness to come in via block proposals from honest validators, making irrelevant any manipulation by the entering or exiting validators. With `MAX_SEED_LOOKAHEAD` set to 4, if only 10% of validators are online and honest, then the chance that an attacker can succeed in forecasting the seed beyond (`MAX_SEED_LOOKAHEAD` ` - ` `MIN_SEED_LOOKAHEAD`) = 3 epochs is $0.9^{3\times 32}$, which is about 1 in 25,000.

<a id="img_randomness_lookahead"></a>
<figure class="diagram" style="width:90%">

![Diagram showing min and max lookahead.](images/diagrams/randomness-lookahead.svg)

<figcaption>

The RANDAO value at the end of epoch $N$ is used to set duties for epoch $N+2$, which is controlled by `MIN_SEED_LOOKAHEAD`. A validator exiting in epoch $N+1$ remains active until at least the end of epoch $N+5$ (depending on the exit queue). This is controlled by `MAX_SEED_LOOKAHEAD`.

</figcaption>
</figure>

##### Single Secret Leader Election

As currently implemented, both the minimum and maximum lookaheads smell a little of engineering hackery. In a perfect design only the block proposer would know ahead of time that it has been chosen to propose in that slot. Once its block is revealed then the rest of the network would be able to verify that, yes, this was indeed the chosen proposer. This feature is called [Single Secret Leader Election](https://eprint.iacr.org/2020/025). We do not yet have it in the Ethereum protocol, and I shall write about it elsewhere. Meanwhile, some [good progress](https://ethresear.ch/t/simplified-ssle/12315?u=benjaminion) is being made towards making it practical.

[TODO: link to SSLE when done]::

#### RANDAO biasability

The RANDAO value for an epoch is set at the end of the previous epoch, and duty assignments for the entire epoch (proposals and committee memberships) depend on that value. (Actually &ndash; due to [`MIN_SEED_LOOKAHEAD`](/part3/config/preset/#min_seed_lookahead) &ndash; on the RANDAO value at the end of the last-but-one epoch, but we'll overlook that in what follows.)

<a id="img_randomness_assignments"></a>
<figure class="diagram" style="width:80%">

![Diagram illustrating calculation of duties based on the RANDAO.](images/diagrams/randomness-assignments.svg)

<figcaption>

Future duty assignments for validators &ndash; block proposers, committee members, sync committee duty &ndash; are calculated based on the state of the RANDAO at the end of each epoch.

</figcaption>
</figure>

Thus, when a validator happens to be assigned to propose a block in the last slot of an epoch, it gains a small amount of control over the assignments for the next epoch. This is because it can choose to reveal its block, which mixes in its RANDAO reveal, or it can choose (at a cost) to withhold its block and keep the existing RANDAO value, knowing that there will be no subsequent RANDAO change before duties are calculated. In this way, a validator is able to exert a little influence over the proposer and committee assignments in the next epoch. This is called "one bit of influence" over the RANDAO as the validator has a choice of two outcomes.

<a id="img_randomness_biasing"></a>
<figure class="diagram" style="width:80%">

![Diagram illustrating biasing the RANDAO.](images/diagrams/randomness-biasing.svg)

<figcaption>

The last proposer in an epoch has a choice. It can propose its block as usual, updating the RANDAO, resulting in a set of duty assignments $A$. Or it can withhold its block, leaving the RANDAO as-is, resulting in a set of duty assignments $B$. If outcome $B$ gives the owner of the validator sufficient advantage to compensate for having missed a proposal, then it is an opportunity to "cheat".

</figcaption>
</figure>

If an attacker gets a string of proposals at the end of an epoch then it has more power. Having $k$ consecutive proposals at the end of an epoch gives the attacker $2^k$ choices for the ultimate value of the RANDAO that will be used to compute future validator duties. In this scenario the attacker has "$k$ bits of influence" over the RANDAO.

#### Biasability analyses

This section is fully optional. I got a bit carried away with the maths; it's fine to skip to the [next section](#verifiable-delay-functions).

To make discussion of RANDAO biasability more concrete I shall try to quantify what it means in practice with a couple of examples. In each case the entity "cheating" or "attacking" has control over a proportion of the stake $r$, either directly or through some sort of collusion, and we will assume that the remaining validators are all acting independently and correctly. We will also assume, of course, that individual `randao_reveal`s are uniformly random.

In the first example, I will try to gain control of the RANDAO by permanently acquiring proposals in the last slots of an epoch. In the second example I will try to improve my expected number of block proposals by biasing the RANDAO when I get the opportunity to do so. In both cases I will be selectively making and withholding proposals having computed the best outcome: a process of "grinding" the RANDAO.

These examples are intended only as illustrations. They are not academic studies, and there are lots of loose ends. It's very likely I've messed something up: probability is _hard_. I'd be very interested if anyone wanted to make them more rigorous and complete. Some related work, more simulation based, was previously done by [Runtime Verification](https://github.com/runtimeverification/rdao-smc/blob/master/report/rdao-analysis.pdf).

##### RANDAO takeover

If I control a proportion $r$ of the total stake, how much can I boost my influence over the protocol by manipulating the RANDAO?

The ability to influence the RANDAO depends on controlling a consecutive string of block proposals at the end of an epoch. We shall call this property "having a tail", and the tail will have a length $k$ from 0 to a maximum of 32, an entire epoch.

Our question can be framed like this: if I have a tail of length $k$ in one epoch, what is my expected length of tail in the next epoch? With a tail of length $k$ I have $2^k$ opportunities to reshuffle the RANDAO by selectively making or withholding block proposals. Can I grind through the possibilities to increase my tail length next time, and eventually take over the whole epoch?

In the absence of any manipulation, my probability of having a tail of length exactly $k$ in any given epoch is $(1-r)r^k$ for $k < 32$, and $r^{32}$ when $k = 32$. This is the chance that I make $k$ proposals in the tail positions preceded by a proposal that I did not make.

$$
q_k =
\begin{cases}
(1-r)r^k & 0 \leq k < 32 \\
r^k      & k = 32
\end{cases}
$$

So the expected tail length for someone controlling a proportion $r$ of the stake is,

$$
E(r) = \sum_{n=1}^{32} n q_n = \sum_{n=1}^{31} n (1-r) r^n + 32 r^{32}
$$

<a id="img_randao_tail"></a>
<figure class="chart" style="width:100%">

![Graph of the expected RANDAO tail.](images/charts/randao_tail.svg)

<figcaption>

The bottom axis is $r$, and the side axis is my expected proposals tail length $E(r)$ assuming no RANDAO manipulation.

</figcaption>
</figure>

Now we will calculate $E^{(k)}(r)$, the expected length of tail I can achieve in the next epoch by using my previous tail of length $k$ to grind the options.

Consider the case where I have a tail of length $k = 1$ in some epoch. This gives me two options: I can publish my RANDAO contribution, or I can withhold my RANDAO contribution (by withholding my block). My strategy is to choose the longest tail for the next epoch that I can gain via either of these options.

The probability, $p^{(1)}_j$, of gaining a tail of exactly length $j$ as a result of having a tail of length 1 is,

$$
p^{(1)}_j =  2\sum_{i=0}^{j-1}q_{j}q_{i} + q_{j}q_{j} = q_j \left( 2\sum_{i=0}^{j-1}q_i + q_j \right)
$$

We can think about this as follows. With $k = 1$ we get two attempts, therefore $q$ appears twice in each product. To calculate $p^{(1)}_j$ we need the sum over the all the combinations of the probability of getting a tail of length exactly $j$ (that is, $q_j$) multiplied by the probability of getting a tail of $j$ or less (that is, not getting a tail longer than $j$, otherwise we would have chosen that length instead of $j$).

Visually, calculating $p^{(1)}_2$ looks like the sum of the values in the shaded area of the next diagram.

<a id="img_randomness_tail_probabilities"></a>
<figure class="diagram" style="width:40%">

![Matrix of tail length probabilities.](images/diagrams/randomness-tail_probabilities.svg)

<figcaption>

The probability that we get a maximum tail length of exactly two with two attempts is the sum of the terms in the shaded areas. Despite the overlap, each term is included only once.

</figcaption>
</figure>

This example with tail length $k = 1$ results in a two-dimensional square since we have two possibilities to try. One way to calculate $p^{(1)}_j$ is to take the difference between the sum of all the products in the square side $j + 1$ and the sum of all the products in the square side $j$.

Thinking of it like this helps us to generalise to the cases when $k > 1$. In those cases we are dealing with a hyper-cube of dimension $2^k$; each element is the product of $2^k$ values of $q$. To calculate $p^{(k)}_j$ we can find the difference between the sum of all the products in the $2^k$-dimensional cube side $j + 1$ and the sum of all the products in the $2^k$-dimensional cube side $j$. This is tedious to write down and involves a mind-boggling number of calculations even for quite small $k$, but see my [example code](#tail-extension-code) for an efficient a way to calculate it.

Now, finally, we can calculate the expected tail length in the next epoch given that we have a tail of length $k$ in this epoch.

$$
E^{(k)}(r) = \sum_{n=1}^{32} n p^{(k)}_n
$$

Graphing this for various values of $k$ we get the following. Note that the solid, $k = 0$, line is the same as $E(r)$ above - the expected tail with no manipulation. That is, $E^{(0)}(r) = E(r)$ as you'd expect.

<a id="img_randao_extend_0"></a>
<figure class="chart" style="width:100%">

![Graph of the expected RANDAO tail.](images/charts/randao_extend_0.svg)

<figcaption>

The bottom axis is $r$, and the side axis is my subsequent expected proposals tail length, $E^{(k)}(r)$ given various values of tail length $k$ that I can play with. Note that $E^{(0)}(r) = E(r)$ from the graph above.

</figcaption>
</figure>

We see that, if I end up with any length of tail in an epoch, I can always grind my RANDAO contributions to improve my expected length of tail in the next epoch when compared with not grinding the RANDAO. And the longer the tail I have, the better the tail I can expect to have in the next epoch. These results are not surprising.

The important question is, under what circumstances can I use this ability in order to indefinitely increase my expected tail length, so that I can eventually gain full control of the RANDAO?

To investigate this, consider the following graph. Here, for each $k$ line we have plotted $E^{(k)}(r) - k$. This allows us to see whether our expected tail in the next epoch is greater or less than our current tail. If $E^{(k)}(r) - k$ is negative then I can expect to have fewer proposals in the next epoch than I have in this one.

<a id="img_randao_extend_1"></a>
<figure class="chart" style="width:100%">

![Graph of the expected change in RANDAO tail.](images/charts/randao_extend_1.svg)

<figcaption>

The bottom axis is $r$, and the side axis is my subsequent expected proposals tail length minus my current tail length, $E^{(k)}(r) - k$ for various values of $k$.

</figcaption>
</figure>

We can see that for $r$ less than around 0.5, especially as $k$ grows, we expect our tail length to shrink rather than grow, despite our best RANDAO grinding efforts. However, for $r$ greater than 0.5, we expect our tail length to grow as a result of our grinding, whatever tail length we start with.

For completeness, we shouldn't only look at expectations, but also at probabilities. The following graph shows the probability that if I have a tail of length $k$ then I will have a tail of length less than $k$ in the next epoch. As $k$ increases you can see that a step function is forming: for a proportion of stake less than about 50% it becomes practically certain that my tail will decrease in length from one epoch to the next despite my best efforts to grow it; conversely, for a proportion of stake greater than a little over 50% it becomes practically certain that I can maintain or grow my tail of block proposals.

<a id="img_randao_extend_2"></a>
<figure class="chart" style="width:100%">

![Graph of the probability that my tail will shrink.](images/charts/randao_extend_2.svg)

<figcaption>

The bottom axis is $r$, and the side axis is the probability that my best tail length in the next epoch is less than my current tail length for various values of tail length $k$.

</figcaption>
</figure>

###### Discussion of RANDAO takeover

What can we conclude from this? If I control less than about half the stake, then I cannot expect to be able to climb the ladder of increasing tail length: with high probability the length of tail I have will decrease rather than increase. Whereas, if I have more than half the stake, my expected length of tail increases each epoch, so I am likely to be able to eventually take over the RANDAO completely. With high enough $r$, the $2^k$ options I have for grinding the RANDAO overwhelm the probability of losing tail proposals. For large values of $k$ it will not be practical to grind through all these options. However, we need to arrive at only one good combination in order to succeed, so we might not need to do the full calculation.

The good news is that, if attackers control more than half the stake, they have more interesting attacks available, such as taking over the LMD fork choice rule. So we generally assume in the protocol that any attacker has less than half the stake, in which case the RANDAO takeover attack appears to be infeasible.

As a final observation, we have ignored cases where two or more of the tail proposals are from the same validator. As discussed [above](#randao_xor), these proposals would each result in the same RANDAO contribution and reduce my grinding options. However, with a large number of validators in the system this is a reasonable approximation to make.

<a id="tail-extension-code"></a>
<details>
<summary>Code for calculating the length of tail with cheating</summary>

Here is the code for generating the data for the graphs above. The length of tail goes up to $k = 12$. Feel free to increase that, although it gets quite compute intensive. Twelve is enough to see the general picture.

```python
def prob_tail_eq(r, k):
    return (1 - r) * r**k if k < N else r**k

# The sum of the products of all the q_i in the hypercube of side j and dim k
# Recursive is cooler, but written iteratively so that python doesn't run out of stack
def hyper(q, j, k):
    h = 1
    for n in range(1, k + 1):
        h = sum([q[i] * h for i in range(j)])
    return h

# Smoke test.
assert abs(hyper([0.9, 0.09, 0.009, 0.0009, 0.00009, 0.00001], 6, 32) - 1.0) < 1e-12

N    = 32 # The number of slots per epoch
KMAX = 12 # The maximum length of prior tail we will consider
NINT = 20 # The number of intervals of r between 0 and 1 to generate

expected = [[] for i in range(KMAX + 1)]
prob_dec = [[] for i in range(KMAX + 1)]
rs = [i / NINT for i in range(1, NINT)]
for r in rs:
    # q[j] = the probability of having a tail of exactly j in one attempt
    q = [prob_tail_eq(r, j) for j in range(N + 1)]
    for k in range(KMAX + 1):
        h = [hyper(q, j, 2**k) for j in range(N + 2)]
        # p[j] = the probability that with a tail of k I can achieve a tail of j in the next epoch
        p = [h[j + 1] - h[j] for j in range(N + 1)]
        # The expected length of tail in the next epoch given r and k
        expected[k].append(sum([j * p[j] for j in range(N + 1)]))
        # The probability of a decrease in tail length to < k
        prob_dec[k].append(h[k])
print(rs)
print(expected)
print(prob_dec)
```

</details>

##### Block proposals boost

For the second worked example I will try to improve the overall number of proposals that I get among my validators. Unlike in the first example, I will not be trying to maximise my advantage at any cost. I will only manipulate the RANDAO when I can do so without any net cost to myself.

Once again, I control a proportion $r$ of the stake. I will only be considering tails of length zero or of length one - going beyond that gets quite messy, and my intuition is that for values of $r$ less than a half or so it will make little difference.

Let $q_j$ be my probability of getting exactly $j$ proposals in an epoch without any manipulation of the RANDAO (different from the $q$ in the first example, but related):

$$
q_j = r^j{(1-r)}^{32-j}{32 \choose j}
$$

My expected number of proposals per epoch when acting honestly is simple to compute,

$$
E = \sum_{n=1}^{32} n q_n = 32 r
$$

Now I will try to bias the RANDAO to give myself more proposals whenever I have the last slot of an epoch, which will happen with probability $r$. Doing this, my expected number of proposals in the next epoch is as follows. The prime is to show that I am trying to maximise my advantage (cheat), and the subscript is to show that we are looking one epoch ahead.

$$
E'_1 = \sum_{n=1}^{32} n ((1 - r) q_n + r p_n)
$$

Unpacking this, the first term in the addition is the probability, $1 - r$, that I did not have the last slot in the previous epoch (so I cannot do any biasing) combined with the usual probability $q_n$ of having $n$ proposals in an epoch.

The second term is the probability, $r$, that I _did_ have the last slot in the previous epoch combined with the probability $p_n$ that I get either $n$ proposals by proposing my block, or $n + 1$ proposals by withholding my block. We need the plus one to make up for the block I would be withholding at the end of the previous epoch in order to get this outcome.

$$
p_j =
\begin{cases}
\sum_{i=0}^{j} q_i (q_j + q_{j+1}) & 0 \leq j < 32 \\
\sum_{i=0}^{j} q_i q_j            & j = 32
\end{cases}
$$

As before, we can illustrate this by considering the matrix of probabilities. With a tail of one I have two choices: to propose or to withhold. To achieve a net number of exactly $j$ proposals we are looking for the combinations where either of the following holds.

 1. Proposing gives me exactly $j$ proposals and withholding gives no more than $j+1$ (that is, $\sum_{i=0}^{j+1}q_{i}q_{j}$). These are the elements in the horizontal bar in the diagram below.
 2. Proposing gives me no more than $j$ proposals and withholding gives me exactly $j + 1$ (that is, $\sum_{i=0}^{j}q_{j+1}q_i$).[^fn-hyper-hurts-head] These are the elements in the vertical bar in the diagram below.

Note that the $q_{j+1}q_j$ element appears in both outcomes, but must be included only once.

[^fn-hyper-hurts-head]: You can see why I am restricting this example to tails of length just zero or one: I don't want to think about what this looks like in a $2^k$ dimensional space.

<a id="img_randomness_propose_probabilities"></a>
<figure class="diagram" style="width:40%">

![Matrix of proposal number probabilities.](images/diagrams/randomness-propose_probabilities.svg)

<figcaption>

The probability that we get a net number of exactly two proposals with two attempts is the sum of the terms in the shaded areas. Despite the overlap, each term is included only once.

</figcaption>
</figure>

We can iterate this epoch by epoch to calculate the maximum long-term improvement in my expected number of proposals. The probability that I gain the last slot of epoch $N$ is $E'_N / 32$.

$$
E'_{N+1} = \sum_{n=1}^{32} n \left((1 - \frac{E'_N}{32}) q_n + \frac{E'_N}{32} p_n\right)
$$

<a id="img_randao_proposals"></a>
<figure class="chart" style="width:100%">

![Graph showing the expected number of proposals per epoch when biasing and not biasing the RANDAO.](images/charts/randao_proposals.svg)

<figcaption>

The solid line is $E$, the expected number of block proposals per epoch for a proportion of the stake that does not seek to bias the RANDAO. The dashed line is $E'$, the long-term expected number of block proposals per epoch for a proportion of the stake that coordinates to bias the RANDAO in its favour.

</figcaption>
</figure>

The maximum percentage gain in block proposals that I can acquire is shown in the following graph.

<a id="img_randao_proposals_percent"></a>
<figure class="chart" style="width:100%">

![Graph showing the percentage increase in proposals per epoch when biasing the RANDAO.](images/charts/randao_proposals_percent.svg)

<figcaption>

The long-term percentage increase in the expected number of proposals per epoch that can be gained by a proportion of the stake coordinating to bias the RANDAO. An entity with 25% of the stake can gain an extra 2.99% of proposals (8.24 per epoch rather than exactly 8), assuming that the remaining stakers are uncoordinated.

</figcaption>
</figure>

<details>
<summary>Code for calculating the expected number of proposals with cheating</summary>

The following Python code calculates $E'_N$ to convergence.

```python
def fac(n):
    return n * fac(n - 1) if n else 1

def choose(n, k):
    return fac(n) / fac(k) / fac(n - k)

def prob(n, k, r):
    return r**k * (1 - r)**(n - k) * choose(n, k)

nintervals = 20
for idx in range(1, nintervals + 1):
    r = r0 = idx / nintervals
    q = [prob(32, j, r0) for j in range(33)]

    p = []
    for j in range(33):
        p.append(sum([q[i] * q[j] + (q[j + 1] * q[i] if (j < 32) else 0) for i in range(j + 1)]))

    # Iterate to convergence
    e = 0
    while (e == 0 or abs(e - e_old) > 0.000001):
        e_old = e
        e = sum([i * (q[i] * (1 - r) + p[i] * r) for i in range(33)])
        r = e / 32

    print(r0, r0 * 32, e, 100 * (e / (r0 * 32) - 1))
```

</details>

###### Discussion of proposals boost

In the above analysis we considered only the effect of using the last slot of an epoch to bias the RANDAO and saw that an entity with any amount of stake can fractionally improve its overall expected number of block proposals, assuming that everyone else is acting honestly.

The expected gain may be higher if we consider using the two last slots, or the $k$ last slots, especially if combined with the previous tail-extension attack. But I expect that for $r$ less than a half or so any further improvement will be very small.

#### Verifiable delay functions

We've seen that, although the RANDAO is biasable, it is not so biasable as to break the protocol: for our purposes the randomness is "good enough".

Nonetheless, it is interesting to explore how it might be improved, especially as, with The Merge, the RANDAO value is now available to Ethereum's smart contract layer. Randomness biasability in a large lottery contract, for example, could be more of a problem than biasability in the consensus protocol.

The long-term fix for biasability is to use a verifiable delay function (VDF). A VDF is guaranteed to be slow to compute its output, but that output can be verified quickly. In practice the VDF is a calculation run on a specialised hardware device that is assumed to have a performance within a small factor of the theoretical maximum performance. So, a VDF might output a result in, say, 20 seconds with the assumption that the best that any other device could do is to obtain the result in, say, 5 seconds.

The idea is that RANDAO updates would come from the output of the VDF. A proposer would have to decide whether to commit its `randao_reveal` before it is possible for it to compute the actual contribution: the future output of the VDF. This eliminates any opportunistic biasing of the RANDAO.

Only one VDF needs to be active at any time on the network since it can publish its result for quick verification by all the other nodes.

Although a [lot of work](https://www.vdfalliance.org/) has been done on designing and specifying VDFs there is no active plan to implement one in Ethereum at this time. If anything, recent results on the [difficulty of constructing VDF functions](https://ethresear.ch/t/statement-regarding-the-public-report-on-the-analysis-of-minroot/16670?u=benjaminion) suggest that the prospect of an in-protocol VDF is receding at this time.

#### See also

Vitalik has some notes on randomness in his [Annotated Ethereum 2.0 Spec](https://notes.ethereum.org/@vbuterin/SkeyEI3xv#Aside-RANDAO-seeds-and-committee-generation). His article [Validator Ordering and Randomness in PoS](https://web.archive.org/web/20160723105229/https://vitalik.ca/files/randomness.html) summarises some early thinking on the options for random validator selection in proof of stake[^fn-paddy-randomness].

[^fn-paddy-randomness]: This article seems only to be available now on the Internet Archive. I am grateful to Patrick McCorry for tracking it down.

On RANDAO biasability, Runtime Verification did an analysis in 2018 that both complements and goes deeper than the sketches I presented in this section. There is both a [statistical model](https://github.com/runtimeverification/rdao-smc) and a thorough [write-up](https://github.com/runtimeverification/rdao-smc/blob/master/report/rdao-analysis.pdf) of their work.

A [search for RANDAO](https://ethresear.ch/search?q=RANDAO) on ethresear.ch yields quite a few articles discussing various issues with it, and proposing some solutions (none of which we have adopted).

A good place to start exploring verifiable delay functions is the [VDF Alliance site](https://www.vdfalliance.org/).

### 混洗 <!-- /part2/building_blocks/shuffling/ -->

<div class="summary">

  - 混洗被用于随机分配验证者到委员会并选择区块提议者。
  - 以太坊 2 采用 “swap-or-not” 混洗算法。
  - “Swap-or-not”是一种不经意混洗：它可以应用于单个列表元素和子集。
  - 这使得它非常适合支持轻客户端。

</div>

#### 引言

混洗被用于将验证者随机分配到委员会，包括认证委员会和同步委员会。它还用于在每个时隙选择区块提议者。

虽然有一些需要注意的[陷阱](https://www.developer.com/tech/article.php/616221/How-We-Learned-to-Cheat-at-Online-Poker-A-Study-in-Software-Security.htm)，但混洗是计算机科学中一个非常成熟的问题。黄金标准可能是 [Fisher&ndash;Yates shuffle](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) 混洗算法。那么，为什么我们不在 Eth2 中使用它呢？简而言之：轻客户端。

其他混洗算法依赖于处理整个元素列表以找到最终的排序。我们希望减轻轻客户端的负担。理想情况下，他们应该只处理他们感兴趣的列表子集。因此，我们使用了一种被称作“swap-or-not”的混洗算法，而不是Fisher–Yates。“Swap-or-not” 混洗可以告诉你单个列表中元素的目的索引（或相反的源索引），因此非常适合用于处理整个验证者集的子集。

例如，在形式上，委员会的分配是通过对完整的验证者列表进行混洗，然后对由此产生的排列进行连续切分。如果我只需要知道委员会 $k$ 的成员，那么这种方式就非常低效。相反，我可以只对 $k$ 中的索引反向运行“swap-or-not”混洗，以找出整个验证者集中的哪些验证者会被混洗到 $k$ 中。这要高效得多。

#### Swap-or-not Specification

The algorithm for shuffling [in the specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/beacon-chain.md#compute_shuffled_index) deals with only a single index at a time.

```python
def compute_shuffled_index(index: uint64, index_count: uint64, seed: Bytes32) -> uint64:
    """
    Return the shuffled index corresponding to ``seed`` (and ``index_count``).
    """
    assert index < index_count

    # Swap or not (https://link.springer.com/content/pdf/10.1007%2F978-3-642-32009-5_1.pdf)
    # See the 'generalized domain' algorithm on page 3
    for current_round in range(SHUFFLE_ROUND_COUNT):
        pivot = bytes_to_uint64(hash(seed + uint_to_bytes(uint8(current_round)))[0:8]) % index_count
        flip = (pivot + index_count - index) % index_count
        position = max(index, flip)
        source = hash(
            seed
            + uint_to_bytes(uint8(current_round))
            + uint_to_bytes(uint32(position // 256))
        )
        byte = uint8(source[(position % 256) // 8])
        bit = (byte >> (position % 8)) % 2
        index = flip if bit else index

    return index
```

An index position in the list to be shuffled, `index`, is provided, along with the total number of indices, `index_count`, and a `seed` value. The output is the index that the initial index gets shuffled to.

The hash functions used to calculate `pivot` and `source` are deterministic, and are used to generate pseudo-random output from the inputs: given the same input, they will generate the same output. So we can see that, for given values of `index`, `index_count`, and `seed`, the routine will always return the same output.

The shuffling proceeds in rounds. In each round, a `pivot` index is pseudo-randomly chosen somewhere in the list, based only on the `seed` value and the round number.

Next, an index `flip` is found, which is `pivot - index`, after accounting for wrap-around due to the modulo function. The important points are that, given `pivot`, every `index` maps to a unique `flip`, and that the calculation is symmetrical, so that `flip` maps to `index`.

  - With `index_count = 100`, `pivot = 70`, `index = 45`, we get `flip = 25`.
  - With `index_count = 100`, `pivot = 70`, `index = 82`, we get `flip = 88`.

As the last step in the round, a decision is made whether to keep the index as-is, or to update it to `flip`. This decision is pseudo-randomly made based on the values of `seed`, the round number, and the higher of `index` and `flip`.

Note that basing the swap-or-not decision on the higher of `index` and `flip` brings a symmetry to the algorithm. Whether we are considering the element at `index` or the element at `flip`, the decision whether to swap the elements or not will be the same. This is the key to seeing the that full algorithm delivers a shuffling (permutation) of the original set.

The algorithm proceeds with the next iteration based on the updated index.

It may not be immediately obvious, but since we are deterministically calculating `flip` based only on the round number, the shuffle can be run in reverse simply by running from `SHUFFLE_ROUND_COUNT - 1` to `0`. The same swap-or-not decisions will be made in reverse. As described above, this reverse shuffle is perfect for finding which validators ended up in a particular committee.

#### A full shuffle

To get an intuition for how this single-index shuffle can deliver a full shuffling of a list of indices, we can consider how the algorithm is typically [implemented in clients](https://github.com/ConsenSys/teku/blob/04294427f2622c86326db68f3b88ed20d1e6cdc1/ethereum/spec/src/main/java/tech/pegasys/teku/spec/logic/common/helpers/MiscHelpers.java#L154) when shuffling an entire list at once.

As an optimisation, the loop over the indices to be shuffled is brought inside the loop over rounds. This hugely reduces the amount of hashing required since the pivot is fixed for the round (it does not depend on the index) and the bits of `source` can be reused for 256 consecutive indices, since the hash has a 256-bit output.

For each round, we do the following.

##### 1. Choose a pivot and find the first mirror index

First, we pick a pivot index $p$. This is pseudo-randomly chosen, based on the round number and some other seed data. The pivot is fixed for the rest of the round.

With this pivot, we then pick the mirror index $m_1$ halfway between $p$ and $0$. That is, $m_1 = p / 2$. (We will simplify by ignoring off-by-one rounding issues for the purposes of this explanation.)

<a id="img_shuffling_0"></a>
<figure class="diagram" style="width:80%">

![A diagram showing the pivot and the first mirror index.](images/diagrams/shuffling-0.svg)

<figcaption>

The pivot and the first mirror index.

</figcaption>
</figure>

##### 2. Traverse first mirror to pivot, swapping or not

For each index between the mirror index $m_1$ and the pivot index $p$, we decide whether we are going to swap the element or not.

Consider the element at index $i$. If we choose not to swap it, we just move on to consider the next index.

If we do decide to swap, then we exchange the list element at $i$ with that at $i'$, its image in the mirror index. That is, $i$ is swapped with $i' = m_1 - (i - m_1)$, so that $i$ and $i'$ are equidistant from $m_1$. In practice, we don't exchange the elements at this point, we just update the indices $i \rightarrow i'$, and $i' \rightarrow i$.

We make the same swap-or-not decision for each index between $m_1$ and $p$.

<a id="img_shuffling_1"></a>
<figure class="diagram" style="width:80%">

![A diagram showing swapping or not from the first mirror up to the pivot.](images/diagrams/shuffling-1.svg)

<figcaption>

Swapping or not from the first mirror up to the pivot.

</figcaption>
</figure>

The decision whether to swap or not is based on hashing together the random seed, the round number, and some position data. A single bit is extracted from this hash for each index, and the swap is made or not according to whether this bit is one or zero.

##### 3. Calculate the second mirror index

After considering all the indices $i$ from $m_1$ to $p$, mirroring in $m_1$, we now find a second mirror index at $m_2$, which is the point equidistant between $p$ and the end of the list: $m_2 = m_1 + n / 2$.

<a id="img_shuffling_2"></a>
<figure class="diagram" style="width:80%">

![A diagram showing the second mirror index.](images/diagrams/shuffling-2.svg)

<figcaption>

The second mirror index.

</figcaption>
</figure>

##### 4. Traverse pivot to second mirror, swapping or not

Finally, we repeat the swap-or-not process, considering all the points $j$ from the pivot, $p$ to the second mirror $m_2$. If we choose not to swap, we just move on. If we choose to swap then we exchange the element at $j$ with its image at $j'$ in the mirror index $m_2$. Here, $j' = m_2 + (m_2 - j)$.

<a id="img_shuffling_3"></a>
<figure class="diagram" style="width:80%">

![A diagram showing swapping or not from the pivot to the second mirror.](images/diagrams/shuffling-3.svg)

<figcaption>

Swapping or not from the pivot to the second mirror.

</figcaption>
</figure>

##### Putting it all together

At the end of the round, we have considered all the indices between $m_1$ and $m_2$, which, by construction, is half of the total indices. For each index considered, we have either left the element in place, or swapped the element at a distinct index in the other half. Thus, all the indices have been considered exactly once for swapping.

The next round begins by incrementing (or decrementing for a reverse shuffle) the round number, which gives us a new pivot index, and off we go again.

<a id="img_shuffling_4"></a>
<figure class="diagram" style="width:80%">

![A diagram showing the whole process running from one mirror to the other in a single round.](images/diagrams/shuffling-4.svg)

<figcaption>

The whole process running from one mirror to the other in a single round.

</figcaption>
</figure>

#### Discussion

##### A key insight

When deciding whether to swap or not for each index, the algorithm cleverly bases its decision on the higher of the candidate index or its image in the mirror. That is, $i$ rather than $i'$ (when below the pivot), and $j'$ rather than $j$ (when above the pivot). This means that we have flexibility when running through the indices of the list: we could do $0$ to $m_1$ and $p$ to $m_2$ as two separate loops, or do it with a single loop from $m_1$ to $m_2$ as I outlined above. The result will be the same: it doesn't matter if we are considering $i$ or its image $i'$; the decision whether to swap or not has the same outcome.

##### The number of rounds

In Ethereum&nbsp;2.0 we do 90 rounds of the algorithm per shuffle, set by the constant [`SHUFFLE_ROUND_COUNT`](/part3/config/preset/#misc). The [original paper](https://link.springer.com/content/pdf/10.1007%2F978-3-642-32009-5_1.pdf) on which this technique is based suggests that $6\lg{N}$ rounds is required "to start to see a good bound on CCA-security", where $N$ is the list length. In his [annotated spec](https://github.com/ethereum/annotated-spec/blob/master/phase0/beacon-chain.md) Vitalik says "Expert cryptographer advice told us ~$4\log_2{N}$ is sufficient for safety". The absolute maximum number of validators in Eth2, and hence the maximum size of the list we would ever need to shuffle, is about $2^{22}$ (4.2 million). On Vitalik's estimate that gives us 88 rounds required, on the paper's estimate, 92 rounds (assuming that $\lg$ is the natural logarithm). So we are in the right ballpark, especially as we are very, very unlikely to end up with that many active validators.

It might be interesting to make the number of rounds adaptive based on list length. But we don't do that; it's probably an optimisation too far.

Fun fact: when Least Authority audited the beacon chain specification, they initially found bias in the shuffling used for selecting block proposers (see Issue F [in their report](https://leastauthority.wpengine.com/static/publications/LeastAuthority-Ethereum-2.0-Specifications-Audit-Report.pdf)). This turned out to be due to mistakenly using a configuration that had only 10 rounds of shuffling. When they increased it to the 90 we use for mainnet, the bias no longer appeared.

##### (Pseudo) randomness

The algorithm requires that we select a pivot point randomly in each round, and randomly choose whether to swap each element or not in each round.

In Eth2, we deterministically generate the "randomness" from a seed value, such that the same seed will always generate the same shuffling.

The pivot index is generated from eight bytes of a SHA256 hash of the seed concatenated with the round number, so it usually changes each round.

The decision bits used to determine whether to swap elements are bits drawn from SHA256 hashes of the seed, the round number, and the index of the element within the list.

##### Efficiency

This shuffling algorithm is much slower than Fisher&ndash;Yates. That algorithm requires $N$ swaps. Our algorithm will require $90N/4$ swaps on average to shuffle $N$ elements.

We should also consider the generation of pseudo-randomness, which is the most expensive part of the algorithm. Fisher&ndash;Yates needs something like $N\log_2{N}$ bits of randomness, and we need $90(\log_2{N} + N/2)$ bits, which, for the range of $N$ we need in Eth2, is many more bits (about twice as many when $N$ is a million).

#### Why swap-or-not?

Why would we use such an inefficient implementation?

##### Shuffling single elements

The brilliance is that, if we are interested in only a few indices, we do not need to compute the shuffling of the whole list. In fact, we can apply the algorithm to a single index to find out which index it will be swapped with.

So, if we want to know where the element with index 217 gets shuffled to, we can run the algorithm with only that index; we do not need to shuffle the whole list. Moreover, if we want to know the converse, which element gets shuffled into index 217, we can just run the algorithm backwards for element 217 (backwards means running the round number from high to low rather than low to high).

In summary, we can compute the destination of element $i$ in $O(1)$ operations, and the source of element $i'$ (the inverse operation) also in $O(1)$, not dependent on the length of the list. Shuffles like the Fisher&ndash;Yates shuffle do not have this property and cannot work with single indices, they always need to iterate the whole list. The technical term for a shuffle having this property is that it is _oblivious_ (to all the other elements in the list).

##### Keeping light clients light

This property is important for light clients. Light clients are observers of the Eth2 beacon chain that do not store the entire state, but do wish to be able to securely access the chain's data. As part of verifying that they have the correct data &ndash; that no-one has lied to them &ndash; it is necessary to compute the committees that attested to that data. This means shuffling, and we don't want light clients to have to hold and shuffle the entire list of validators. By using the swap-or-not shuffle, light clients need only to consider the small subset of validators that they are interested in, which is vastly more efficient overall.

#### See also

  - The initial discussion about the search for a good shuffling algorithm is [Issue 323](https://github.com/ethereum/consensus-specs/issues/323) on the specs repo.
  - The winning algorithm was announced in [Issue 563](https://github.com/ethereum/consensus-specs/issues/563).
  - The original paper describing the swap-or-not shuffle is Hoang, Morris, and Rogaway, 2012, ["An Enciphering Scheme Based on a Card Shuffle"](https://link.springer.com/content/pdf/10.1007%2F978-3-642-32009-5_1.pdf). See the "generalized domain" algorithm on page 3.

### 委员会 <!-- /part2/building_blocks/committees/ -->

<div class="summary">

  - Committees are subsets of the full set of active validators that are used to distribute the overall workload.
  - Beacon committees manage attestations for the consensus protocol; sync committees are discussed [elsewhere](/part2/building_blocks/sync_committees/).
  - Having 64 beacon committees at each slot is a relic of previous Eth2 designs.
  - Nonetheless, multiple committees per slot allow us to parallelise attestation aggregation.
  - Beacon committee membership is random and transient.
  - A target minimum committee size of 128 protects them against capture.

</div>

#### Introduction

One of the challenges of building a highly scalable consensus protocol is organising the work involved so as not to overwhelm the network or individual nodes.

A goal of the Ethereum&nbsp;2 proof of stake protocol is to achieve economic finality. In the current design (though see [below](#see-also) for discussion of single slot finality) this requires us to gather votes from at least two-thirds of the validator set, and we must do this twice: once to justify an epoch, and once again to finalise it.

If the whole validator set were to attest simultaneously, the number of messages on the network would be immense, and the amount of work required of beacon nodes too much for modest hardware. This is where committees help. The work of attesting is divided among subsets of the validator set (committees) and spread across an epoch (6.4 minutes). Each validator participates in only one of the committees.

The Altair spec introduced two types of committees, beacon committees and sync committees, each having quite a different function. We will focus on beacon committees in this section, and deal with sync committees in a [later section](/part2/building_blocks/sync_committees/).

The current beacon committee structure was strongly influenced by a previous roadmap that included in-protocol data sharding. That design is [now deprecated](https://github.com/ethereum/consensus-specs/pull/1428), yet a remnant of it remains in our 64 beacon committees per slot. These were originally intended to map directly to 64 shards as "crosslink committees" but no longer have that function. Nonetheless, beacon committees still serve a useful purpose in parallelising the aggregation of attestations. Whether 64 remains the right number of committees per slot has not been analysed to my knowledge. The trade-off is that fewer beacon committees would reduce the amount of block space needed for aggregate attestations, but would increase the time needed for [aggregators](/part2/building_blocks/aggregator/) to do their work.

In any case, logically, all of the committees in a slot now act as a single large super-committee, all voting on the same information.

#### Committee assignments

Beacon committees are convened to vote exactly once and then disbanded immediately - they are completely transient. By contrast, a sync committee lasts for 256 epochs (a little over 27 hours), and votes 8192 times during that period.

During an epoch, every active validator is a member of exactly one committee, so all the epoch's committees are disjoint. At the start of the next epoch, all the existing committees are disbanded and the active validator set is divided into a fresh set of committees.

The composition of the committees for an epoch is fully determined at the start of an epoch by (1) the active validator set for that epoch, and (2) the [RANDAO seed](/part2/building_blocks/randomness/#lookahead) value at the start of the previous epoch.

<a id="img_committees_random"></a>
<figure class="diagram" style="width: 80%">

![Diagram showing circles and triangles randomly divided into committees.](images/diagrams/committees-random.svg)

<figcaption>

Here we have divided thirty circles and fifteen triangles into five committees at random. The attacking triangles do not have a majority in any committee.

</figcaption>
</figure>

We assign validators to committees randomly in order to defend against a minority attacker being able to capture any single committee. If committee assignments were not random, or were calculable long in advance, then it might be possible for an attacker with a minority of validators to organise them so that they became a supermajority in some committees. They might do this by manipulating the entries and exits of their validators, for example.

<a id="img_committees_organised"></a>
<figure class="diagram" style="width: 80%">

![Diagram showing circles and triangles divided into committees under the influence of an attacker.](images/diagrams/committees-organised.svg)

<figcaption>

It would be improbable for the triangles to gain a 2/3 supermajority in a committee purely by chance. But if the attacker could manipulate the assignments then they might gain a supermajority in some committees, such as the first two here.

</figcaption>
</figure>

The committee sizes used in the Eth2 protocol were chosen to make the takeover of a committee by a minority attacker extremely unlikely. See [target committee size](#target-committee-size), below, for further analysis of this.

#### The number of committees

The protocol adjusts the total number of committees in each epoch according to the number of active validators. The goals are,

1. to have the same number of committees per slot throughout the epoch (so the number of committees in an epoch is always a multiple of `SLOTS_PER_EPOCH`),
2. to have the largest number of committees that ensures that each committee has at least `TARGET_COMMITTEE_SIZE` members, and
3. to have a maximum of `MAX_COMMITTEES_PER_SLOT` committees per slot.

Clearly, the first goal is not achievable if there are fewer than `SLOTS_PER_EPOCH` validators &ndash; is a committee a committee if nobody is in it? &ndash; and the second goal is not achievable if there are fewer than `SLOTS_PER_EPOCH` `*` `TARGET_COMMITTEE_SIZE` (4096) validators. The protocol could hardly be considered secure with fewer than 4096 validators, so this is not a significant issue in practice.

<a id="img_committees_all"></a>
<figure class="diagram" style="width: 90%">

![A diagram showing N committees at each slot and 32 slots per epoch.](images/diagrams/committees-all.svg)

<figcaption>

Every slot in an epoch has the same number of committees, $N$, up to a maximum of `MAX_COMMITTEES_PER_SLOT`. Every active validator in the epoch appears in exactly one committee, so the committees are all disjoint.

</figcaption>
</figure>

The number of committees per slot is calculated by the spec function [`get_committee_count_per_slot()`](/part3/helper/accessors/#get_committee_count_per_slot). This can be simplified for illustrative purposes, given the number $n$ of active validators in the epoch, as

```code
MAX_COMMITTEES_PER_SLOT = 64
SLOTS_PER_EPOCH = 32
TARGET_COMMITTEE_SIZE = 128
def committees_per_slot(n):
    return max(1, min(MAX_COMMITTEES_PER_SLOT, n // SLOTS_PER_EPOCH // TARGET_COMMITTEE_SIZE))
```

This generates a committee structure that evolves as per the following table as the number of validators grows or shrinks.

| $n$ min | $n$ max | Committees / slot | Members per committee | Min | Max |
| - | - | -- | ----- | - | - |
| $0$ | $31$ | $1$ | Some committees have zero members | 0 | 1 |
| $32$ | $4095$ | $1$ | ${\lceil n / 32 \rceil}$ or ${\lfloor n / 32 \rfloor}$, which is below `TARGET_COMMITTEE_SIZE` | 1 | 128 |
| $4096$ | $262\,143$ | ${N = \lfloor n / 4096 \rfloor}$ | ${\lceil n / (32N) \rceil}$ or ${\lfloor n / (32N) \rfloor}$ | 128 | 256 |
| $262\,144$ | $4\,194\,304$ | 64 | ${\lceil n / 2048 \rceil}$ or ${\lfloor n / 2048 \rfloor}$ | 128 | 2048 |
| $4\,194\,305$ | - | 64 | [Things break](https://consensys.net/blog/news/formal-verification-of-ethereum-2-0-part-1-fixing-the-array-out-of-bound-runtime-error/) Note that this can [never happen](/part3/config/preset/#max_validators_per_committee) in practice. | - | - |

The numbers at the various thresholds in this table are calculated from the spec constants:

  - 32 is [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch).
  - 4096 is `SLOTS_PER_EPOCH` `*` [`TARGET_COMMITTEE_SIZE`](/part3/config/preset/#target_committee_size). This is the point at which all the committees achieve their target minimum size.
  - 262,144 is `SLOTS_PER_EPOCH` `*` `TARGET_COMMITTEE_SIZE` `*` [`MAX_COMMITTEES_PER_SLOT`](/part3/config/preset/#max_committees_per_slot). We have reached the maximum number of committees per slot (64). We no longer add new committees as the validator set grows, we just make the committees larger.
  - 4,194,304 is `SLOTS_PER_EPOCH` `*` [`MAX_VALIDATORS_PER_COMMITTEE`](/part3/config/preset/#max_validators_per_committee) `*` `MAX_COMMITTEES_PER_SLOT`. There is not enough Ether in existence to allow us to reach this number of active validators. The limit exists in protocol to enable us to specify a maximum size for the [`aggregation_bits`](/part3/containers/operations/#attestation) SSZ [`Bitlist`](/part2/building_blocks/ssz/#bitlists) type in attestations.

##### Committee index

Each of the $N$ committees within a slot has a committee index from $0$ to $N-1$. I will call this $i$ in what follows and refer to it as the slot-based index. This [slot-based index](/part3/config/types/#committeeindex) is included in committees' attestations via the [`AttestationData`](/part3/containers/dependencies/#attestationdata) object,

```code
class AttestationData(Container):
    slot: Slot
    index: CommitteeIndex
    # LMD GHOST vote
    beacon_block_root: Root
    # FFG vote
    source: Checkpoint
    target: Checkpoint
```

The `slot` and the committee `index` within that slot together uniquely identify a committee, and together with the RANDAO value, its membership.

Since all committees in a slot are voting on exactly the same information (source, target, and head block), the `index` is the only thing that varies between the aggregate attestations produced by the slot's committees (assuming that most of the validators have the same view of the network). This prevents the attestations from the slot's committees being aggregated further, so we will generally end up with $N$ aggregate attestations per slot that we must store in a beacon block.

If it were not for the `index` then all these $N$ aggregate attestations could be further aggregated into a single aggregate attestation, combining the votes from all the validators voting at that slot.

As a thought experiment we can calculate the potential space savings of doing this. Given a committee size of $k$ and $N$ committees per slot, the current space required for $N$ aggregate `Attestation` objects is $N * (229 + \lfloor k / 8 \rfloor)$ bytes. If we could remove the committee index from the signed data and combine all of these into a single aggregate `Attestation` the space required would be $221 + \lfloor kN / 8 \rfloor$ bytes. So we could save $229N - 221$ bytes per block, which is 14.4&nbsp;KB with the maximum 64 committees. This seems nice to have, but would likely make the [committee aggregation process](/part2/building_blocks/aggregator/) more complex.

There is another index that appears when assigning validators to committees in [`compute_committee()`](/part3/helper/misc/#compute_committee): an epoch-based committee index that I shall call $j$. The indices $i$ and $j$ are related as $i = \mod(j, N)$ and $j = Ns + i$  where $s$ is the slot number in the epoch.

#### The size of committees

Validators are divided among the committees in an epoch by the [`compute_committee()`](/part3/helper/misc/#compute_committee) function.

Given the epoch-based index $j$, `compute_committee()` returns a slice of the full, shuffled validator set as the committee membership. Within the shuffled list, the index of the first validator in the committee is $\lfloor nj / 32N \rfloor$, and the index of the last validator in the committee is $\lfloor n(j + 1) / 32N \rfloor - 1$. So the size of each committee is either $\lfloor n / 32N \rfloor$ or $\lceil n / 32N \rceil$. In any case, the committee sizes within an epoch differ by at most one.

In simplified form the [`compute_committee()`](/part3/helper/misc/#compute_committee) calculation looks like this. `N` is the number of committees per slot, `n` is the total number of active validators, and `j` is the epoch-based committee index,

```code
def compute_committee_size(n, j, N):
    start = n * j // (32 * N)
    end = n * (j + 1) // (32 * N)
    return end - start
```

The length of the vector returned will be either `n // (32 * N)` or `1 + n // (32 * N)`. The function [`compute_shuffled_index()`](/part3/helper/misc/#compute_shuffled_index) is described in the [previous section](/part2/building_blocks/shuffling/).

<a id="img_committees_selection"></a>
<figure class="diagram" style="width: 95%">

![A diagram showing how the validator set is sliced up into committees.](images/diagrams/committees-selection.svg)

<figcaption>

Conceptually, to calculate the committee assignments for an epoch, the entire active validator set is shuffled into a list of length $n$, then sliced into $32N$ committees of as close to the same size as possible. $N$ is the number of committees per slot. The epoch-based committee number, $j$, is shown.

</figcaption>
</figure>

In the caption to the diagram above I said that this is "conceptually" how committee membership is determined. In practice, due to our use of an [oblivious shuffle](/part2/building_blocks/shuffling/), the membership of an individual committee can be calculated without shuffling the entire validator set; the result will be the same.

##### Target committee size

To achieve a desirable level of security, committees need to be larger than a certain size. This makes it infeasible for an attacker to randomly end up with a super-majority in a committee even if they control a significant number of validators. The target here is a kind of lower-bound on committee size. If there are not enough validators for all committees to have at least `TARGET_COMMITTEE_SIZE` (128) members, then, as a first measure, the number of committees per slot is reduced to maintain this minimum. Only if there are fewer than `SLOTS_PER_EPOCH` `*` `TARGET_COMMITTEE_SIZE` (4096) validators in total will the committee size be reduced below `TARGET_COMMITTEE_SIZE`. With so few validators the system would be insecure in any case.

Given a proportion of the validator set controlled by an attacker, what is the probability that the attacker ends up controlling a two-thirds majority in a uniformly randomly selected committee drawn from the full set of validators? Vitalik [calculated 111](https://web.archive.org/web/20190504131341/https://vitalik.ca/files/Ithaca201807_Sharding.pdf) to be the minimum committee size required to maintain a $2^{-40}$ chance (one-in-a-trillion) of an attacker with one third of the validators gaining by chance a two-thirds majority in any one committee. The value 128 was chosen as being the next higher power of two.

If an attacker has a proportion $p$ of the validator set, then the probability of selecting a committee of $n$ validators that has $k$ or more validators belonging to the attacker is,

$$
\sum_{i=k}^{n} p^i{(1-p)}^{n-i}{n\choose i}
$$

Using this we can calculate that, in fact, 109 members is sufficient to give only a $2^{-40}$ chance of an attacker with one third of the validators gaining a two-thirds majority by chance.

<a id="target-committee-size-code"></a>
<details>
<summary>Code for calculating the target committee size</summary>

The following is Vitalik's Python code for calculating the probabilities.

```code
def fac(n):
    return n * fac(n-1) if n else 1

def choose(n, k):
    return fac(n) / fac(k) / fac(n-k)

def prob(n, k, p):
    return p**k * (1-p)**(n-k) * choose(n,k)

def probge(n, k, p):
    return sum([prob(n,i,p) for i in range(k,n+1)])
```

Armed with this we find that the minimum committee size to avoid a two-thirds majority with a $2^{-40}$ probability is 109 rather than 111.

```code
>>> probge(108, 72, 1.0 / 3) < 2**-40
False
>>> probge(109, 73, 1.0 / 3) < 2**-40
True
```

In any case, a committee size of 128 is very safe against an attacker with 1/3 of the stake:

```code
>>> probge(128, 86, 1.0 / 3)
5.551560731791749e-15
```

</details>

Odds of one-in-trillion may sound like over-engineering, but we must also consider that an attacker might gain some [power over](/part2/building_blocks/randomness/#randao-biasability) the RANDAO, so some safety margin is desirable.

Notwithstanding all of this, in the current beacon chain design the minimum target committee size is irrelevant as committees never operate alone. As long as we have at least 8192 active validators, each slot has multiple committees all operating together, and it is their aggregate size that confers security, not the size of any individual committee. As previously mentioned, the current committee design is influenced by an old data sharding model that is now superseded. Nonetheless, individual committees might find a role in future versions of the protocol, so the minimum target size is worth preserving.

#### See also

In his survey article, [Paths toward single-slot finality](https://notes.ethereum.org/@vbuterin/single_slot_finality), Vitalik considers what it would take to introduce a single "super-committee" at each slot to replace the existing beacon committees. The super-committee would be a large enough subset of the whole validator set to achieve a satisfactorily secure level of finality within a single (extended, 16 second or longer) slot.

### Aggregator Selection <!-- /part2/building_blocks/aggregator/ -->

<div class="summary">

  - In each committee, a subset of validators is selected to perform aggregation of the committee's messages. This improves scaling.
  - Selection of aggregators is probabilistic based on BLS signatures.
  - This selection method preserves both secrecy and easy verifiability of the identity of the aggregators.

</div>

#### Introduction

In both [beacon committees](/part2/building_blocks/committees/) and [sync committees](/part2/building_blocks/sync_committees/) validators create and sign their own votes (`Attestation`s and `SyncCommitteeMessage`s respectively). These votes must be [aggregated](/part2/building_blocks/signatures/#aggregation) into a much smaller number of aggregate signed votes, ideally into a single aggregate signature over a single vote, before being included in beacon blocks.

The goals of aggregation are three-fold: to reduce the signature verification load on the next block proposer, to reduce the network load on the global gossip channel, and to reduce the amount of block space required to store the signatures.

In the current beacon chain design, voting is done in committees with the goal of getting a majority of committee members to sign off on the same vote, although in practice there might be a number of different votes depending on the network views of the individual committee members. In any case, members of different committees are signing different data that cannot be aggregated across committees.

The process of aggregation is as follows:

1. Committee members sign their votes ([`Attestation`](/part3/containers/operations/#attestation)s or [`SyncCommitteeMessage`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/validator.md#synccommitteemessage)s depending on which type of committee we are considering) and broadcast them to a peer-to-peer subnet that the whole committee is subscribed to.
2. A subset of the committee is selected to be aggregators for that committee.
3. The aggregators listen on the subnet for votes, then aggregate all the votes they receive that agree with their own view of the network into a single aggregate vote (aggregate [`Attestation`](/part3/containers/operations/#attestation) or [`SyncCommitteeContribution`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/validator.md#synccommitteecontribution)).
4. Each aggregator wraps its aggregate vote with a proof that it was indeed an aggregator for that committee, and it signs the resulting data ([`SignedAggregateAndProof`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#signedaggregateandproof) or [`SignedContributionAndProof`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/validator.md#signedcontributionandproof))
5. Finally, the aggregator broadcasts its aggregated vote and proof to a global channel to be received by the next block proposer.

This section is concerned with steps 2 and 4: how the aggregators are selected for duty, and how they prove that they were indeed selected.

<a id="img_aggregators"></a>
<figure class="diagram" style="width: 80%">

![A diagram of the workflow of aggregating attestations from beacon committees.](images/diagrams/aggregators.svg)

<figcaption>

Within a beacon committee, all members send their individual attestations to a gossip subnet. Aggregators are a chosen subset of the committee who listen to the subnet and aggregate the attestations they receive. The aggregators broadcast their aggregates to the global channel for the next block proposer to pick up.

</figcaption>
</figure>

#### Aggregator selection desiderata

Aggregator selection has been designed with three properties in mind.

First, the size of the resulting aggregator set. With very high probability we want a small, non-empty subset of the committee to be selected in order that we have a very high chance of selecting at least one honest, well-connected aggregator. It doesn't matter too much if our set of aggregators is slightly on the large side, but we really want to avoid having no aggregators at all. Bearing in mind that there's a chance of validators being down or malicious, selecting only one or two aggregators is also risky.

Second, secrecy. We'd prefer that nobody be able to calculate who the aggregators are until after they have broadcast their aggregations. This helps to avoid denial of service (DoS) attacks. Disrupting consensus would be much simpler via a network DoS attack against a small number of aggregators than against a whole committee. The secrecy property prevents this.

Third, verifiability. We want it to be easy to verify a claim that a particular validator was selected to be an aggregator. The rationale for this is [explained in the p2p spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/p2p-interface.md#why-are-aggregate-attestations-broadcast-to-the-global-topic-as-aggregateandproofs-rather-than-just-as-attestations). Basically, without verifiability it would be a good strategy for _all_ the validators in the committee to make and broadcast aggregate attestations to ensure that at least one aggregate includes their own attestation. This would destroy the benefits of the whole aggregator scheme.

#### Aggregator selection details

The current aggregation strategy was introduced in [PR 1440](https://github.com/ethereum/consensus-specs/pull/1440) and is described in the Honest Validator specs for [beacon committees](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#attestation-aggregation) and [sync committees](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/validator.md#aggregation-selection).

It turns out that we can straightforwardly satisfy our three desirable properties of size, secrecy, and verifiability using [BLS signatures](/part2/building_blocks/signatures/). The algorithm is quite simple. Each validator in the committee generates a verifiable random number; if that random number modulo another number is zero then it is an aggregator, otherwise it is not an aggregator.

The validator creates its verifiable random number by making a signature over the current slot number using its normal secret signing key, and then hashing the signature. We assume that the result of this is uniformly random; we have no reason to suspect it isn't.

Any validator whose random number modulo `len(committee)` `//` `TARGET_AGGREGATORS_PER_COMMITTEE` equals zero is then an aggregator. This modulus is chosen to provide an average of 16 aggregators per beacon committee.

The following are the [spec functions](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#aggregation-selection) for determining which validators are the aggregators in beacon committees.

<a id="def_get_slot_signature"></a>

```python
def get_slot_signature(state: BeaconState, slot: Slot, privkey: int) -> BLSSignature:
    domain = get_domain(state, DOMAIN_SELECTION_PROOF, compute_epoch_at_slot(slot))
    signing_root = compute_signing_root(slot, domain)
    return bls.Sign(privkey, signing_root)
```

<a id="def_is_aggregator"></a>

```python
def is_aggregator(state: BeaconState, slot: Slot, index: CommitteeIndex, slot_signature: BLSSignature) -> bool:
    committee = get_beacon_committee(state, slot, index)
    modulo = max(1, len(committee) // TARGET_AGGREGATORS_PER_COMMITTEE)
    return bytes_to_uint64(hash(slot_signature)[0:8]) % modulo == 0
```

This approach provides secrecy since it relies on the validator's secret key: no-one else can determine whether or not I am an aggregator until after I have published the proof. And it provides verifiability since, once the proof is published, it is easy to check the validity of the signature using the validator's public key.

What about the size criterion?

##### Beacon committee aggregators

Assuming that BLS signatures are uniformly random, then in a committee of size $N$ each validator will have a probability of being selected of `TARGET_AGGREGATORS_PER_COMMITTEE` `/` $N$ (ignoring the integer arithmetic). So in expectation we will have `TARGET_AGGREGATORS_PER_COMMITTEE` (16) aggregators per committee.

The probability of having zero aggregators is ${(1 - \frac{16}{N})}^N$. For the minimum target committee size of $N = 128$ this is 1 in 26 million, and for the maximum committee size of $N = 2048$, 1 in 9.5 million. So we would expect to see a beacon committee with no aggregators about once every 13,000 epochs (8 weeks) in the former case and once every 5000 epochs (3 weeks) in the latter. Each committee comprises only a fraction $1/2048$ of the total validator set, so occasionally having no aggregator is insignificant for the protocol, but it is unfortunate for those in that committee who will most likely not have their attestations included in a block as a result.

<a id="img_committee_aggregators"></a>
<figure class="chart">

![A bar chart showing the probability of different numbers of aggregators in a committee of 256.](images/charts/committee_aggregators.svg)

<figcaption>

The probability of having $k$ aggregators in a beacon committee of size 256. The expected number is 16.

</figcaption>
</figure>

##### Sync committee aggregators

Sync committees operate similarly. Each committee has 512 members that are divided across four independent subnets. The target is to have 16 aggregators per subnet as above, with the aggregators changing in each slot.

The `TARGET_AGGREGATORS_PER_SYNC_SUBCOMMITTEE` value was [increased from 4 to 16](https://github.com/ethereum/consensus-specs/pull/2514) ahead of the implementation of sync committees. This was based on an [analysis](https://docs.google.com/spreadsheets/d/1C7pBqEWJgzk3_jesLkqJoDTnjZOODnGTOJUrxUMdxMA/edit#gid=1790975994) showing that, by targeting only four aggregators, there would be an unacceptably high chance of having no aggregators on a sync committee subnet.

#### Incentivisation

Aggregators are not directly incentivised by the protocol: there are no explicit rewards or penalties for performing or not performing aggregation duties.

However, there are implicit incentives. For one, if I produce a high quality aggregate signature it helps to ensure that my own signature is included in a block (there's a chance that someone else's aggregate may not include my signature). For another, since overall attestation rewards [scale in proportion to participation](/part2/incentives/rewards/#rewards-scale-with-participation) (inclusion of attestations in blocks), aggregators benefit alongside all the other validators from slightly higher rewards when they make high quality aggregates that include many votes.

#### See also

Hsiao-Wei Wang has documented the [original research](https://notes.ethereum.org/@hww/aggregation) around aggregator selection.

This aggregation strategy presents a difficulty for building distributed validator technology (DVT). One approach to implementing DVT is for the multiple validators representing a single validator to operate independently, alongside a middleware that combines their signed attestations. This works because BLS signatures are additive: each validator has part of the key, and the signed attestations can be combined with a [threshold signature](/part2/building_blocks/signatures/#threshold-signatures) scheme into a signature from the full key. However, the process of hashing the (combined) signature can't be done in a distributed way, so it is difficult for the individual validators to determine whether the collective validator has been selected to be an aggregator or not. Oisín Kyne's [ethresear.ch article](https://ethresear.ch/t/distributed-validator-middlewares-and-the-aggregation-duty/13044?u=benjaminion) explores this problem and suggests a solution, which appears (slightly modified) in the [proposed addition](https://github.com/ethereum/beacon-APIs/pull/224) of two endpoints to the Beacon API spec.

[TODO: link to DVT when done]::

### SSZ: Simple Serialize <!-- /part2/building_blocks/ssz/ -->

<div class="summary">

  - The beacon chain uses a novel serialisation method called Simple Serialize (SSZ).
  - After much debate we chose to use SSZ for both consensus and communication.
  - SSZ is not self-describing; you need to know in advance what you are deserialising.
  - An offset scheme allows fast access to subsets of the data.
  - SSZ plays nicely with Merkleization and generalised indices in Merkle proofs.

</div>

#### Introduction

[Serialisation](https://en.wikipedia.org/wiki/Serialization) is the process of taking structured information (in our case, a data structure) and transforming it into a representation that can be stored or transmitted.

A cooking recipe is a kind of serialisation. I can write down a method for cooking something in such a way that you and others can recreate the method to cook the same thing. The recipe can be written in a book, appear online, even be spoken and memorised &ndash; this is serialisation. Using the recipe to cook something is deserialisation.

Serialisation is used for three main purposes on the beacon chain.

1. Consensus: if you and I each have information in a data structure, such as the beacon state, how can we know if our data structures are the same or not? Serialisation allows us to answer this question, as long as all clients use the same method. Note that this is also bound up with [Merkleization](/part2/building_blocks/merkleization/).
2. Peer-to-peer communication: we need to exchange data structures over the Internet, such as attestations and blocks. We can't transmit structured data as-is, it must be serialised for transmission and deserialised at the other end. All clients must use the same p2p serialisation, but it doesn't need to be the same as the consensus serialisation.
3. Similarly, data structures need to be serialised for users accessing a beacon node's API. Clients are free to choose their own API serialisation. For example, the Prysm client has [an API](https://docs.prylabs.network/docs/how-prysm-works/prysm-public-api/) that uses [Protocol Buffers](https://developers.google.com/protocol-buffers) (which is being deprecated now that we have agreed a [common API format](https://github.com/ethereum/beacon-APIs) that uses both SSZ and JSON).

In addition, data must be serialised before being written to disk. Each client is free to do this internally however they wish.

Ethereum&nbsp;2.0 uses a bespoke serialisation scheme called Simple Serialize, or more commonly just "SSZ"[^fn-ssz-z], for all of these purposes.

[^fn-ssz-z]: Thus enshrining that ugly "z" in the full name, and the [ghastly](/preface/#british-english) "ess-ess-zee" pronunciation.

#### History

It seems like we spent months over the end of 2018 and the start of 2019 talking about serialisation, and the story below is highly simplified. But I think it's worth recording some of the considerations and design decisions.

Ethereum&nbsp;1 has always used a serialisation format called [RLP](https://eth.wiki/fundamentals/rlp) (recursive length prefix). This was deemed unsuitable for Ethereum&nbsp;2, largely because it is regarded as [overly complex](https://ethereum.org/en/developers/docs/networking-layer/#ssz-vs-rlp).[^fn-rlp-complexity]

[TODO - https://web.archive.org/web/20220528042454/https://eth.wiki/en/concepts/wishlist#rlp is a better link, but archive.org seems very slow right now. Need to revisit this.]::

[^fn-rlp-complexity]: [Vitalik](https://github.com/ethereum/consensus-specs/issues/692#issuecomment-467684205), "As the inventor of RLP, I'm inclined to prefer SSZ", and [again](https://ethresear.ch/t/replacing-ssz-with-rlp-zip-and-sha256/5706/12?u=benjaminion), "RLP honestly sucks" (with some explanation as to why!).

So, we had the freedom to choose a new serialisation protocol. What kind of decision points did we consider?

##### Serialisation for consensus

Starting with serialisation in the consensus protocol, the first big question was whether to adopt an existing off-the-shelf protocol or to roll our own.

One major issue with many [existing schemes](https://notes.ethereum.org/15_FcGc0Rq-GuxaBV5SP2Q?view) is that they do not guarantee that the serialisation is deterministic: they sometimes re-order fields in unpredictable ways. This makes them totally unsuitable for consensus; the same data must result in the same output every time.

A more general concern was around using third-party libraries in a consensus-critical situation. Back in 2014, Vitalik wrote a justification, titled [Why not use X?](https://blog.ethereum.org/2014/02/09/why-not-just-use-x-an-instructive-example-from-bitcoin/), of Ethereum implementing its own technology (such as RLP) for so many things. Here's an excerpt:

> One of our core principles in Ethereum is simplicity; the protocol should be as simple as possible, and the protocol should not contain any black boxes. Every single feature of every single sub-protocol should be precisely 100% documented on the whitepaper or wiki, and implemented using that as a specification.

Certainly, with respect to serialisation, some third-party libraries are far more generic than we need, which can lead to issues. Others don't map nicely to the data types that we want to use.

In view of these concerns, momentum was in favour of adopting a bespoke, tightly specified serialisation method. It was the development of [Merkleization](/part2/building_blocks/merkleization/) on top of SSZ that cemented this, making SSZ (in some form) the clear leader for consensus serialisation.

##### Serialisation for communications

That decision made, the next big question was whether to use the same scheme for both consensus serialisation and peer-to-peer communications serialisation (the "wire-protocol"). This was finely balanced, and [good arguments](https://github.com/ethereum/consensus-specs/issues/129) were made in favour of using Protocol Buffers for p2p communication and SSZ for consensus.

Discussion around this was extensive (see the references [below](#see-also)), but we eventually [decided](https://github.com/ethereum/eth2.0-pm/blob/master/eth2.0-implementers-calls/call_003.md#tentative-decisions) to use SSZ for p2p communications.

The factors that tipped the balance in favour of SSZ for communications were (1) a desire to maintain only one serialisation library, and (2) some possible performance benefit.

On the first of these, there is a bias in Ethereum&nbsp;2 to [favour](https://github.com/ethereum/consensus-specs/issues/692#issuecomment-467684205) "simplicity over efficiency". Maintaining two serialisation libraries is arguably more overhead than any potential gain from using different ones. Having said that, RLP is [still used](https://github.com/ethresearch/p2p/issues/15) in Eth2's discovery layer (since it is shared with Eth1), so this argument loses some of its force.

On the second, when we receive an object over the wire, often the first thing we will want to do is to serialise it to calculate its data root for consensus. If we receive it already serialised in the right format then it saves a deserialise/reserialise round trip.

SSZ does not make any effort to compact or compress the serialised data, and there were concerns that this might make it inefficient for the wire transfer protocol. These concerns were alleviated by adding [Snappy compression](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/p2p-interface.md#encoding-strategies) on the wire, as is already done in Ethereum&nbsp;1.

##### SSZ development

SSZ is [based on](https://ethresear.ch/t/replacing-ssz-with-rlp-zip-and-sha256/5706/12?u=benjaminion) Ethereum's smart contract [ABI](https://docs.soliditylang.org/en/v0.8.11/abi-spec.html), but with 4-byte position and size records rather than 32-byte, and different basic data types. It will immediately feel familiar to anyone who has fiddled with that. The rudiments of SSZ were laid down by Vitalik in [August 2017](https://github.com/ethereum/research/tree/master/py_ssz).

The initial, more developed, spec for SSZ was merged into the beacon chain repository in [October 2018](https://github.com/ethereum/consensus-specs/pull/18), with the `Container` type being added [a month later](https://github.com/ethereum/consensus-specs/pull/102/files).

A big step forward in the utility of SSZ, and what established it as the serialisation protocol of choice for consensus, was the development of [Merkleization](/part2/building_blocks/merkleization/) (also known as tree hashing), first discussed in [October 2018](https://github.com/ethereum/consensus-specs/issues/54) and adopted into the spec in [November](https://github.com/ethereum/consensus-specs/pull/120).

Also in [November 2018](https://github.com/ethereum/consensus-specs/pull/139) we agreed to switch the byte ordering for integer types from big-endian to little-endian at the request of the Nimbus team. This means that the 32-bit number representing 66 decimal is now serialised as `0x42000000` rather than `0x00000042`. The main motivation for the change was to map better to byte-ordering in typical microprocessors.

[April 2019](https://github.com/ethereum/consensus-specs/pull/787) saw a major change to SSZ with the adoption of offsets. This came from a scheme, [Simple Offset Serialisation](https://gist.github.com/karalabe/3a25832b1413ee98daad9f0c47be3632), previously proposed by Péter Szilágyi. The idea is to split the objects we are serialising according to whether they are fixed length or variable length. The serialisation then has two sections. The first section contains both actual serialisations of any fixed length objects, and pointers (offsets) to the serialisations of any variable length objects. The second section contains the serialisations of the variable length objects. The motivation for this is to allow fast access to arbitrary parts of the serialised data without having to deserialise the whole structure.

There was one final substantial re-work of the SSZ spec in [June 2019](https://github.com/ethereum/consensus-specs/pull/1180) in which SSZ lists were required to have a maximum length specified, and bitlist and bitvector types [were added](https://github.com/ethereum/consensus-specs/pull/1224).

#### Overview

The [specification of SSZ](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md) is maintained in the main consensus specs repo, and that's the place to go for all the details. I will only be presenting an introductory overview here, with a few examples.

The ultimate goal of SSZ is to be able to represent complex internal data structures such as the [BeaconState](/part3/containers/state/#beaconstate) as strings of bytes.

The formal properties that we require for SSZ to be useful for both consensus and communications are as defined in the [SSZ formal verification](https://github.com/ConsenSys/eth2.0-dafny/blob/master/wiki/ssz-notes.md#expected-properties-of-serialisedeserialise) exercise. Given objects $O_1$ and $O_2$, both of type $T$, we require that SSZ be

  1. involutive: $\texttt{deserialise}\langle T \rangle(\texttt{serialise}\langle T \rangle(O_1)) = O_1$  (required for communications), and
  2. injective: $\texttt{serialise}\langle T \rangle(O_1) = \texttt{serialise}\langle T \rangle(O_2)$ implies that $O_1 = O_2$ (required for consensus).

The first property says that when we serialise an object of a certain type then deserialise the result, we end up with an object identical to the one we started with. This is essential for the communications protocol.

The second says that if we serialise two objects of the same type and get the same result then the two objects are identical. Equivalently, if we have two different objects of the same type then their serialisations will differ. This is essential for the consensus protocol.

Beyond those basic functional requirements, other goals for SSZ are to be (relatively) simple, to create (fairly) compact serialisations, and to be compatible with [Merkleization](/part2/building_blocks/merkleization/). It is also useful to be able to quickly access specific bits of data within the serialisation without deserialising the entire object. The adoption of offsets into SSZ improved its performance in that respect.

Unlike RLP, SSZ is not self-describing. You can decode RLP data into a structured object without knowing in advance what that object looks like. This is not the case for SSZ: you must know in advance exactly what you are deserialising. In practice this has not been a problem for Eth2: we always know in advance what class of object a particular deserialised blob of data corresponds to. A consequence of this is that, while in RLP two objects of different types cannot serialise to the same output, in SSZ they can. We'll see an example of this shortly.

#### Specification

I don't plan to go into every last detail of SSZ &ndash; that's what the [specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md) is for &ndash; rather, we'll take a general overview and then dive into a [worked example](#worked-example).

The building blocks of SSZ are its basic types and its composite types.

##### Basic types

SSZ's basic types are very simple and limited, comprising only the following two classes.

  - Unsigned integers: a `uintN` is an `N`-bit unsigned integer, where `N` can be 8, 16, 32, 64, 128 or 256.
  - Booleans: a `boolean` is either `True` or `False`.

The serialisation of basic types lives up to the "simple" name:

  - `uintN` types are encoded as the little-endian representation in `N/8` bytes. For example, the decimal number 12345 (`0x3039` in hexadecimal) as a `uint16` type is serialised as `0x3930` (two bytes). The same number as a `uint32` type is serialised as `0x39300000` (four bytes).
  - `boolean` types are always one byte and serialised as `0x01` for true and `0x00` for false.

I have embedded some examples in the following descriptions. You can run them yourself if you set up the Eth2 spec as per the [instructions](/appendices/running/) in the Appendices. The examples can be run via the Python REPL or by putting the commands in a file (I show both approaches).

```python
>>> from eth2spec.utils.ssz.ssz_typing import uint64, boolean
>>> uint64(0x0123456789abcdef).encode_bytes().hex()
'efcdab8967452301'
>>> boolean(True).encode_bytes().hex()
'01'
>>> boolean(False).encode_bytes().hex()
'00'
```

##### Composite types

Composite types hold combinations of or multiples of smaller types. The spec defines the following composite types: vectors, lists, bitvectors, bitlists, unions, and containers. I will skip unions in the following as they are not currently used in Ethereum&nbsp;2.

###### Vectors

A vector is an ordered fixed-length homogeneous collection with exactly `N` values. "Homogeneous" means that all the elements of a vector must be of the same type, but they do not need to be of the same size. For example, we could have a vector containing lists that each have different numbers of elements.

In the SSZ spec a vector is denoted by `Vector[type, N]`. For example `Vector[uint8, 32]` is a 32 element list of `uint8` types (bytes). The `type` can be anything, including other vectors or even containers.

Vectors provide a simple example of needing to know what kind of object you are deserialising before you attempt it. In the following example, the same string of bytes encodes both a four element set of two-byte integers, and an eight element set of one-byte integers. When we deserialise this we need to know which of these (or many other possibilities) we are expecting to get.

```python
>>> from eth2spec.utils.ssz.ssz_typing import uint8, uint16, Vector
>>> Vector[uint16, 4](1, 2, 3, 4).encode_bytes().hex()
'0100020003000400'
>>> Vector[uint8, 8](1, 0, 2, 0, 3, 0, 4, 0).encode_bytes().hex()
'0100020003000400'
```

Fun fact: in early versions of the SSZ spec, vectors were called [tuples](https://github.com/ethereum/consensus-specs/pull/794).

###### Lists

A list is an ordered variable-length homogeneous collection with a maximum of `N` values.

In the SSZ spec a list is denoted by `List[type, N]`. For example, `List[uint64, 100]` is a list containing anywhere between zero and one hundred `uint64` types.

The maximum length parameter, `N`, on lists is [not used](https://github.com/ethereum/consensus-specs/pull/1180#issuecomment-504169216) in serialisation or deserialisation. It is used, however, in Merkleization, and in particular enables [generalised indices](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/merkle-proofs.md#generalized-merkle-tree-index) in Merkle proof generation.

[TODO: link to Merkleization and generalised indices]::

Both vectors and lists have the same serialisation when they are treated as stand-alone objects:

```python
>>> from eth2spec.utils.ssz.ssz_typing import uint8, List, Vector
>>> List[uint8, 100](1, 2, 3).encode_bytes().hex()
'010203'
>>> Vector[uint8, 3](1, 2, 3).encode_bytes().hex()
'010203'
```

So why not use lists everywhere? Since lists are variable sized objects in SSZ they are encoded differently from fixed sized vectors when contained within another object, so there is a small overhead. The container `Foo` holding the variable sized list is encoded with an extra four-byte offset at the start. We'll see why a bit later.

```python
>>> from eth2spec.utils.ssz.ssz_typing import uint8, Vector, List, Container
>>> class Foo(Container):
...     x: List[uint8, 3]
>>> class Bar(Container):
...     x: Vector[uint8, 3]
>>> Foo(x = [1, 2, 3]).encode_bytes().hex()
'04000000010203'
>>> Bar(x = [1, 2, 3]).encode_bytes().hex()
'010203'
```

###### Bitvectors

A bitvector is an ordered fixed-length collection of `boolean` values with `N` bits. In the SSZ spec, a bitvector is denoted by `Bitvector[N]`.

It is not obvious from the spec, but bitvectors use little-endian bit format:

```python
>>> from eth2spec.utils.ssz.ssz_typing import Bitvector
>>> Bitvector[8](0,0,0,0,0,0,0,1).encode_bytes().hex()
'80'
```

Bitvectors are encoded into the minimum necessary number of whole bytes (`N // 8`) and padded with zeroes in the high bits if `N` is not a multiple of 8.

As noted in the spec, functionally we could use either `Vector[boolean, N]` or `Bitvector[N]` to represent a list of bits. However, the latter will have a serialisation up to eight times shorter in practice since the former will use a whole byte per bit.

```python
>>> from eth2spec.utils.ssz.ssz_typing import Vector, Bitvector, boolean
>>> Bitvector[5](1,0,1,0,1).encode_bytes().hex()
'15'
>>> Vector[boolean,5](1,0,1,0,1).encode_bytes().hex()
'0100010001'
```

The same consideration applies for lists and bitlists.

###### Bitlists

A bitlist is an ordered variable-length collection of `boolean` values with a maximum of `N` bits. In the SSZ spec, a bitlist is denoted by `Bitlist[N]`.

An interesting feature of bitlists[^fn-bitlist-sentinel] is that they use a sentinel bit to indicate the length of the list. The number of whole bytes in the bitlist is easily derived from the offsets in the serialisation, but that doesn't give us the precise number of bits. For example, in a naive scheme 13 bits would be serialised into two bytes, so we would only know that the actual list length is somewhere between 9 and 16 bits.

[^fn-bitlist-sentinel]: Though [not entirely](https://github.com/ethereum/consensus-specs/issues/1266) uncontroversial. Basically, if the application layer already knows what length of bitlist it expects &ndash; which it generally does in Eth2, since although committee sizes change, the sizes are known &ndash; then we could in principle dispense with the sentinel bit.

To resolve this problem, bitlist serialisation adds an extra `1` bit at the end of the list (which becomes the highest-order bit in the little-endian encoding). The exact length of the bitlist can then be found by ignoring any consecutive high-order zero bits and then stripping off the single sentinel bit.

As an example, this bitlist with three elements is encoded into a single byte. To deserialise this, we take the total length in bits (eight), skip the four high-order zero bits, skip the sentinel bit, and then our list comprises the remaining three bits. Equivalently, the bitlist length is the index of the highest `1` bit in the serialisation.

```python
>>> from eth2spec.utils.ssz.ssz_typing import Bitlist
>>> Bitlist[100](0,0,0).encode_bytes().hex()
'08'
```

<a id="img_ssz_bitlist"></a>
<figure class="diagram" style="width: 60%">

![A diagram showing how the bitlist sentinel works.](images/diagrams/ssz-bitlist.svg)

<figcaption>

The sentinel bit indicates the end of the bitlist. All bits beyond the sentinel are zero.

</figcaption>
</figure>

As a consequence of the sentinel, we require an extra byte to serialise a bitlist if its actual length is a multiple of eight (irrespective of the maximum length). This is not the case for a bitvector.

```python
>>> Bitlist[8](0,0,0,0,0,0,0,0).encode_bytes().hex()
'0001'
>>> Bitvector[8](0,0,0,0,0,0,0,0).encode_bytes().hex()
'00'
```

###### Containers

A container is an ordered heterogeneous collection of values. Basically, a container can contain any arbitrary mix of types, including containers.

We define containers using Python's `dataclass` notation with key&ndash;type pairs. For example, this is a [`Deposit`](/part3/containers/operations/#deposit) container. In the following examples I have indicated the underlying types in the appended comments.

```python
class Deposit(Container):
    proof: Vector[Bytes32, DEPOSIT_CONTRACT_TREE_DEPTH + 1] # Vector[Vector[uint8, 32], N]
    data: DepositData
```

The `Deposit` container contains a [`DepositData`](/part3/containers/dependencies/#depositdata) container which is defined as follows.

```python
class DepositData(Container):
    pubkey: BLSPubkey                # Bytes48 / Vector[uint8, 48]
    withdrawal_credentials: Bytes32  # Vector[uint8, 32]
    amount: Gwei                     # uint64
    signature: BLSSignature          # Bytes96 / Vector[uint8, 96]
```

We'll see how containers are serialised in the [worked example](#worked-example), below.

##### Fixed and variable size types

SSZ distinguishes between fixed size and variable size types, and treats them differently when they are contained within other types.

  - Variable size types are lists, bitlists, and any type that contains a variable size type.
  - Everything else is fixed size.

This distinction is important when we serialise a compound type. The serialised output is created in two parts, as follows.

  1. The serialisation of fixed length types, along with 32-bit offsets to any variable length types.
  2. The serialisation of any variable length types.

This split between a fixed length part and a variable length part came about as a result of the offset encoding described earlier: it allows fast access to specific fields within a serialised data structure without needing to deserialise the whole thing.

As an example, consider the following container. It has a single fixed length `uint8` type, followed by a variable length `List[uint8,10]` type, followed again by a fixed length `uint8`.

```python
>>> from eth2spec.utils.ssz.ssz_typing import uint8, List, Container
>>> class Baz(Container):
...     x: uint8
...     y: List[uint8, 10]
...     z: uint8
>>> Baz(x = 1, y = [2, 3], z = 4).encode_bytes().hex()
'0106000000040203'
```

We see that the serialisation contains an unexpected `0x06` byte and some zero bytes. To see where they come from I'll break down the output as follows, where the first column is the byte number in the serialised string.

```none
Start of Part 1 (fixed size elements)
00 01       - The serialisation of x = uint8(1)
01 06000000 - A 32-bit offset to byte 6 (in little-endian format),
              the start of the serialisation of y
05 04       - The serialisation of z = uint8(4)

Start of Part 2 (variable size elements)
06 0203     - The serialisation of y = List[uint8, N]([2, 3])
```

In Part&nbsp;1, instead of directly encoding the variable size list in place, it is replaced with a pointer (an offset) to its serialisation in Part&nbsp;2. So, for any container, the size of Part&nbsp;1 is known and fixed no matter what kinds of variable size types are present. The actual lengths of the variable size objects can be deduced from the offsets in Part&nbsp;1 and the overall length of the serialisation string.

<a id="img_ssz_examples_baz"></a>
<figure class="diagram" style="width:60%">

![Diagram of the serialisation of the Baz container.](images/diagrams/ssz-examples_Baz.svg)

<figcaption>

Serialisation of the `Baz` container. Fixed size parts are done first, with an offset specified for the variable size `List` data.

</figcaption>
</figure>

It's not only containers that use this format, it applies to any type that contains variable size types. Here's a vector whose elements are lists. As an exercise for the reader I'll leave you to decode what's going on here.

```python
>>> from eth2spec.utils.ssz.ssz_typing import uint8, List, Vector
>>> Vector[List[uint8,3],4]([1,2],[3,4,5],[],[6]).encode_bytes().hex()
'10000000120000001500000015000000010203040506'
```

##### Aliases

Just quoting directly from [the SSZ spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#aliases) here for completeness[^fn-ssz-json]:

> For convenience we alias:
>
>   - `bit` to `boolean`
>   - `byte` to `uint8` (this is a basic type)
>   - `BytesN` and `ByteVector[N]` to `Vector[byte, N]` (this is _not_ a basic type)
>   - `ByteList[N]` to `List[byte, N]`

[^fn-ssz-json]: An instructive discussion of the wisdom or otherwise of aliasing `byte` to `uint8` was sparked when we began defining a [canonical JSON mapping](https://github.com/ethereum/consensus-specs/pull/2983) for SSZ data. The words "fight to the death" were used.

In the main beacon chain spec, a bunch of [custom types](/part3/config/types/#table_custom_types) are also defined in terms of the standard SSZ types and aliases. For example, `Slot` is an SSZ `uint64` type, `BLSPubkey` is an SSZ `Bytes48` type, and so on.

##### Default values

Finally, each type has a default value. Once again directly from [the SSZ spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#default-values):

| Type | Default Value |
| ---- | ------------- |
| `uintN` | `0` |
| `boolean` | `False` |
| `Container` | `[default(type) for type in container]` |
| `Vector[type, N]` | `[default(type)] * N` |
| `Bitvector[N]` | `[False] * N` |
| `List[type, N]` | `[]` |
| `Bitlist[N]` | `[]` |

#### Worked example

Let's explore a worked example to gather all of this together. I'd rather use a real example than make up a synthetic object, so we are going to look at the aggregate `IndexedAttestation` that was included in the beacon chain block [at slot 3080831](https://beaconcha.in/slot/3080831#attestations), at position 87 within the block. (It would actually have been an [`Attestation`](/part3/containers/operations/#attestation) object in the block, but those bitlists are fiddly, so we'll look at the equivalent [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation).)

##### The data structures

The [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation) container looks like this.

```python
class IndexedAttestation(Container):
    attesting_indices: List[ValidatorIndex, MAX_VALIDATORS_PER_COMMITTEE]
    data: AttestationData
    signature: BLSSignature
```

It contains an [`AttestationData`](/part3/containers/dependencies/#attestationdata) container,

```python
class AttestationData(Container):
    slot: Slot
    index: CommitteeIndex
    beacon_block_root: Root
    source: Checkpoint
    target: Checkpoint
```

which in turn contains two [`Checkpoint`](/part3/containers/dependencies/#checkpoint) containers,

```python
class Checkpoint(Container):
    epoch: Epoch
    root: Root
```

##### The serialisation

Now we have enough information to build the `IndexedAttestation` object and calculate its SSZ serialisation.

```python
from eth2spec.utils.ssz.ssz_typing import *
from eth2spec.capella import mainnet
from eth2spec.capella.mainnet import *

attestation = IndexedAttestation(
    attesting_indices = [33652, 59750, 92360],
    data = AttestationData(
        slot = 3080829,
        index = 9,
        beacon_block_root = '0x4f4250c05956f5c2b87129cf7372f14dd576fc152543bf7042e963196b843fe6',
        source = Checkpoint (
            epoch = 96274,
            root = '0xd24639f2e661bc1adcbe7157280776cf76670fff0fee0691f146ab827f4f1ade'
        ),
        target = Checkpoint(
            epoch = 96275,
            root = '0x9bcd31881817ddeab686f878c8619d664e8bfa4f8948707cba5bc25c8d74915d'
        )
    ),
    signature = '0xaaf504503ff15ae86723c906b4b6bac91ad728e4431aea3be2e8e3acc888d8af'
                + '5dffbbcf53b234ea8e3fde67fbb09120027335ec63cf23f0213cc439e8d1b856'
                + 'c2ddfc1a78ed3326fb9b4fe333af4ad3702159dbf9caeb1a4633b752991ac437'
)

print(attestation.encode_bytes().hex())
```

The resulting serialised blob of data that represents this `IndexedAttestation` object is (in hexadecimal):

```none
e40000007d022f000000000009000000000000004f4250c05956f5c2b87129cf7372f14dd576fc15
2543bf7042e963196b843fe61278010000000000d24639f2e661bc1adcbe7157280776cf76670fff
0fee0691f146ab827f4f1ade13780100000000009bcd31881817ddeab686f878c8619d664e8bfa4f
8948707cba5bc25c8d74915daaf504503ff15ae86723c906b4b6bac91ad728e4431aea3be2e8e3ac
c888d8af5dffbbcf53b234ea8e3fde67fbb09120027335ec63cf23f0213cc439e8d1b856c2ddfc1a
78ed3326fb9b4fe333af4ad3702159dbf9caeb1a4633b752991ac437748300000000000066e90000
00000000c868010000000000
```

This can be transmitted as a string of bytes over the wire and, knowing at the other end that it represents an `IndexedAttestation`, reconstituted into an identical copy.

##### The serialisation unpacked

To make sense of this, we'll break down the serialisation into its parts. The first column is the byte-offset from the start of the byte string (in hexadecimal). Before each line I've indicated which part of the data structure it corresponds to, and I've translated the type aliases into their basic underlying SSZ types. Remember that all integer types are little-endian, so `7d022f0000000000` is the hexadecimal number `0x2f027d`, which is 3080829 in decimal (the slot number).

```none
Start of Part 1 (fixed size elements)
   4-byte offset to the variable length attestation.attesting_indices starting at 0xe4
00 e4000000

   attestation.data.slot: Slot / uint64
04 7d022f0000000000

   attestation.data.index: CommitteeIndex / uint64
0c 0900000000000000

   attestation.data.beacon_block_root: Root / Bytes32 / Vector[uint8, 32]
14 4f4250c05956f5c2b87129cf7372f14dd576fc152543bf7042e963196b843fe6

   attestation.data.source.epoch: Epoch / uint64
34 1278010000000000

   attestation.data.source.root: Root / Bytes32 / Vector[uint8, 32]
3c d24639f2e661bc1adcbe7157280776cf76670fff0fee0691f146ab827f4f1ade

   attestation.data.target.epoch: Epoch / uint64
5c 1378010000000000

   attestation.data.target.root: Root / Bytes32 / Vector[uint8, 32]
64 9bcd31881817ddeab686f878c8619d664e8bfa4f8948707cba5bc25c8d74915d

   attestation.signature: BLSSignature / Bytes96 / Vector[uint8, 96]
84 aaf504503ff15ae86723c906b4b6bac91ad728e4431aea3be2e8e3acc888d8af
a4 5dffbbcf53b234ea8e3fde67fbb09120027335ec63cf23f0213cc439e8d1b856
c4 c2ddfc1a78ed3326fb9b4fe333af4ad3702159dbf9caeb1a4633b752991ac437

Start of Part 2 (variable size elements)
   attestation.attesting_indices: List[uint64, MAX_VALIDATORS_PER_COMMITTEE]
e4 748300000000000066e9000000000000c868010000000000
```

The first thing to notice is that the `attesting_indices` list is variable size, so it is represented in Part&nbsp;1 by an offset pointing to where the actual data is. In this case, at `0xe4` bytes (228 bytes) from the start of the serialised data. The actual length of the list can be calculated as the length of the whole string (252 bytes) minus 228 bytes (the start of the list) divided by 8 bytes, one per element. And so, we recover our list of three validator indices.

All the remaining items are fixed size, and are encoded in-place, including recursively encoding the fixed size `AttestationData` object, and its fixed size `Checkpoint` children.

<a id="img_ssz_examples_indexedattestation"></a>
<figure class="diagram" style="width:72%">

![Diagram of the serialisation of the IndexedAttestation container.](images/diagrams/ssz-examples_IndexedAttestation.svg)

<figcaption>

Serialisation of the `IndexedAttestation` container.

</figcaption>
</figure>

##### Multiple variable size objects

It is instructive to see how container with multiple variable size child objects is serialised. For this example we will make an [`AttesterSlashing`](/part3/containers/operations/#attesterslashing) object that contains two of the above `IndexedAttestation` objects. This is a contrived example; the slashing report is not valid since the contents are duplicates.

An `AttesterSlashing` container is defined as follows,

```python
class AttesterSlashing(Container):
    attestation_1: IndexedAttestation
    attestation_2: IndexedAttestation
```

which we can populate and serialise like this, using our previously defined `IndexedAttestation` object, `attestation`.

```python
slashing = AttesterSlashing(
    attestation_1 = attestation,
    attestation_2 = attestation
)

print(slashing.encode_bytes().hex())
```

From this we get the following serialisation, again shown with the byte-offset within the byte string in the first column.

```none
Start of Part 1 (fixed size elements)
0000 08000000
0004 04010000

Start of Part 2 (variable size elements)
0008 e40000007d022...
0104 e40000007d022...
```

This time we have two variable length types, so they are both replaced by offsets pointing to the start of the actual variable length data which appears in Part 2. The length of `attestation_1` is calculated as the difference between the two offsets, and the length of `attestation_2` is calculated as the length from its offset to the end of the string.

Another thing to note is that, since `attestation_1` and `attestation_2` are identical, their serialisations within this compound object are identical, _including_ their internal offsets to their own variable length parts. That is, both attestations have variable length data at offset `0xe4` within their own serialisations; the offset is relative to the start of each sub-object's serialisation, not the entire string. This property simplifies recursive serialisation and deserialisation: a given object will have the same serialisation no matter what context it is found in.

<a id="img_ssz_examples_attesterslashing"></a>
<figure class="diagram" style="width:60%">

![Diagram of the serialisation of the AttesterSlashing container.](images/diagrams/ssz-examples_AttesterSlashing.svg)

<figcaption>

Serialisation of the `AttesterSlashing` container.

</figcaption>
</figure>

#### See also

The [SSZ specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md) is the authoritative source. There is also a curated list of [SSZ implementations](https://github.com/ethereum/consensus-specs/issues/2138).

The historical discussion threads around whether to use SSZ for both consensus and p2p serialisation or not are a goldmine of insight and wisdom.

  - [Possibly the first](https://ethresear.ch/t/discussion-p2p-message-serialization-standard/2781?u=benjaminion) substantial discussion around which serialisation scheme to adopt. It covers various alternatives, touches on the p2p vs. consensus issues, and rehearses some of the desirable properties.
  - An [early discussion of SSZ](https://github.com/ethereum/beacon_chain/issues/94) went over some of the issues and led into the discussion below.
  - [Proposal to use SSZ for consensus only](https://github.com/ethereum/consensus-specs/issues/129).
  - Piper Merriam's [Everything You Never Wanted To Know About Serialization](https://notes.ethereum.org/QF8jgOQbRTWUhK1zoi8D4Q#) remains a good summary of many of the considerations.

Other SSZ resources:

  - [SSZ encoding diagrams](https://github.com/protolambda/eth2-docs#ssz-encoding) by Protolambda.
  - Formal verification of the SSZ specification: [Notes](https://github.com/ConsenSys/eth2.0-dafny/blob/master/wiki/ssz-notes.md) and [Code](https://github.com/ConsenSys/eth2.0-dafny/tree/master/src/dafny/ssz).
  - An excellent [SSZ explainer](https://rauljordan.com/go-lessons-from-writing-a-serialization-library-for-ethereum/) by Raul Jordan with a deep dive into implementing it in Golang. (Note that the specific library referenced in the article has now been [deprecated](https://github.com/prysmaticlabs/go-ssz) in favour of [fastssz](https://github.com/ferranbt/fastssz).)
  - An [interactive SSZ serialiser/deserialiser](https://simpleserialize.com/) by ChainSafe with all the containers for the various consensus layer upgrades available to play with. On the "Deserialize" tab you can paste the data from the `IndexedAttestation` above and verify that it deserialises correctly (you'll need to remove line breaks).

### Hash Tree Roots and Merkleization <!-- /part2/building_blocks/merkleization/ -->

<div class="summary">

  - A hash tree root provides a succinct cryptographic digest of an SSZ data structure.
  - Calculating the hash tree root involves recursively Merkleizing the data structure.
  - Merkleization is tightly coupled to [SSZ](/part2/building_blocks/ssz/) and is defined in the same spec.
  - The use of hash tree roots enables large parts of the beacon state to be cached, making it practical to operate with a monolithic beacon state.
  - Eth2's Merkleization approach facilitates [generalised indices and Merkle proofs](/part2/building_blocks/merkle_proofs/) which are important for light clients.

</div>

#### Introduction

While discussing [SSZ](/part2/building_blocks/ssz/), I asserted that serialisation is important for consensus without going into the details. In this section we will unpack that and take a deep dive into how Ethereum&nbsp;2 nodes know that they share a view of the world.

Let's say that you and I want to compare our beacon states to see if we have an identical view of the state of the chain. One way we could do this is by serialising our respective beacon states and sending them to each other. We could then compare them byte-by-byte to check that they match. The problem with this is that the serialised beacon state at the time of writing is over 41&nbsp;MB in size and takes several seconds to transmit over the Internet. This is completely impractical for a global consensus protocol.

What we need is a _digest_ of the state: a brief summary that is enough to determine with a very high degree of confidence whether you and I have the same state, or whether they differ. The digest must also have the property that no-one can fake it. That is, you can't convince me that you have the same state as I do while actually having a different state.

Thankfully, such digests exist in the form of [cryptographic hash functions](https://en.wikipedia.org/wiki/Cryptographic_hash_function). These take a (potentially) large amount of input data and mangle it up into a small number of bytes, typically 32, that for all practical purposes uniquely fingerprint the data.

Armed with such a hash function[^fn-hash-function-search] we can improve on the previous idea. You and I separately serialise our beacon states and then hash (apply the hash function to) the resulting strings. This is much faster than sending all the data over the network. Now we only need to exchange and compare our very short 32-byte hashes. If they match then we have the same state; if they don't match then our states differ.

[^fn-hash-function-search]: See the [Annotated Spec](/part3/helper/crypto/#hash) for the saga of Eth2's hash function, and how we ended up with SHA256.

This process is very common, and was an early candidate for consensus purposes in Ethereum&nbsp;2, though it was apparent [fairly early](https://github.com/ethereum/consensus-specs/blame/24c8a53b5c7be0248015413b6c0f8586e79d6b67/specs/casper_sharding_v2.1.md#L588) that there might be better ways.

A problem with this approach is that, if you modify any part of the state &ndash; even a single bit &ndash; then you need to recalculate the hash of the entire serialised state. This is potentially a huge overhead. It was dealt with in early versions of the spec by splitting the beacon state into [two parts](https://github.com/ethereum/consensus-specs/commit/0001b7b9de2bf87ff267547acdb99788cf9b463c#diff-4b26170476a5cef3886e7a1e74bb27a76abf80c7f4c4413d0ad1b47692571b6bR85): a slowly changing "crystallised" state that would rarely need re-hashing, and a smaller fast changing "active" state. However, this division of the state was a bit arbitrary and began to compromise some [other parts](https://github.com/ethereum/consensus-specs/pull/122#issuecomment-437170249) of the design.

Ultimately, the split state approach was abandoned in favour of a method called "tree hashing", which is built on a technique called Merkleization[^fn-merkleization-name]. The remainder of this section explores this approach.

<!-- markdownlint-disable code-block-style -->
[^fn-merkleization-name]: The name Merkleization derives from [Merkle trees](https://en.wikipedia.org/wiki/Merkle_tree), which in turn are named for the computer scientist [Ralph Merkle](https://en.wikipedia.org/wiki/Ralph_Merkle).

    I believe the noun "Merkleization", though, is ours. I've adopted the [majority](https://web.archive.org/web/20230630135623/https://nitter.it/sina_mahmoodi/status/1266026711512162305) preferred spelling, which is also the version that made it into the [SSZ spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#merkleization). The ugly version won despite my [best efforts](https://web.archive.org/web/20230630135649/https://nitter.it/benjaminion_xyz/status/1266049966163857408).
<!-- markdownlint-enable code-block-style -->

Tree hashing brings two significant advantages over other methods of creating a beacon state digest.

The first advantage is performance. On the face of it, tree hashing is [quite inefficient](https://github.com/ethereum/consensus-specs/pull/120#issue-378791752) since it requires hashing around double the amount of data to calculate the digest (the root) of a structure compared with the other method of simply hashing the entire serialisation. However, the way that hash trees are constructed in Ethereum&nbsp;2 allows us to cache the roots of entire subtrees that have not changed. So, for example, [by design](/part2/incentives/balances/#engineering-aspects-of-effective-balance) the list of validator records in the state does not change frequently. As a result we can cache the hash tree root of the list and do not need to recalculate it every time we recalculate the entire beacon state root. Overall this feature results in a huge reduction in the total amount of hashing required to calculate state roots, and is an important part of making the beacon chain protocol usable in practice.

The second advantage is light-client support. Indeed, the [original motivation](https://github.com/ethereum/consensus-specs/issues/54) for implementing tree hashing was only about supporting light clients. Tree hashing enables efficient Merkle proofs that allow subsets of the beacon state to be provided to light clients. As long as a light client has the hash tree root by some means it can use the proofs to verify that the provided data is correct.

We will first recap Merkle trees, then extend them to Merkleization, and finally look at the construction of the hash tree root, which is our ultimate goal.

##### Terminology

The SSZ specification uses the term "Merkleization" to refer to both

  - the operation of finding the root of a Merkle tree given its leaves, and
  - the operation of finding the hash tree root of an SSZ object.

For didactic purposes I've chosen to distinguish between these more precisely. In the following sections I'll be calling the first "Merkleization", and the second "calculating a hash tree root".

With these definitions, calculating the hash tree root of an SSZ object _uses_ Merkleization, potentially multiple steps of it, but also involves other steps such packing, chunking, and length mix-ins. Moreover, Merkleization always works with full binary trees (the number of leaves is a power of two), whereas hash tree roots can be derived from much more complex binary tree structures.

#### Merkle Trees

To understand Merkleization we first need to understand [Merkle trees](https://en.wikipedia.org/wiki/Merkle_tree). These are not at all new, and date back to the 1970s.

The idea is that we have a set of "leaves", which is our data, and we iteratively reduce those leaves down to a single, short root via hashing. This reduction is done by hashing the leaves in pairs to make a "parent" node. We repeat the process on the parent nodes to make grandparent nodes, and so on to build a binary tree structure that culminates in a single ancestral root. In Merkleization we will be dealing only with structures that have a power of two number of leaves, so we have a full binary tree.

In the following diagram, the leaves are our four blobs of data, $A$, $B$, $C$, and $D$. These can be any string of data, though in Merkleization they will be 32 byte "chunks". The function $H$ is our hash function, and the operator $+$ concatenates strings. So $H(A+B)$ is the hash of the concatenation of strings $A$ and $B$[^fn-roots-and-leaves].

[^fn-roots-and-leaves]: For some reason, in computer science, trees are traditionally depicted the other way up. Call me eccentric, but I like my trees to have their leaves at the top and their roots at the bottom.

<a id="img_merkleization_tree"></a>
<figure class="diagram" style="width:80%">

![Diagram of a Merkle tree.](images/diagrams/merkleization-tree.svg)

<figcaption>

Example of a Merkle tree.

</figcaption>
</figure>

In the Eth2 implementation, each box in the diagram is a 32-byte string of data: either a 32-byte leaf, or the 32-byte output of the hash function. Thus, we obtain the 32-byte root of the tree, which is a "digest" of the data represented by the leaves. The root uniquely represents the data in the leaves; any change in the leaves leads to a different root.

Here's the same thing again on the Python REPL, assigning leaf values as $A=1$, $B=2$, $C=3$ and $D=4$. We construct the root of the tree starting from the leaves and descending through its levels until reaching the root, $H(H(A + B) + H(C + D))$. Note that all the leaf values are padded to 32-bytes and are little-endian (as per their SSZ serialisation).

```python
>>> from eth2spec.utils.ssz.ssz_typing import uint256
>>> from eth2spec.utils.hash_function import hash
>>> a = uint256(1).to_bytes(length = 32, byteorder='little')
>>> b = uint256(2).to_bytes(length = 32, byteorder='little')
>>> c = uint256(3).to_bytes(length = 32, byteorder='little')
>>> d = uint256(4).to_bytes(length = 32, byteorder='little')
>>> ab = hash(a + b)
>>> cd = hash(c + d)
>>> abcd = hash(ab + cd)
>>> abcd.hex()
'bfe3c665d2e561f13b30606c580cb703b2041287e212ade110f0bfd8563e21bb'
```

Merkle tree constructions are a fairly common way to calculate a digest of a bunch of data, such as a blockchain state. Ethereum&nbsp;1 uses a more sophisticated version of this called a hexary Merkle&ndash;Patricia trie (in Eth1 it's a "trie" not a "tree" for [complicated reasons](https://en.wikipedia.org/wiki/Trie)), though there are proposals to [simplify that](https://eips.ethereum.org/EIPS/eip-3102).

An extremely useful feature of Merkle trees is that it is quite efficient to construct inclusion proofs using them. This is critical functionality for light clients, and we will discuss it in depth when we look at [Merkle proofs](/part2/building_blocks/merkle_proofs/).

#### Merkleization

The normal way to implement a Merkle tree is to store the entire tree structure in memory or on disk, including all the intermediate levels between the leaves and the root. As leaves are updated the affected nodes in the tree are updated: changing $A$ means updating $H(A+B)$ and then the root, everything else is unchanged.

The difference with Merkleization is that the Merkle tree is computed on-the-fly from the given leaves. We can pick up where we left off from the last REPL session as follows.

```python
>>> from eth2spec.utils.merkle_minimal import merkleize_chunks
>>> merkleize_chunks([a, b, c, d]).hex()
'bfe3c665d2e561f13b30606c580cb703b2041287e212ade110f0bfd8563e21bb'
```

The Merkleization function (called `merkleize()` in the SSZ spec, and [`merkleize_chunks()`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/tests/core/pyspec/eth2spec/utils/merkle_minimal.py#L47) in the executable spec) takes a list of 32-byte chunks and returns the root of the tree for which those chunks are the leaves.

The list of chunks passed to `merkleize_chunks()` can be any length, but will be padded with zero chunks so that the total number of chunks is rounded up to the next whole power of two, such that we conceptually have a full binary tree. Thus, a list of three chunks gets implicitly padded with an extra zero chunk:

```python
>>> z = bytearray(32)
>>> merkleize_chunks([a, b, c]).hex()
'66c419026fee8793be7fd0011b9db46b98a79f9c9b640e25317865c358f442db'
>>> merkleize_chunks([a, b, c, z]).hex()
'66c419026fee8793be7fd0011b9db46b98a79f9c9b640e25317865c358f442db'
```

A larger tree width can be provided as a parameter to `merkleize_chunks()`, and the list will be padded with zero chunks accordingly. This capability is used when dealing with lists and bitlists.

```python
>>> merkleize_chunks([a]).hex()
'0100000000000000000000000000000000000000000000000000000000000000'
>>> merkleize_chunks([a], 4).hex()
'553c8ccfd20bb4db224b1ae47359e9968a5c8098c15d8bf728b19e55749c773b'
>>> merkleize_chunks([a, z, z, z]).hex()
'553c8ccfd20bb4db224b1ae47359e9968a5c8098c15d8bf728b19e55749c773b'
```

An implementation can do this zero padding "virtually", and can optimise further by pre-computing the various levels of hashes of zero chunks: $H(0 + 0)$, $H(H(0 + 0) + H(0 + 0))$, and so on. In this way we don't always need to build the whole tree to find the Merkle root.

Note that the Merkleization of a single chunk is always just the chunk itself. This reduces the overall amount of hashing needed.

#### The Hash Tree Root

The hash tree root is a generalisation of Merkleization that we can apply to the kind of complex, compound data structures we have in the beacon state. Calculating hash tree roots is tightly connected to the type-scheme of [Simple Serialize](/part2/building_blocks/ssz/).

Calculating the hash tree root of an SSZ object is recursive. Given a composite SSZ object, we iteratively move through the layers of its structure until we reach a basic type or collection of basic types that we can pack into chunks and Merkleize directly. Then we move back through the structure using the calculated hash tree roots as chunks themselves.

The process of calculating a hash tree root is defined in the [Simple Serialize specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#merkleization), and that's the place to go for the full details. However, in simplified form (once again ignoring the SSZ union type) there are basically two paths to choose from when finding an object's hash tree root.

  - For basic types or collections of basic types (lists and vectors), we just pack and Merkleize directly.
  - For containers and collections of composite types, we recursively find the hash tree roots of the contents.

The following two rules are a simplified summary of the first six rules listed in [the specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#merkleization).

 1. If `X` is an SSZ basic object, a list or vector of basic objects, or a bitlist or bitvector, then `hash_tree_root(X) = merkleize_chunks(pack(X))`. The `pack()` function returns a list of chunks that can be Merkleized directly.
 2. If `X` is an SSZ container, or a vector or list of composite objects, then the hash tree root is calculated recursively, `hash_tree_root(X) = merkleize_chunks([hash_tree_root(x) for x in X])`. The list comprehension is a list of hash tree roots, which is equivalent to a list of chunks.

We'll see plenty of concrete applications of these two rules in the [worked example](#worked-example) below.

##### Packing and Chunking

Merkleization operates on lists of "chunks" which are 32-byte blobs of data. Lists generated by means of step 2 above are already in this form. However, step 1 involves basic objects that require a "packing and chunking" operation prior to Merkleization.

The [spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#merkleization) gives the precise rules, but it basically looks like this:

  - The object (a basic type, a list/vector of basic types, or a bitlist/bitvector) is serialised via SSZ. The sentinel bit is omitted from the serialisation of bitlist types.
  - The serialisation is right-padded with zero bytes up to the next full chunk (32 byte boundary).
  - The result is split into a list of 32 byte chunks.
  - If necessary, further (virtual) zero chunks will be appended to reach the following total lengths (only lists and bitlists might actually need extra padding):
    - All basic types give a single chunk; no basic type has a serialisation longer than 32 bytes.
    - `Bitlist[N]` and `Bitvector[N]`: `(N + 255) // 256` (dividing by chunk size in bits and rounding up)
    - `List[B, N]` and `Vector[B, N]`, where `B` is a basic type: `(N * size_of(B) + 31) // 32` (dividing by chunk size in bytes and rounding up)

Containers and composite objects that result from rule 2 will have the following numbers of chunks, including zero-chunk padding where required for lists.

  - `List[C, N]` and `Vector[C, N]`, where `C` is a composite type: `N`, since the Merkleization comprises `N` hash tree roots.
  - Containers: `len(fields)`, since there is one hash tree root per field in the container.

It is not immediately obvious why lists and bitlists are padded with zero chunks up to their full maximum lengths, even if these are "virtual" chunks. However, this enables the use of generalised indices which provide a consistent way of creating Merkle proofs against hash tree roots, the topic of our [next section](/part2/building_blocks/merkle_proofs/).

Recall that, in addition to any padding added here, the Merkleization process will further pad the list with zero chunks to make it up to a power-of-two in length.

##### Mixing in the length

We want objects that have the same type but different contents to have different hash tree roots. This presents a problem for lists. Consider the list `a` of three elements, and the list `b` which is the same three elements plus a fourth zero element on the end. These are different lists of the same type, but both Merkleize to the same value.

```python
>>> from eth2spec.utils.ssz.ssz_typing import uint256, List
>>> from eth2spec.utils.merkle_minimal import merkleize_chunks
>>> a = List[uint256, 4](1, 2, 3).encode_bytes()
>>> b = List[uint256, 4](1, 2, 3, 0).encode_bytes()
>>> merkleize_chunks([a[0:32], a[32:64], a[64:96]])
0x66c419026fee8793be7fd0011b9db46b98a79f9c9b640e25317865c358f442db
>>> merkleize_chunks([b[0:32], b[32:64], b[64:96], b[96:128]])
0x66c419026fee8793be7fd0011b9db46b98a79f9c9b640e25317865c358f442db
```

We need to ensure that a list ending with a zero value has a different hash tree root from the same list without the zero value. To do this, we put lists (and bitlists) through an extra `mix_in_length()` process that involves hashing a concatenation of the Merkle root of the list and the length of the list. This is equivalent to the Merkleization of two chunks, the first being the Merkle root of the list, the second being its length.

See the [diagram](#img_merkleization_attestingindices) for `attesting_indices` below for an illustration of this in practice.

Bitlists require a similar treatment since we remove the sentinel bit before Merkleizing.

#### Summaries and Expansions

The SSZ spec describes features of Merkleization called [summaries and expansions](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#summaries-and-expansions). These are not explicit functions of Merkleization, but implicitly arise as consequences of the design.

Simply put, anywhere in the process, an entire SSZ object can be replaced with its hash tree root without affecting the final result.

We make use of this in a number of ways. First and foremost is the ability to cache the hash tree roots of any unchanged parts of the state, which makes it practical to recalculate the hash tree root of the whole state when required. For example, if a validator record is unchanged we do not need to recalculate its hash tree root when finding the root of the validator registry. If the validator registry is unchanged, we do not need to recalculate its hash tree root when calculating the full state root.

As another example, consider the [`BeaconBlock`](/part3/containers/blocks/#beaconblock) and the [`BeaconBlockHeader`](/part3/containers/dependencies/#beaconblockheader) types.

```python
class BeaconBlock(Container):
    slot: Slot
    proposer_index: ValidatorIndex
    parent_root: Root
    state_root: Root
    body: BeaconBlockBody

class BeaconBlockHeader(Container):
    slot: Slot
    proposer_index: ValidatorIndex
    parent_root: Root
    state_root: Root
    body_root: Root
```

These differ only in their last fields, `body` and `body_root` respectively. If `body_root` is the hash tree root of the `BeaconBlockBody`, `body`, then these two objects will have exactly the same hash tree root. `BeaconBlock` is the expansion type of `BeaconBlockHeader`; `BeaconBlockHeader` is a summary type of `BeaconBlock`. [Proposer slashing](/part3/containers/operations/#proposerslashing) reports make use of this fact to save space by stripping out the block bodies and replacing them with their hash tree roots.

The Flashbots [MEV-Boost](https://ethresear.ch/t/mev-boost-merge-ready-flashbots-architecture/11177?u=benjaminion) design also makes use of this capability. In the MEV-Boost system validators are required to sign "blinded blocks". That is, blocks for which they do not have the bodies. Since the header is a summary of the block (in the sense we are using the word summary here) the same signature will be valid both for the `BeaconBlockHeader` and the corresponding full `BeaconBlock`. This simplifies the protocol design.

#### Worked example

For this section's worked example we shall revisit our friend, the [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation). This gives us nice instances of Merkleizing composite type, list types, and vector types, as well as demonstrating summaries and expansions.

Recall that the `IndexedAttestation` type is defined as follows,

```python
class IndexedAttestation(Container):
    attesting_indices: List[ValidatorIndex, MAX_VALIDATORS_PER_COMMITTEE]
    data: AttestationData
    signature: BLSSignature
```

We will create an instance of this just as we did [previously](/part2/building_blocks/ssz/#the-serialisation), only for brevity I shall call it `a`, rather than `attestation`. We want to calculate the hash tree root of this `IndexedAttestation`, `a`.

A container's hash tree root is the Merkleization of the list of hash tree roots of the objects it contains (by rule 2). Diagrammatically we are building the following tree and finding its root.

<a id="img_merkleization_indexedattestation"></a>
<figure class="diagram" style="width:60%">

![Diagram showing how to calculate the hash tree root of an IndexedAttestation type.](images/diagrams/merkleization-IndexedAttestation.svg)

<figcaption>

Calculating the hash tree root of an `IndexedAttestation`. In this and the following diagrams, $R(X)$ is the Merkleization of $X$, $S(X)$ is the SSZ serialisation of $X$. Each box is a 32 byte chunk, and the small digits are the number of leaves in the Merkleization operation.

</figcaption>
</figure>

Alternatively, in code, we have the following.

```python
assert(a.hash_tree_root() == merkleize_chunks(
    [
        a.attesting_indices.hash_tree_root(),
        a.data.hash_tree_root(),
        a.signature.hash_tree_root()
    ]))
```

The [`merkleize_chunks()`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/tests/core/pyspec/eth2spec/utils/merkle_minimal.py#L47) function is provided by the `merkle_minimal.py` library. We can apply this function directly as the hash tree roots in the list already constitute chunks. (We could also use the [`get_merkle_root()`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/tests/core/pyspec/eth2spec/utils/merkle_minimal.py#L30) function, but then we'd have to specify a `pad_to` value of 4 to get a tree of the correct depth.)

##### The `attesting_indices` root

Working down the members of the list, we need the hash tree root of the `attesting_indices` object, which has type `List[ValidatorIndex, MAX_VALIDATORS_PER_COMMITTEE]`. This is a list of basic types, namely `uint64` since that's the type of [`ValidatorIndex`](/part3/config/types/#table_custom_types), and rule 1 applies.

Our `attesting_indices` list has three elements, `[33652, 59750, 92360]`, which we need to chunk and pad. First we serialise the list as usual with SSZ, then we pad it up to 32 bytes:

```python
>>> serialize(a.attesting_indices).hex()
'748300000000000066e9000000000000c868010000000000'
>>> (serialize(a.attesting_indices) + bytearray(8)).hex()
'748300000000000066e9000000000000c8680100000000000000000000000000'
```

This gives us our first chunk. However, the full number of chunks we need is `2048 // 4 = 512` (`MAX_VALIDATORS_PER_COMMITTEE` divided by `uint64`s per chunk), so we must add 511 zero chunks. In practice this padding is done "virtually". The `merkleize_chunks()` function allows us to specify the full number of chunks and takes care of adding the extras. Behind the scenes, it is creating a ten-layer deep Merkle tree with our 512 chunks as leaves and returning the tree's root.

```python
>>> merkleize_chunks([serialize(a.attesting_indices) + bytearray(8)], 512).hex()
'04e3bf0951474a6b06dd506648fdf8e84866542614e1c14fa832cd4bebfda0e3'
```

If this were a vector then our work would be done. However, when working with lists, there is a little further wrinkle: as a final step we need to concatenate the root that we have with the actual length of the list and hash them together. This is the `mix_in_length()` function described [above](#mixing-in-the-length) which we implement here by Merkleizing the list's Merkle root with the list's length.

```python
assert(a.attesting_indices.hash_tree_root() ==
       merkleize_chunks(
           [
               merkleize_chunks([a.attesting_indices.encode_bytes() + bytearray(8)], 512),
               a.attesting_indices.length().to_bytes(32, 'little')
           ]))
```

In diagram form the hash tree root calculation for the list looks like this.

<a id="img_merkleization_attestingindices"></a>
<figure class="diagram" style="width:60%">

![Diagram showing how to calculate the hash tree root of a List type.](images/diagrams/merkleization-AttestingIndices.svg)

<figcaption>

Calculating the hash tree root of the `attesting_indices`. This is a `List[uint256, 2048]` type, and our example list has three elements, comprising a single chunk. Note the extra `mix_in_length()` step that's applied to lists.

</figcaption>
</figure>

##### The `data` root

The `data` field of the `IndexedAttestation` is another container, an [`AttestationData`](/part3/containers/dependencies/#attestationdata) object, defined as,

```python
class AttestationData(Container):
    slot: Slot
    index: CommitteeIndex
    beacon_block_root: Root
    source: Checkpoint
    target: Checkpoint
```

As before, to find the hash tree root of a container, by rule 2 we need the root of the roots it contains. That is,

```python
assert(a.data.hash_tree_root() == merkleize_chunks(
    [
        a.data.slot.hash_tree_root(),
        a.data.index.hash_tree_root(),
        a.data.beacon_block_root.hash_tree_root(),
        a.data.source.hash_tree_root(),
        a.data.target.hash_tree_root()
    ]))
```

The `Slot` and the `CommitteeIndex` are just basic `uint64` types. Their hash tree roots are their little-endian 256-bit representations.

```python
>>> a.data.slot.hash_tree_root().hex()
'7d022f0000000000000000000000000000000000000000000000000000000000'
>>> a.data.index.hash_tree_root().hex()
'0900000000000000000000000000000000000000000000000000000000000000'
```

The `Root` is `Bytes32` type, which is equivalent to a `Vector[unit8, 32]`. Handily, the hash tree root is just the `Root` value itself since it is only a single chunk.

```python
>>> a.data.beacon_block_root.hex()
'4f4250c05956f5c2b87129cf7372f14dd576fc152543bf7042e963196b843fe6'
>>> a.data.beacon_block_root.hash_tree_root().hex()
'4f4250c05956f5c2b87129cf7372f14dd576fc152543bf7042e963196b843fe6'
```

The `source` and `target` are once again containers, both having type [`Checkpoint`](/part3/containers/dependencies/#checkpoint). The `Checkpoint` type is simple to Merkleize with the knowledge we have. So, putting everything together, we can find the hash tree root of the `data` field by hand as follows.

```python
assert(a.data.hash_tree_root() == merkleize_chunks(
    [
        a.data.slot.to_bytes(32, 'little'),
        a.data.index.to_bytes(32, 'little'),
        a.data.beacon_block_root,
        merkleize_chunks([a.data.source.epoch.to_bytes(32, 'little'), a.data.source.root]),
        merkleize_chunks([a.data.target.epoch.to_bytes(32, 'little'), a.data.target.root])
    ]))
```

<a id="img_merkleization_attestationdata"></a>
<figure class="diagram" style="width:80%">

![Diagram showing how to calculate the hash tree root of an AttestationData type.](images/diagrams/merkleization-AttestationData.svg)

<figcaption>

Calculating the hash tree root of an `AttestationData` container. It contains in turn two `Checkpoint` containers, `source` and `target`.

</figcaption>
</figure>

##### The `signature` root

The final part of the `IndexedAttestation` we need to deal with is the `signature` field. This is of type `Signature`, which is a `Vector[uint8, 96]` and rule 1 applies. This is simple to Merkleize as it is exactly three chunks when packed. The `merkleize_chunks()` function takes care of adding a single virtual zero chunk to make a power-of-two number of leaves.

```python
assert(a.signature.hash_tree_root() ==
       merkleize_chunks([a.signature[0:32], a.signature[32:64], a.signature[64:96]]))
```

<a id="img_merkleization_signature"></a>
<figure class="diagram" style="width:60%">

![Diagram showing how to calculate the hash tree root of a Signature type.](images/diagrams/merkleization-Signature.svg)

<figcaption>

Calculating the hash tree root of a `Signature`, which is really a `Bytes96`, or `Vector[uint8, 96]` type.

</figcaption>
</figure>

#### Putting it all together

Assembling all these parts we can illustrate in both diagram form and code form how the hash tree root of the `IndexedAttestation` is calculated from the serialisation of the underlying basic types via repeated applications of Merkleization.

##### The full picture

<a id="img_merkleization_indexedattestation_all"></a>
<figure class="diagram" style="width:100%">

![Diagram showing the full picture of how to calculate the hash tree root of an IndexedAttestation type.](images/diagrams/merkleization-IndexedAttestation_all.svg)

<figcaption>

Illustrating the steps required to calculate the hash tree root of an `IndexedAttestation`. The small digits are the number of leaves in each Merkleization operation.

</figcaption>
</figure>

##### The full code

The following code illustrates all the points from the worked example. You can run it by setting up the executable spec as described in [the Appendix](/appendices/running/). If everything goes well the only thing it should print is `Success!`.

```python
from eth2spec.capella import mainnet
from eth2spec.capella.mainnet import *
from eth2spec.utils.ssz.ssz_typing import *
from eth2spec.utils.merkle_minimal import merkleize_chunks

# Initialise an IndexedAttestation type
a = IndexedAttestation(
    attesting_indices = [33652, 59750, 92360],
    data = AttestationData(
        slot = 3080829,
        index = 9,
        beacon_block_root = '0x4f4250c05956f5c2b87129cf7372f14dd576fc152543bf7042e963196b843fe6',
        source = Checkpoint (
            epoch = 96274,
            root = '0xd24639f2e661bc1adcbe7157280776cf76670fff0fee0691f146ab827f4f1ade'
        ),
        target = Checkpoint(
            epoch = 96275,
            root = '0x9bcd31881817ddeab686f878c8619d664e8bfa4f8948707cba5bc25c8d74915d'
        )
    ),
    signature = '0xaaf504503ff15ae86723c906b4b6bac91ad728e4431aea3be2e8e3acc888d8af'
                + '5dffbbcf53b234ea8e3fde67fbb09120027335ec63cf23f0213cc439e8d1b856'
                + 'c2ddfc1a78ed3326fb9b4fe333af4ad3702159dbf9caeb1a4633b752991ac437'
)

# A container's root is the merkleization of the roots of its fields.
# This is IndexedAttestation.
assert(a.hash_tree_root() == merkleize_chunks(
    [
        a.attesting_indices.hash_tree_root(),
        a.data.hash_tree_root(),
        a.signature.hash_tree_root()
    ]))

# A list is serialised then (virtually) padded to its full number of chunks before Merkleization.
# Finally its actual length is mixed in via a further hash/merkleization.
assert(a.attesting_indices.hash_tree_root() ==
       merkleize_chunks(
           [
               merkleize_chunks([a.attesting_indices.encode_bytes() + bytearray(8)], 512),
               a.attesting_indices.length().to_bytes(32, 'little')
           ]))

# A container's root is the merkleization of the roots of its fields.
# This is AttestationData.
assert(a.data.hash_tree_root() == merkleize_chunks(
    [
        a.data.slot.hash_tree_root(),
        a.data.index.hash_tree_root(),
        a.data.beacon_block_root.hash_tree_root(),
        a.data.source.hash_tree_root(),
        a.data.target.hash_tree_root()
    ]))

# Expanding the above AttestationData roots by "manually" calculating the roots of its fields.
assert(a.data.hash_tree_root() == merkleize_chunks(
    [
        a.data.slot.to_bytes(32, 'little'),
        a.data.index.to_bytes(32, 'little'),
        a.data.beacon_block_root,
        merkleize_chunks([a.data.source.epoch.to_bytes(32, 'little'), a.data.source.root]),
        merkleize_chunks([a.data.target.epoch.to_bytes(32, 'little'), a.data.target.root]),
    ]))

# The Signature type has a simple Merkleization.
assert(a.signature.hash_tree_root() ==
       merkleize_chunks([a.signature[0:32], a.signature[32:64], a.signature[64:96]]))

# Putting everything together, we have a "by-hand" Merkleization of the IndexedAttestation.
assert(a.hash_tree_root() == merkleize_chunks(
    [
        # a.attesting_indices.hash_tree_root()
        merkleize_chunks(
            [
                merkleize_chunks([a.attesting_indices.encode_bytes() + bytearray(8)], 512),
                a.attesting_indices.length().to_bytes(32, 'little')
            ]),
        # a.data.hash_tree_root()
        merkleize_chunks(
            [
                a.data.slot.to_bytes(32, 'little'),
                a.data.index.to_bytes(32, 'little'),
                a.data.beacon_block_root,
                merkleize_chunks([a.data.source.epoch.to_bytes(32, 'little'), a.data.source.root]),
                merkleize_chunks([a.data.target.epoch.to_bytes(32, 'little'), a.data.target.root]),
            ]),
        # a.signature.hash_tree_root()
        merkleize_chunks([a.signature[0:32], a.signature[32:64], a.signature[64:96]])
    ]))

print("Success!")
```

#### See also

[What is a Merkle Tree?](https://decentralizedthoughts.github.io/2020-12-22-what-is-a-merkle-tree/) by Alin Tomescu is the best primer I have found on Merkle trees, and a great starting point if you are unsure about their construction and properties.

The [SSZ specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md) is the authoritative source for Merkleization as well as serialisation. Many [SSZ implementations](https://github.com/ethereum/consensus-specs/issues/2138) also include Merkleization.

A formal verification of Merkleization has been performed: [Notes](https://github.com/ConsenSys/eth2.0-dafny/blob/master/wiki/merkleise-notes.md) and [Code](https://github.com/ConsenSys/eth2.0-dafny/tree/master/src/dafny/merkle).

The [Remerkleable](https://github.com/protolambda/remerkleable) library is a Python implementation that introduces some more advanced tools such as backing trees for the data structures. [Ztyp](https://github.com/protolambda/ztyp) is a further exploration of backing trees. Backing trees are a useful approach to representing and maintaining the beacon state within client implementations.

[TODO: Link to backing trees section when done]::

Given the limited type of hashing that's done during Merkleization (always hashing the concatenation of two 32 byte strings), it's worth looking into whether specific performance optimisations are available. Potuz has produced an optimised library, [Hashtree](https://github.com/prysmaticlabs/hashtree), for Merkle tree computation that takes advantage of this.

### Generalised indices and Merkle proofs <!-- /part2/building_blocks/merkle_proofs/* -->

TODO

### Sync Committees <!-- /part2/building_blocks/sync_committees/* -->

TODO

## Networking <!-- /part2/networking/* -->

### Introduction

TODO

### Discovery <!-- /part2/networking/discovery/* -->

TODO

### Gossip <!-- /part2/networking/gossip/* -->

TODO

### RPC <!-- /part2/networking/rpc/* -->

TODO

### Syncing <!-- /part2/networking/syncing/* -->

TODO

### Message Types <!-- /part2/networking/messages/* -->

TODO

## Implementation <!-- /part2/implementation/* -->

### Introduction

TODO

### Protoarray <!-- /part2/implementation/protoarray/* -->

TODO

### SSZ backing tree <!-- /part2/implementation/backing_tree/* -->

TODO

### Batch signature verification <!-- /part2/implementation/batch_verification/* -->

TODO

### Slashing protection <!-- /part2/implementation/anti_slash/* -->

TODO

### Checkpoint sync <!-- /part2/implementation/checkpoint_sync/* -->

TODO

# 第三部分: 规范注解 <!-- /part3/ -->

## 介绍 <!-- /part3/introduction/ -->

The beacon chain specification is the guts of the machine. Like the guts of a computer, all the components are showing and the wires are hanging out: everything is on display. In the course of the next sections I will be dissecting the entire core beacon chain specification line by line. My aim is not only to explain how things work, but also to give some historical context: some of the reasoning behind how we ended up where we are today.

[Early versions](https://github.com/ethereum/consensus-specs/blob/86ec833172704ea0889b5d595d17f45ba1a6676f/specs/core/0_beacon-chain.md) of the specs were written with much more narrative and explanation than today's. Over time, they were coded up in Python for better precision and the benefits of being executable. However, in that process, most of the explanation and intuition was removed.[^fn-justinification] Vitalik has created his own [annotated specifications](https://github.com/ethereum/annotated-spec) that covers many of the key insights. It's hard to compete with Vitalik, but my intention here is to go one level deeper in thoroughness and detail. And perhaps to give an independent perspective.

[^fn-justinification]: A process called "Justinification". Iykyk `;-)`

As and when other parts of the book get written I will add links to the specific chapters on each topic (for example on Simple Serialize, consensus, networking).

Note that the online annotated specification is available in two forms:

  - divided into chapters in [Part 3](/part3/) of the main book, and
  - as a standalone [single page](/annotated-spec/) that's useful for searching.

The contents of each are identical.

### Version information

This edition of Upgrading Ethereum is based on the Capella version of the beacon chain specification, and corresponds to [Release v1.3.0](https://github.com/ethereum/consensus-specs/releases/tag/v1.3.0), made on the 18th of April, 2023.

There is no single specification document that covers Capella. Rather, we have the [Phase&nbsp;0 specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/beacon-chain.md), the [Altair specification changes](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/beacon-chain.md), the [Bellatrix specification changes](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/beacon-chain.md), and the [Capella specification changes](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/capella/beacon-chain.md). Each builds on top of the previous version in a kind of text-based diff. In addition, these documents are not stable between upgrades. For example, the Phase&nbsp;0 specs [were updated](https://github.com/ethereum/consensus-specs/compare/v1.2.0..v1.3.0#diff-0e824f6ab9ff551699ddf9d108c0b3705ce41e2bd72f68c7f1f32269e58f0bdf) as part of the Capella release. This can all be rather confusing and difficult to track.

To make the whole thing easier to follow in this chapter, I have consolidated all of the specifications to date, (mostly) omitting parts that have been superseded[^fn-superseded-parts]. In general, I have tried to reflect the existing structure of the documents to make them easier to read side-by-side with the original specs. However, I have included the separate [BLS](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/bls.md) document into the flow of this one.

[^fn-superseded-parts]: You can still find old versions, as described in the [Preface](/preface/#versions).

#### See also

In addition to the spec documents referenced above, a few other current and historical documents exist.

  - Vitalik's [annotated specifications](https://github.com/ethereum/annotated-spec), covering Phase&nbsp;0, Altair, The Merge, and beyond.
  - [Serenity Design Rationale](https://notes.ethereum.org/@vbuterin/rkhCgQteN)
  - [Phase 0 for Humans \[v0.10.0\]](https://notes.ethereum.org/@djrtwo/Bkn3zpwxB)
  - [Phase 0 design notes](https://notes.ethereum.org/@JustinDrake/rkPjB1_xr) (Justin Drake)
  - My own [Phase&nbsp;0 annotated specification](https://benjaminion.xyz/eth2-annotated-spec/phase0/beacon-chain/) remains available for historical interest.

Hsiao-Wei Wang gave a [Lightning Talk](https://archive.devcon.org/archive/watch/6/how-to-use-executable-consensus-pyspec/) on the consensus Pyspec at Devcon VI that briefly describes its structure and how it can be executed.

## 类型、常量、预设和配置 <!-- /part3/config/ -->

### 序言

对一些人来说，关于常量、预设和参数的章节可能比纳米比沙漠还要枯燥，但长久以来我一直发现这些是进入我们将在后续章节中详细解释的思想和机制的丰富而肥沃的途径。共识层规范的这一部分远非沙漠，它充满生机。

一套自定义的数据类型奠定了基础。信标链规范可在 Python 中执行；在规范顶部定义的数据类型代表了将频繁出现的基本量。

然后——通过常量、预设和参数——我们将检视定义和限制链的行为的数字。这些量中的每一个都讲述了一个故事。每个参数都封装了一个洞察、机制或妥协。它为什么会在这里？它随着时间的推移有何变化？它的价值从何而来？

### 自定义类型 <!-- /part3/config/types/ -->

The specification defines the following Python custom types, "for type hinting and readability": the data types defined here appear frequently throughout the spec; they are the building blocks for everything else.

Each type has a name, an "SSZ equivalent", and a description. [SSZ](/part2/building_blocks/ssz/) is the encoding method used to pass data between clients, among other things. Here it can be thought of as just a primitive data type.

Throughout the spec, (almost) all integers are unsigned 64-bit numbers, `uint64`, but this hasn't always been the case.

Regarding "unsigned", there was [much discussion](https://github.com/ethereum/consensus-specs/issues/626) around whether Eth2 should use signed or unsigned integers, and eventually unsigned was chosen. As a result, it is critical to preserve the order of operations in some places to avoid inadvertently causing underflows since negative numbers are forbidden.

And regarding "64-bit", early versions of the spec used [other](https://github.com/ethereum/consensus-specs/commit/4c3c8510d4abf969a7170fce10dcfb5d4df408c8) bit lengths than 64 (a "[premature optimisation](https://wiki.c2.com/?PrematureOptimization)"), but arithmetic integers are now [standardised at 64 bits](https://github.com/ethereum/consensus-specs/pull/1746) throughout the spec, the only exception being [`ParticipationFlags`](#participationflags), introduced in the Altair upgrade, which has type `uint8`, and is really a `byte` type.

<a id="table_custom_types"></a>

| Name                 | SSZ equivalent | Description                                                |
| --                   | ---            | -----                                                      |
| `Slot`               | `uint64`       | a slot number                                              |
| `Epoch`              | `uint64`       | an epoch number                                            |
| `CommitteeIndex`     | `uint64`       | a committee index at a slot                                |
| `ValidatorIndex`     | `uint64`       | a validator registry index                                 |
| `Gwei`               | `uint64`       | an amount in Gwei                                          |
| `Root`               | `Bytes32`      | a Merkle root                                              |
| `Hash32`             | `Bytes32`      | a 256-bit hash                                             |
| `Version`            | `Bytes4`       | a fork version number                                      |
| `DomainType`         | `Bytes4`       | a domain type                                              |
| `ForkDigest`         | `Bytes4`       | a digest of the current fork data                          |
| `Domain`             | `Bytes32`      | a signature domain                                         |
| `BLSPubkey`          | `Bytes48`      | a BLS12-381 public key                                     |
| `BLSSignature`       | `Bytes96`      | a BLS12-381 signature                                      |
| `ParticipationFlags` | `uint8`        | a succinct representation of 8 boolean participation flags |
| `Transaction`        | `ByteList[MAX_BYTES_PER_TRANSACTION]` | either a [typed transaction envelope](https://eips.ethereum.org/EIPS/eip-2718#opaque-byte-array-rather-than-an-rlp-array) or a legacy transaction |
| `ExecutionAddress`   | `Bytes20`      | Address of account on the execution layer |
| `WithdrawalIndex`    | `uint64`       | an index of a `Withdrawal` |

#### 时隙（Slot）

时间被划分为固定长度的时隙。在每个时隙内，都会随机选择一个验证者来提出信标链块。时隙的进展是信标链的基本心跳。

[TODO: link to Slots chapter]::

#### 时段（Epoch）

时隙序列被组合成固定长度的时段。

时段边界是链可以被合理化和最终确定的点（通过 Casper FFG 机制）。它们也是验证者余额更新、验证者委员会洗牌、以及验证者退出、加入和罚没被处理的点。也就是说，主要的状态转换工作是按时段进行的，而不是按时隙。

在依照时隙进展的信标链之上，时段总让人感觉有点不舒服，但 Casper FFG 的最终确定性又使其成为必须。曾有人[提议](https://ethresear.ch/t/epoch-less-casper-ffg-liveness-safety-argument/2702?u=benjaminion)摒弃时段，未来的发展也可能让我们完全[摆脱](https://ethresear.ch/t/a-model-for-cumulative-committee-based-finality/10259?u=benjaminion)它。但目前，它们仍然存在。

[TODO: link to Epochs chapter]::
[TODO: link to Casper FFG]::

Fun fact: Epochs were originally [called Cycles].趣事：时段最初被称为周期 ([Cycles](https://github.com/ethereum/consensus-specs/pull/149))。

#### 委员会索引 (CommitteeIndex)

验证者被组织成委员会，集体对区块投票（做出认证）。在每个时段，每个委员会只在其中一个时隙活跃，但在每个时隙都有几个委员会活跃。委员会索引（`CommitteeIndex`）类型是对在一个时隙活跃的委员会列表的索引。

信标链[基于委员会](/part2/building_blocks/committees/)的设计在很大程度上使其在保持安全性的同时，也使其实现更容易。如果所有验证者都一直处于活跃状态，那么要处理的信息数量将使网络不堪重负。委员会的随机洗牌使得它们很难被没有绝对多数质押的攻击者颠覆。

#### 验证者索引 (ValidatorIndex)

每个成功存款的验证者连续被分配一个唯一的验证者索引号，这个号码是永久的，即使验证者退出后也保留。它是永久的，因为验证者的余额与其索引相关联，所以当验证者退出时，每个成功存款的验证者会依次获得一个连贯且唯一的验证者索引号，该索引号是永久的，即使验证者退出后也会保留。之所以说它是永久的，是因为验证者的余额与其索引相关联，因此当验证者退出时，至少要保留数据直到在未知的将来时间取出余额。

#### Gwei

共识层上的所有以太币数额都以 Gwei 为单位（为 $10^9$ Wei,  $10^{-9}$ 个以太币）。这基本上是一个为避免使用超过 64 位宽的整数来存储验证者余额和进行计算的小技巧，因为 $2^{64}$ Wei 只有 18 个以太币。即便如此，在某些地方仍需小心处理以太币计算中的算术溢出。

#### 根 (Root)

默克尔根（Merkle roots）在 Eth2 协议中无处不在。它们是一种非常简洁和防篡改的表示大量数据的方式，是密码学累加器（[cryptographic accumulator](https://en.wikipedia.org/wiki/Accumulator_%28cryptography%29)）的一个例子。区块由其默克尔根概括；状态由其默克尔根概括；Eth1 存款列表由其默克尔根概括；消息的数字签名是由消息内数据结构的默克尔根计算得出的。

#### Hash32

默克尔根是用加密哈希函数构建的。在规范中，`Hash32` 类型用于表示 Eth1 的区块根（它也是默克尔根）。

我不知道为什么只有 Eth1 区块哈希被授予 `Hash32` 类型：规范中的其他哈希[仍然](https://github.com/ethereum/consensus-specs/pull/2689)是 `Bytes32`。在规范的早期版本中，`Hash32` 被用于所有加密哈希值，但后来被[改为](https://github.com/ethereum/consensus-specs/pull/458)  `Bytes32`。

无论如何，值得花点时间欣赏一下不起眼的[加密哈希函数](https://en.wikipedia.org/wiki/Cryptographic_hash_function)。哈希函数可以说是支撑区块链技术的单一最重要的算法创新，事实上也是我们大多数网络生活的基础。它很容易被忽视，但对我们的当代世界却至关重要。

#### 版本 (Version)

与以太坊 1 [^fn-eth1-forkid]不同，信标链有一个协议内的版本号概念。预计协议将不时更新/升级，这个过程通常被称为“硬分叉”。例如，从 Phase 0 到 Altair 的升级发生在 2021 年 10 月27 日，并分配了[它的分叉版本](https://eth2book.info/capella/part3/config/configuration/#altair_fork_version)。同样，从 Altair 到 Bellatrix 的升级也被分配了[不同的分叉版本](https://eth2book.info/capella/part3/config/configuration/#bellatrix_fork_version)。

在计算[分叉摘要（`ForkDigest`）](https://eth2book.info/capella/part3/config/types/#forkdigest)时会用到版本（`version`）。

[^fn-eth1-forkid]: Ethereum 1.0 introduced a fork identifier as defined in [EIP-2124](https://eips.ethereum.org/EIPS/eip-2124) which is similar to `Version`, but the Eth1 fork ID is not part of the consensus protocol and is used only in the [networking protocol](https://eips.ethereum.org/EIPS/eip-2364).

[TODO: Link to networking section]::

#### 域类型 (DomainType)

DomainType 只是一个[密码学的小巧思](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-hash-to-curve-12#section-2.2.5)：用于不同目的的信息在被哈希和可能被签名之前会被标记不同的域。这是一种避免冲突的命名空间；可能没有必要，但被认为是一种最佳做法。[Capella 中定义](/part3/config/constants/#domain-types)了 11 种域类型。

#### 分叉摘要 (ForkDigest)

`ForkDigest` 是唯一的链标识符，由创世时收集的信息与当前链 [`Version`](#version) 标识符结合生成。

`ForkDigest` 有两个作用。

  1. 在共识协议中，例如，防止来自一个分叉（可能尚未升级）上验证者的认证被计算到另一个分叉上。
  2. 在网络协议中，帮助区分在同一链上的有用的同等节点和不同链上的无用的同等节点。以太坊 2.0 网络规范（[Ethereum 2.0 networking specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/p2p-interface.md#how-should-fork-version-be-used-in-practice)）中描述了这种用法，其中 `ForkDigest` 频繁出现。

具体来说，`ForkDigest` 是 [`ForkData`](/part3/containers/dependencies/#forkdata) 对象的哈希树根的前四个字节，其中包含当前链的 [`Version`](#version) 和信标链[初始化](/part3/initialise/#def_initialize_beacon_state_from_eth1)时创建的 [`genesis_validators_root`](/part3/containers/state/#genesis_validators_root)。它在 [`compute_fork_digest()`](/part3/helper/misc/#def_compute_fork_digest) 中计算。

[TODO: link to networking section]::

#### 域 (Domain)

`Domain` 在验证协议消息验证者时使用。消息必须与正确的域和正确的分叉版本[结合](/part3/helper/misc/#def_compute_signing_root)才有效。它的计算方法是将 4 个字节的 [`DomainType`](#domaintype) 和分叉数据根（[fork data root](/part3/helper/misc/#compute_fork_data_root)）的前 28 个字节相加。

#### BLSPubkey

BLS (Boneh-Lynn-Shacham) is the digital signature scheme used by Eth2. It has some [very nice properties](https://ethresear.ch/t/pragmatic-signature-aggregation-with-bls/2105?u=benjaminion), in particular the ability to aggregate signatures. This means that many validators can sign the same message (for example, that they support block X), and these signatures can all be efficiently aggregated into a single signature for verification. The ability to do this efficiently makes Eth2 practical as a protocol. Several other protocols have adopted or will adopt BLS, such as Zcash, Chia, Dfinity and Algorand. We are using the BLS signature scheme based on the [BLS12-381](https://hackmd.io/@benjaminion/bls12-381) (Barreto-Lynn-Scott) elliptic curve.

The `BLSPubkey` type holds a validator's public key, or the aggregation of several validators' public keys. This is used to verify messages that are claimed to have come from that validator or group of validators.

In Ethereum&nbsp;2.0, BLS public keys are elliptic curve points from the BLS12-381 $G_1$ group, thus are 48 bytes long when compressed.

See the section on [BLS signatures](/part2/building_blocks/signatures/) in part&nbsp;2 for a more in-depth look at these things.

#### BLSSignature

As above, we are using BLS signatures over the [BLS12-381](https://hackmd.io/@benjaminion/bls12-381) elliptic curve in order to sign messages between participants. As with all digital signature schemes, this guarantees both the identity of the sender and the integrity of the contents of any message.

In Ethereum&nbsp;2.0, BLS signatures are elliptic curve points from the BLS12-381 $G_2$ group, thus are 96 bytes long when compressed.

#### ParticipationFlags

The `ParticipationFlags` type was introduced in the Altair upgrade as part of the accounting reforms.

Prior to Altair, all attestations seen in blocks were stored in state for two epochs. At the end of an epoch, finality calculations, and reward and penalty calculations for each active validator, would be done by processing all the attestations for the previous epoch as a batch. This created a spike in processing at epoch boundaries, and led to a noticeable increase in late blocks and attestations during the first slots of epochs. With Altair, [participation flags](https://github.com/ethereum/consensus-specs/pull/2140) are now used to continuously track validators' attestations, reducing the processing load at the end of epochs.

Three of the eight bits are [currently used](/part3/config/constants/#participation-flag-indices); five are reserved for future use.

As an aside, it might have been more intuitive if `ParticipationFlags` were a `Bytes1` type, rather than introducing a weird `uint8` into the spec. After all, it is not used as an arithmetic integer. However, `Bytes1` is a composite type in SSZ, really an alias for `Vector[uint8, 1]`, whereas `uint8` is a basic type. When computing the hash tree root of a `List` type, multiple basic types can be packed into a single leaf, while composite types take a leaf each. This would result in 32 times as many hashing operations for a list of `Bytes1`. For similar reasons the type of `ParticipationFlags` [was changed](https://github.com/ethereum/consensus-specs/pull/2176#pullrequestreview-566879992) from `bitlist` to `uint8`.

#### Transaction

The Transaction type was introduced in the Bellatrix pre-Merge upgrade to allow for Ethereum transactions to be included in beacon blocks. It appears in [`ExecutionPayload`](/part3/containers/execution/#executionpayload) objects.

Transactions are completely opaque to the beacon chain and are exclusively handled in the execution layer. A note reflecting this is included in the [Bellatrix specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/beacon-chain.md):

> _Note_: The `Transaction` type is a stub which is not final.

The maximum size of a transaction is [`MAX_BYTES_PER_TRANSACTION`](/part3/config/preset/#max_bytes_per_transaction) which looks huge, but since the underlying type is an SSZ `ByteList` (which is a [`List`](/part2/building_blocks/ssz/#lists)), a Transaction object will only occupy as much space as necessary.

#### ExecutionAddress

The ExecutionAddress type was introduced in the Bellatrix pre-Merge upgrade to represent the fee recipient on the execution chain for beacon blocks that contain transactions. It is a normal, 20-byte, Ethereum address, and is used in the [`ExecutionPayload`](/part3/containers/execution/#executionpayload) class.

#### WithdrawalIndex

The WithdrawalIndex keeps track of the total number of withdrawal transactions made from the consensus layer to the execution layer. All nodes store this number in their state, so a block containing withdrawal transactions that have unexpected withdrawal indices is invalid.

At the maximum rate of 16 withdrawals per slot, a `uint64` will take 438 billion years to overflow. This ought to be enough.

#### References

  - A [primer on Merkle roots](https://www.mycryptopedia.com/merkle-tree-merkle-root-explained/).
    - See also [Wikipedia on Merkle Trees](https://en.wikipedia.org/wiki/Merkle_tree).
  - I have written an [intro to the BLS12-381 elliptic curve](https://hackmd.io/@benjaminion/bls12-381) elsewhere.

### Constants <!-- /part3/config/constants/ -->

The distinction between "constants", "presets", and "configuration values" is not always clear, and things have moved back and forth between the sections at times[^fn-presets]. In essence, "constants" are things that are expected never to change for the beacon chain, no matter what fork or test network it is running.

[^fn-presets]: See [Issue 2390](https://github.com/ethereum/consensus-specs/pull/2390) for discussion and a rationale for the current categorisation into constants, presets, and configuration variables.

#### Miscellaneous

| Name | Value |
| - | - |
| `GENESIS_SLOT` | `Slot(0)` |
| `GENESIS_EPOCH` | `Epoch(0)` |
| `FAR_FUTURE_EPOCH` | `Epoch(2**64 - 1)` |
| `DEPOSIT_CONTRACT_TREE_DEPTH` | `uint64(2**5)` (= 32) |
| `JUSTIFICATION_BITS_LENGTH` | `uint64(4)` |
| `PARTICIPATION_FLAG_WEIGHTS` | `[TIMELY_SOURCE_WEIGHT, TIMELY_TARGET_WEIGHT, TIMELY_HEAD_WEIGHT]` |
| `ENDIANNESS` | `'little'` |

##### `GENESIS_SLOT`

The very first slot number for the beacon chain is zero.

Perhaps this seems uncontroversial, but it actually featured heavily in the Great Signedness Wars mentioned [previously](/part3/config/types/#custom-types). The issue was that calculations on unsigned integers might have negative intermediate values, which would cause problems. A proposed work-around for this was to start the chain at a non-zero slot number. It was initially set to [2^19](https://github.com/ethereum/consensus-specs/commit/656eae6f6ad85de5f4b9493ca0a4f8ca16d2e261#diff-51a43328a58414e132a744f3771f018cR193), then [2^63](https://github.com/ethereum/consensus-specs/commit/7f39f79b2e72654920b2e12127cfdfe6ad0088c6), then [2^32](https://github.com/ethereum/consensus-specs/commit/9b7b35bc9d18d0fac92ee142f1ea66ab289d3175), and finally [back to zero](https://github.com/ethereum/consensus-specs/commit/8c32128ffbda5c7e056c218cdb78ab76d856c5f5#diff-51a43328a58414e132a744f3771f018cR219). In my humble opinion, this madness only confirms that we should have been using signed integers all along.

##### `GENESIS_EPOCH`

As above. When the chain starts, it starts at epoch zero.

##### `FAR_FUTURE_EPOCH`

A candidate for the dullest constant. It's used as a default initialiser for validators' activation and exit times before they are properly set. No epoch number will ever be bigger than this one.

##### `DEPOSIT_CONTRACT_TREE_DEPTH`

`DEPOSIT_CONTRACT_TREE_DEPTH` specifies the size of the (sparse) Merkle tree used by the Eth1 deposit contract to store deposits made. With a value of 32, this allows for $2^{32}$ = 4.3 billion deposits. Given that the minimum deposit it 1 Ether, that number is clearly enough.

Since deposit receipts contain Merkle proofs, their size depends on the value of this constant.

##### `JUSTIFICATION_BITS_LENGTH`

As an optimisation to Casper FFG &ndash; the process by which finality is conferred on epochs &ndash; the beacon chain uses a "$k$-finality" rule. We will describe this more fully when we look at processing [justification and finalisation](/part3/transition/epoch/#def_weigh_justification_and_finalization). For now, this constant is just the number of bits we need to store in state to implement $k$-finality. With $k = 2$, we track the justification status of the last four epochs.

##### `PARTICIPATION_FLAG_WEIGHTS`

This array is just a convenient way to access the various weights given to different validator achievements when calculating rewards. The three weights are defined under [incentivization weights](#incentivization-weights), and each weight corresponds to a flag stored in state and defined under [participation flag indices](#participation-flag-indices).

##### `ENDIANNESS`

[Endianness](https://en.wikipedia.org/wiki/Endianness) refers to the order of bytes in the binary representation of a number: most-significant byte first is big-endian; least-significant byte first is little-endian. For the most part, these details are hidden by compilers, and we don't need to worry about endianness. But endianness matters when converting between integers and bytes, which is relevant to shuffling and proposer selection, the RANDAO, and when serialising with SSZ.

The spec began life as big-endian, but the Nimbus team from Status successfully lobbied for it to be changed to little-endian in order to better match processor hardware implementations, and the endianness [of WASM](https://webassembly.org/docs/portability/). SSZ was changed [first](https://github.com/ethereum/consensus-specs/pull/139), and then the rest of the spec [followed](https://github.com/ethereum/consensus-specs/pull/564).

#### Participation flag indices

| Name                       | Value |
| -                          | -     |
| `TIMELY_SOURCE_FLAG_INDEX` | `0`   |
| `TIMELY_TARGET_FLAG_INDEX` | `1`   |
| `TIMELY_HEAD_FLAG_INDEX`   | `2`   |

Validators making attestations that get included on-chain are rewarded for three things:

  - getting attestations included with the correct source checkpoint within 5 slots (`integer_squareroot(SLOTS_PER_EPOCH)`);
  - getting attestations included with the correct target checkpoint within 32 slots (`SLOTS_PER_EPOCH`); and,
  - getting attestations included with the correct head within 1 slot (`MIN_ATTESTATION_INCLUSION_DELAY`), basically immediately.

These flags are temporarily recorded in the [`BeaconState`](/part3/containers/state/#beaconstate) when attestations are processed, then used at the ends of epochs to update finality and to calculate validator rewards for making attestations.

The mechanism for rewarding timely inclusion of attestations (thus penalising late attestations) differs between Altair and Phase&nbsp;0. In Phase&nbsp;0, attestations included within 32 slots would receive the full reward for the votes they got correct (source, target, head), plus a declining reward based on the delay in inclusion: $\frac{1}{2}$ for a two slot delay, $\frac{1}{3}$ for a three slot delay, and so on. With Altair, for each vote, we now have a cliff before which the validator receives the full reward and after which a penalty. The cliffs differ in duration, which is intended to more accurately target incentives at behaviours that genuinely help the chain (there is little value in rewarding a correct head vote made 30 slots late, for example). See [`get_attestation_participation_flag_indices()`](/part3/helper/accessors/#get_attestation_participation_flag_indices) for how this is implemented in code.

#### Incentivization weights

| Name | Value |
| - | - |
| `TIMELY_SOURCE_WEIGHT` | `uint64(14)` |
| `TIMELY_TARGET_WEIGHT` | `uint64(26)` |
| `TIMELY_HEAD_WEIGHT` | `uint64(14)` |
| `SYNC_REWARD_WEIGHT` | `uint64(2)` |
| `PROPOSER_WEIGHT` | `uint64(8)` |
| `WEIGHT_DENOMINATOR` | `uint64(64)` |

These weights are used to calculate the reward earned by a validator for performing its duties. There are five duties in total. Three relate to making attestations: attesting to the source epoch, attesting to the target epoch, and attesting to the head block. There are also rewards for proposing blocks, and for participating in sync committees. Note that the sum of the five weights is equal to `WEIGHT_DENOMINATOR`.

On a long-term average, a validator can expect to earn a total amount of [`get_base_reward()`](/part3/transition/epoch/#def_get_base_reward) per epoch, with these weights being the relative portions for each of the duties comprising that total. Proposing blocks and participating in sync committees do not happen in every epoch, but are randomly assigned, so over small periods of time validator earnings may differ from `get_base_reward()`.

[TODO: link to discussion of things that can reduce rewards, a la V's annotated spec]::

The apportioning of rewards was overhauled in the Altair upgrade to better reflect the importance of each activity within the protocol. The total reward amount remains the same, but sync committee rewards were added, and the relative weights were adjusted. Previously, the weights corresponded to 16 for correct source, 16 for correct target, 16 for correct head, 14 for inclusion (equivalent to correct source), and 2 for block proposals. The factor of four increase in the proposer reward addressed a long-standing [spec bug](https://github.com/ethereum/consensus-specs/issues/2152#issuecomment-747465241).

<a id="img_incentives_weights"></a>
<figure class="diagram" style="width:50%">

![A piechart of the proportion of a validator's total reward derived from each of the micro-rewards.](images/diagrams/incentives-weights.svg)

<figcaption>

The proportion of the total reward derived from each of the micro-rewards.

</figcaption>
</figure>

#### Withdrawal Prefixes

| Name | Value |
| - | - |
| `BLS_WITHDRAWAL_PREFIX` | `Bytes1('0x00')` |
| `ETH1_ADDRESS_WITHDRAWAL_PREFIX` | `Bytes1('0x01')` |

[TODO: link to somewhere useful for withdrawal creds]::

Withdrawal prefixes relate to the withdrawal credentials provided when deposits are made for validators.

Two ways to specify the withdrawal credentials are currently available, versioned with these prefixes, with others such as [`0x02`](https://github.com/ethereum/consensus-specs/pull/2454) and [`0x03`](https://ethresear.ch/t/0x03-withdrawal-credentials-simple-eth1-triggerable-withdrawals/10021?u=benjaminion) under discussion.

When processing deposits onto the consensus layer, the `withdrawal_credential` of the deposit is not checked in any way. It's up to the depositor to ensure that they are using the correct prefix and contents to be able to receive their rewards and retrieve their stake back after exiting the consensus layer. This also means that we can potentially introduce new types of withdrawal credentials at any time, enabling them later with a hard fork, just as we did with `0x01` credentials ahead of the [Capella upgrade](/part4/history/capella/) that began using them.

##### `BLS_WITHDRAWAL_PREFIX`

The beacon chain launched with only BLS-style withdrawal credentials available, so all early stakers used this.

It was not at all clear in the early days what accounts on Ethereum&nbsp;2.0 would look like, and what addressing scheme they might use. The `0x00` credential created a placeholder or commitment to a future withdrawal credential change.

With this type of credential, in addition to a BLS signing key, stakers have a second BLS "withdrawal" key. Since the Capella upgrade, stakers have been able to use their withdrawal key to sign a message instructing the consensus layer to [change its withdrawal credential](/part3/transition/block/#def_process_bls_to_execution_change) from type `0x00` to type `0x01`.

The credential registered in the deposit data is the 32 byte SHA256 hash of the validator's withdrawal public key, with the first byte set to `0x00` (`BLS_WITHDRAWAL_PREFIX`).

##### `ETH1_ADDRESS_WITHDRAWAL_PREFIX`

Eth1 withdrawal credentials are much simpler, and were [adopted](https://github.com/ethereum/consensus-specs/pull/2149) once it became clear that Ethereum&nbsp;2.0 would not be using a BLS-based address scheme for accounts at any time soon. The Capella upgrade enables automatic partial and full withdrawals of validators' balances from the beacon chain to normal Ethereum accounts and wallets.

An Eth1 withdrawal credential looks like the byte `0x01` (`ETH1_ADDRESS_WITHDRAWAL_PREFIX`), followed by eleven `0x00` bytes, followed by the 20-byte Ethereum address of the destination account.

In addition to enabling withdrawal transactions for validators having `0x01` Eth1 credentials, the Capella upgrade gives validators with old `0x00` BLS style credentials the opportunity to make a [one-time change](/part3/transition/block/#def_process_bls_to_execution_change) from BLS to Eth1 withdrawal credentials[^fn-capella-bls-eth1].

[^fn-capella-bls-eth1]: Fun fact: at the point of the Capella upgrade, out of 567,144 total validators, 322,491 (56.9%) had `0x00` BLS withdrawal credentials and 244,653 (43.1%) had `0x01` Eth1 withdrawal credentials.

#### Domain types

<a id="domain_beacon_proposer"></a>
<a id="domain_beacon_attester"></a>
<a id="domain_randao"></a>
<a id="domain_deposit"></a>
<a id="domain_voluntary_exit"></a>
<a id="domain_selection_proof"></a>
<a id="domain_aggregate_and_proof"></a>
<a id="domain_sync_committee"></a>
<a id="domain_sync_committee_selection_proof"></a>
<a id="domain_contribution_and_proof"></a>
<a id="domain_bls_to_execution_change"></a>

| Name | Value |
| - | - |
| `DOMAIN_BEACON_PROPOSER`                | `DomainType('0x00000000')` |
| `DOMAIN_BEACON_ATTESTER`                | `DomainType('0x01000000')` |
| `DOMAIN_RANDAO`                         | `DomainType('0x02000000')` |
| `DOMAIN_DEPOSIT`                        | `DomainType('0x03000000')` |
| `DOMAIN_VOLUNTARY_EXIT`                 | `DomainType('0x04000000')` |
| `DOMAIN_SELECTION_PROOF`                | `DomainType('0x05000000')` |
| `DOMAIN_AGGREGATE_AND_PROOF`            | `DomainType('0x06000000')` |
| `DOMAIN_SYNC_COMMITTEE`                 | `DomainType('0x07000000')` |
| `DOMAIN_SYNC_COMMITTEE_SELECTION_PROOF` | `DomainType('0x08000000')` |
| `DOMAIN_CONTRIBUTION_AND_PROOF`         | `DomainType('0x09000000')` |
| `DOMAIN_BLS_TO_EXECUTION_CHANGE`        | `DomainType('0x0A000000')` |

These domain types are used in three ways: for seeds, for signatures, and for selecting aggregators.

##### As seeds

When random numbers are required in-protocol, one way they are generated is by hashing the RANDAO mix with other quantities, one of them being a domain type (see [`get_seed()`](/part3/helper/accessors/#def_get_seed)). The [original motivation](https://github.com/ethereum/consensus-specs/pull/1415) was to avoid occasional collisions between Phase&nbsp;0 committees and Phase&nbsp;1 persistent committees, back when they were a thing. So, when computing the beacon block proposer, `DOMAIN_BEACON_PROPOSER` is hashed into the seed, when computing attestation committees, `DOMAIN_BEACON_ATTESTER` is hashed in, and when computing sync committees, `DOMAIN_SYNC_COMMITTEE` is hashed in.

See the [Randomness](/part2/building_blocks/randomness/) chapter for more information.

##### As signatures

In addition, as a cryptographic nicety, each of the protocol's signature types is augmented with the appropriate domain before being signed:

  - Signed block proposals incorporate `DOMAIN_BEACON_PROPOSER`
  - Signed attestations incorporate `DOMAIN_BEACON_ATTESTER`
  - RANDAO reveals are BLS signatures, and use `DOMAIN_RANDAO`
  - Deposit data messages incorporate `DOMAIN_DEPOSIT`
  - Validator voluntary exit messages incorporate `DOMAIN_VOLUNTARY_EXIT`
  - Sync committee signatures incorporate `DOMAIN_SYNC_COMMITTEE`
  - BLS withdrawal credential change messages incorporate `DOMAIN_BLS_TO_EXECUTION_CHANGE`

For most of these, the fork version is [also incorporated](/part3/helper/accessors/#get_domain) before signing. This allows validators to participate, if they wish, in two independent forks of the beacon chain without fear of being slashed.

However, the user-signed messages for deposits (`DOMAIN_DEPOSIT`) and for BLS withdrawal credential changes (`DOMAIN_BLS_TO_EXECUTION_CHANGE`) do not incorporate the fork version when signed. This makes them valid across all forks, which is a usability enhancement.

Voluntary exit messages (`DOMAIN_VOLUNTARY_EXIT`) are a bit of an anomaly in that they are user signed but also incorporate the fork version, meaning that they expire after two upgrades (voluntary exit messages signed in Phase 0 or Altair are no longer valid in Capella). There is [some discussion](https://github.com/ethereum/consensus-specs/pull/3288) about making voluntary exits non-expiring in future.

See the [BLS signatures](/part2/building_blocks/signatures/) chapter for more information.

##### Aggregator selection

The remaining four types, suffixed `_PROOF` are not used directly in the beacon chain specification. They [were introduced](https://github.com/ethereum/consensus-specs/pull/1615) to implement [attestation subnet validations](https://github.com/ethereum/consensus-specs/issues/1595) for denial of service resistance. The technique was [extended](https://github.com/ethereum/consensus-specs/pull/2266) to sync committees with the Altair upgrade.

Briefly, at each slot, validators are selected to aggregate attestations from their committees. The selection is done based on the validator's signature over the slot number, mixing in `DOMAIN_SELECTION_PROOF`. The validator then signs the whole aggregated attestation, including the previous signature as proof that it was selected to be a validator, using `DOMAIN_AGGREGATE_AND_PROOF`. And similarly for sync committees. In this way, everything is verifiable and attributable, making it hard to flood the network with fake messages.

These four are not part of the consensus-critical state-transition, but are nonetheless important to the healthy functioning of the chain.

This mechanism is described in the [Phase&nbsp;0 honest validator spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#aggregation-selection) for attestation aggregation, and in the [Altair honest validator spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/validator.md#aggregation-selection) for sync committee aggregation.

See the [Aggregator Selection](/part2/building_blocks/aggregator/) chapter for more information.

#### Crypto

<a id="g2_point_at_infinity"></a>

| Name | Value |
| - | - |
| `G2_POINT_AT_INFINITY` | `BLSSignature(b'\xc0' + b'\x00' * 95)` |

This is the compressed [serialisation](https://github.com/zcash/librustzcash/blob/6e0364cd42a2b3d2b958a54771ef51a8db79dd29/pairing/src/bls12_381/README.md#serialization) of the "point at infinity", the identity point, of the G2 group of the BLS12-381 curve that we are using for signatures. Note that it is in big-endian format (unlike all other constants in the spec).

It was introduced as a convenience when verifying aggregate signatures that contain no public keys in [`eth_fast_aggregate_verify()`](/part3/helper/crypto/#def_eth_fast_aggregate_verify). The underlying [FastAggregateVerify](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-3.3.4) function from the BLS signature standard would reject these.

`G2_POINT_AT_INFINITY` is described in the separate [BLS Extensions](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/bls.md) document, but included here for convenience.

### Preset <!-- /part3/config/preset/ -->

The "presets" are consistent collections of configuration variables that are bundled together. The [specs repo](https://github.com/ethereum/consensus-specs/tree/v1.3.0/configs) currently defines two sets of presets, [mainnet](https://github.com/ethereum/consensus-specs/blob/v1.3.0/configs/mainnet.yaml) and [minimal](https://github.com/ethereum/consensus-specs/blob/v1.3.0/configs/minimal.yaml). The mainnet configuration is running in production on the beacon chain; minimal is often used for testing. Other configurations are possible. For example, Teku uses a [swift](https://github.com/ConsenSys/teku/blob/d368fd44ec43eb93923dd4c150a6649d82798e43/util/src/main/resources/tech/pegasys/teku/util/config/configs/swift.yaml) configuration for acceptance testing.

All the values discussed below are from the mainnet configuration.

You'll notice that most of these values are powers of two. There's no huge significance to this. Computer scientists think it's neat, and it ensures that things cleanly divide other things in general. There is a [view](https://github.com/ethereum/consensus-specs/issues/1633#issuecomment-592949297) that this practice helps to minimise [bike-shedding](https://en.wikipedia.org/wiki/Law_of_triviality) (endless arguments over trivial matters).

Some of the configuration parameters below are quite technical and perhaps obscure. I'll take the opportunity here to introduce some concepts, and give more detailed explanations when they appear in later chapters.

#### Misc

| Name | Value |
| - | - |
| `MAX_COMMITTEES_PER_SLOT` | `uint64(2**6)` (= 64) |
| `TARGET_COMMITTEE_SIZE` | `uint64(2**7)` (= 128) |
| `MAX_VALIDATORS_PER_COMMITTEE` | `uint64(2**11)` (= 2,048) |
| `SHUFFLE_ROUND_COUNT` | `uint64(90)` |

##### `MAX_COMMITTEES_PER_SLOT`

Validators are organised into committees to do their work. At any one time, each validator is a member of exactly one beacon chain committee, and is called on to make an attestation exactly once per epoch. An attestation is a vote for, or a statement of, the validator's view of the chain at that point in time.

On the beacon chain, up to 64 committees are active in a slot and effectively act as a single committee as far as the fork-choice rule is concerned. They all vote on the proposed block for the slot, and their votes/attestations are pooled. In a similar way, all committees active during an epoch (that is, the whole active validator set) act effectively as a single committee as far as justification and finalisation are concerned.

The number 64 was intended to map to [one committee per shard](https://github.com/ethereum/consensus-specs/pull/1428) once data shards were deployed in the now abandoned Phase&nbsp;1 of the Ethereum&nbsp;2.0 roadmap. The plan was for each committee to also vote on one shard crosslink, for a total of 64 shards. We are no longer going down that path, but the committees remain at each slot.

All the above is discussed further in the section on [Committees](/part2/building_blocks/committees/).

Note that sync committees are a different thing: there is only one sync committee active at any time.

##### `TARGET_COMMITTEE_SIZE`

To achieve a desirable level of security, committees need to be larger than a certain size. This makes it infeasible for an attacker to randomly end up with a super-majority in a committee even if they control a significant number of validators. The target here is a kind of lower-bound on committee size. If there are not enough validators for all committees to have at least 128 members, then, as a first measure, the number of committees per slot is reduced to maintain this minimum. Only if there are fewer than `SLOTS_PER_EPOCH` * `TARGET_COMMITTEE_SIZE` = 4096 validators in total will the committee size be reduced below `TARGET_COMMITTEE_SIZE`. With so few validators, the system would be insecure in any case.

For further discussion and an explanation of how the value of `TARGET_COMMITTEE_SIZE` was set, see the [section on committees](/part2/building_blocks/committees/#target-committee-size).

##### `MAX_VALIDATORS_PER_COMMITTEE`

This is just used for sizing some data structures, and is not particularly interesting. Reaching this limit would imply over 4 million active validators, staked with a total of 128 million Ether, which exceeds the [total supply](https://etherscan.io/stat/supply) today.

##### `SHUFFLE_ROUND_COUNT`

The beacon chain implements a [rather interesting](/part2/building_blocks/shuffling/) way of shuffling validators in order to select committees, called the "swap-or-not shuffle". This shuffle proceeds in rounds, and the degree of shuffling is determined by the number of rounds, `SHUFFLE_ROUND_COUNT`. The time taken to shuffle is linear in the number of rounds, so for light-weight, non-mainnet configurations, the number of rounds can be reduced.

The value 90 was introduced in Vitalik's [initial commit](https://github.com/ethereum/consensus-specs/pull/576/commits/c58410e6ce9904c6619cd925b64fbd04c00b9a89) without explanation. The [original paper](https://link.springer.com/content/pdf/10.1007%2F978-3-642-32009-5_1.pdf) describing the shuffling technique seems to suggest that a cryptographically safe number of rounds is $6\log{N}$. With 90 rounds, then, we should be good for shuffling 3.3 million validators, which is close to the maximum number possible (given the Ether supply).

#### Hysteresis parameters

| Name | Value |
| - | - |
| `HYSTERESIS_QUOTIENT` | `uint64(4)` |
| `HYSTERESIS_DOWNWARD_MULTIPLIER` | `uint64(1)` |
| `HYSTERESIS_UPWARD_MULTIPLIER` | `uint64(5)` |

The parameters prefixed `HYSTERESIS_` control the way that effective balance is changed (see [`EFFECTIVE_BALANCE_INCREMENT`](#effective_balance_increment)). As described there, the effective balance of a validator follows changes to the actual balance in a step-wise way, with [hysteresis](https://en.wikipedia.org/wiki/Hysteresis) applied. This ensures that the effective balance does not change often.

The original hysteresis design had an [unintended effect](https://github.com/ethereum/consensus-specs/issues/1609) that might have encouraged stakers to over-deposit or make multiple deposits in order to maintain a balance above 32 Ether at all times. If a validator's balance were to drop below 32 Ether soon after depositing, however briefly, the effective balance would have immediately dropped to 31 Ether and taken a long time to recover. This would have resulted in a 3% reduction in rewards for a period.

This problem was addressed by [making the hysteresis configurable](https://github.com/ethereum/consensus-specs/pull/1627) via these parameters. Specifically, these settings mean:

 1. if a validators' balance falls 0.25&nbsp;Ether below its effective balance, then its effective balance is reduced by 1&nbsp;Ether
 2. if a validator's balance rises 1.25&nbsp;Ether above its effective balance, then its effective balance is increased by 1&nbsp;Ether

These calculations are done in [`process_effective_balance_updates()`](/part3/transition/epoch/#def_process_effective_balance_updates) during end of epoch processing.

#### Gwei values

| Name | Value |
| - | - |
| `MIN_DEPOSIT_AMOUNT` | `Gwei(2**0 * 10**9)` (= 1,000,000,000) |
| `MAX_EFFECTIVE_BALANCE` | `Gwei(2**5 * 10**9)` (= 32,000,000,000) |
| `EFFECTIVE_BALANCE_INCREMENT` | `Gwei(2**0 * 10**9)` (= 1,000,000,000) |

##### `MIN_DEPOSIT_AMOUNT`

`MIN_DEPOSIT_AMOUNT` is not actually used anywhere within the beacon chain specification document. Rather, it is enforced in the [deposit contract](https://github.com/ethereum/consensus-specs/blob/v1.3.0/solidity_deposit_contract/deposit_contract.sol#L113) that [was deployed](https://etherscan.io/address/0x00000000219ab540356cbb839cbe05303d7705fa#code) to the Ethereum 1 chain. Any amount less than this value sent to the deposit contract is reverted.

Allowing stakers to make deposits smaller than a full stake is useful for topping-up a validator's balance if its effective balance has dropped below 32&nbsp;Ether in order to maintain full productivity. However, this actually led to a [vulnerability](https://medium.com/immunefi/rocketpool-lido-frontrunning-bug-fix-postmortem-e701f26d7971) for some staking pools, involving the front-running of deposits. In some circumstances, a front-running attacker could change a genuine depositor's withdrawal credentials to their own.

##### `MAX_EFFECTIVE_BALANCE`

There is a concept of "effective balance" for validators: whatever a validator's total balance, its voting power is weighted by its effective balance, even if its actual balance is higher. Effective balance is also the amount on which all rewards, penalties, and slashings are calculated - it's used a lot in the protocol

The `MAX_EFFECTIVE_BALANCE` is the highest effective balance that a validator can have: 32 Ether. Any balance above this is ignored. Note that this means that staking rewards don't compound in the usual case (unless a validator's effective balance somehow falls below 32&nbsp;Ether, in which case rewards kind of compound).

There is a discussion in the [Design Rationale](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Why-32-ETH-validator-sizes) of why 32 Ether was chosen as the staking amount. In short, we want enough validators to keep the chain both alive and secure under attack, but not so many that the message overhead on the network becomes too high.

##### `EFFECTIVE_BALANCE_INCREMENT`

Throughout the protocol, a quantity called "effective balance" is used instead of the validators' actual balances. Effective balance tracks the actual balance, with two differences: (1) effective balance is capped at `MAX_EFFECTIVE_BALANCE` no matter how high the actual balance of a validator is, and (2) effective balance is much more granular - it changes only in steps of `EFFECTIVE_BALANCE_INCREMENT` rather than [`Gwei`](/part3/config/types/#gwei).

This discretisation of effective balance is intended to reduce the amount of hashing required when making state updates. The goal is to avoid having to re-calculate the hash tree root of validator records too often. Validators' actual balances, which change frequently, are stored as a contiguous list in BeaconState, outside validators' records. Effective balances are stored inside validators' individual records, which are more costly to update (more hashing required). So we try to update effective balances relatively infrequently.

Effective balance is changed according to a process with hysteresis to avoid situations where it might change frequently. See [`HYSTERESIS_QUOTIENT`](#hysteresis-parameters).

You can read more about effective balance in the [Design Rationale](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Effective-balances) and in [this article](https://www.attestant.io/posts/understanding-validator-effective-balance/).

#### Time parameters

| Name                               | Value                     | Unit   | Duration     |
|------------------------------------|---------------------------|--------|--------------|
| `MIN_ATTESTATION_INCLUSION_DELAY`  | `uint64(2**0)` (= 1)      | slots  | 12 seconds   |
| `SLOTS_PER_EPOCH`                  | `uint64(2**5)` (= 32)     | slots  | 6.4 minutes  |
| `MIN_SEED_LOOKAHEAD`               | `uint64(2**0)` (= 1)      | epochs | 6.4 minutes  |
| `MAX_SEED_LOOKAHEAD`               | `uint64(2**2)` (= 4)      | epochs | 25.6 minutes |
| `MIN_EPOCHS_TO_INACTIVITY_PENALTY` | `uint64(2**2)` (= 4)      | epochs | 25.6 minutes |
| `EPOCHS_PER_ETH1_VOTING_PERIOD`    | `uint64(2**6)` (= 64)     | epochs | ~6.8 hours   |
| `SLOTS_PER_HISTORICAL_ROOT`        | `uint64(2**13)` (= 8,192) | slots  | ~27 hours    |

##### `MIN_ATTESTATION_INCLUSION_DELAY`

A design goal of Ethereum&nbsp;2.0 is not to heavily disadvantage validators that are running on lower-spec systems, or, conversely, to reduce any advantage gained by running on high-spec systems.

One aspect of performance is network bandwidth. When a validator becomes the block proposer, it needs to gather attestations from the rest of its committee. On a low-bandwidth link, this takes longer, and could result in the proposer not being able to include as many past attestations as other better-connected validators might, thus receiving lower rewards.

`MIN_ATTESTATION_INCLUSION_DELAY` was an attempt to "level the playing field" by setting a minimum number of slots before an attestation can be included in a beacon block. It was [originally set at 4](https://github.com/ethereum/consensus-specs/pull/143), with a 6-second slot time, allowing 24 seconds for attestations to propagate around the network.

It was [later set to one](https://github.com/ethereum/consensus-specs/pull/1157) &ndash; attestations are included as early as possible &ndash; and `MIN_ATTESTATION_INCLUSION_DELAY` exists today as a relic of the earlier design. The current slot time of 12 seconds is assumed to allow sufficient time for attestations to propagate and be aggregated sufficiently within one slot.

##### `SLOTS_PER_EPOCH`

We currently have 12-second slots and 32-slot epochs. In earlier designs, slots were 6 seconds and there were 64 slots per epoch. So the time between epoch boundaries was unchanged when slots were lengthened.

The choice of 32 slots per epoch is a trade-off between time to finality (we need two epochs to finalise, so we prefer to keep them as short as we can) and being as certain as possible that at least one honest proposer per epoch will make a block to update the RANDAO (for which we prefer longer epochs).

In addition, [epoch boundaries](/part3/transition/epoch/#epoch-processing) are where the heaviest part of the beacon chain state-transition calculation occurs, so that's another reason for not having them too close together.

Since every validator attests one every epoch, there is an interplay between the number of slots per epoch, the number of committees per slot, committee sizes, and the total number of validators.

##### `MIN_SEED_LOOKAHEAD`

A random seed is used to select all the committees and proposers for an epoch. During each epoch, the beacon chain accumulates randomness from proposers via the RANDAO and stores it. The seed for the current epoch is based on the RANDAO output from the epoch `MIN_SEED_LOOKAHEAD` ` + ` `1` ago. With `MIN_SEED_LOOKAHEAD` set to one, the effect is that we can know the seed for the current epoch and the next epoch, but not beyond, since the next-but-one epoch depends on randomness from the current epoch that hasn't been accumulated yet.

This mechanism is designed to allow sufficient time for members of newly formed committees to find each other on the peer-to-peer network, while preventing committee makeup being known too far ahead limits the opportunity for coordinated collusion between validators.

##### `MAX_SEED_LOOKAHEAD`

The above notwithstanding, if an attacker has a large proportion of the stake, or is, for example, able to DoS block proposers for a while, then it might be possible for the attacker to predict the output of the RANDAO further ahead than `MIN_SEED_LOOKAHEAD` would normally allow. This might enable the attacker to manipulate committee memberships to their advantage by performing well-timed exits and activations of their validators.

To prevent this, we assume a maximum feasible lookahead that an attacker might achieve (`MAX_SEED_LOOKAHEAD`) and delay all activations and exits by this amount, which allows new randomness to come in via block proposals from honest validators. With `MAX_SEED_LOOKAHEAD` set to 4, if only 10% of validators are online and honest, then the chance that an attacker can succeed in forecasting the seed beyond (`MAX_SEED_LOOKAHEAD` ` - ` `MIN_SEED_LOOKAHEAD`) = 3 epochs is $0.9^{3\times 32}$, which is about 1 in 25,000.

##### `MIN_EPOCHS_TO_INACTIVITY_PENALTY`

The inactivity penalty is discussed [below](#inactivity_penalty_quotient). This parameter sets the length of time until it kicks in. If the last finalised epoch is longer ago than `MIN_EPOCHS_TO_INACTIVITY_PENALTY`, then the beacon chain starts operating in "leak" mode. In this mode, participating validators no longer get rewarded, and validators that are not participating get penalised.

##### `EPOCHS_PER_ETH1_VOTING_PERIOD`

In order to safely onboard new validators, the beacon chain needs to take a view on what the Eth1 chain looks like. This is done by collecting votes from beacon block proposers - they are expected to consult an available Eth1 client in order to construct their vote.

`EPOCHS_PER_ETH1_VOTING_PERIOD` ` * ` `SLOTS_PER_EPOCH` is the total number of votes for Eth1 blocks that are collected. As soon as half of this number of votes are for the same Eth1 block, that block is adopted by the beacon chain and deposit processing can continue. This processing is done in [`process_eth1_data()`](/part3/transition/block/#def_process_eth1_data).

Rules for how validators select the right block to vote for are set out in the [validator guide](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#get_eth1_data). [`ETH1_FOLLOW_DISTANCE`](/part3/config/configuration/#eth1_follow_distance) is the (approximate) minimum depth of block that can be considered.

This parameter [was increased](https://github.com/ethereum/consensus-specs/pull/2093/files) from 32 to 64 epochs for the beacon chain mainnet. This increase is intended to allow devs more time to respond if there is any trouble on the Eth1 chain, in addition to the eight hours grace provided by `ETH1_FOLLOW_DISTANCE`.

For a detailed analysis of these parameters, see this [article](https://ethresear.ch/t/on-the-way-to-eth1-finality/7041?u=benjaminion).

##### `SLOTS_PER_HISTORICAL_ROOT`

There have been several redesigns of the way the beacon chain stores its past history. The current design is a [double batched accumulator](https://ethresear.ch/t/double-batched-merkle-log-accumulator/571?u=benjaminion). The block root and state root for every slot are stored in the state for `SLOTS_PER_HISTORICAL_ROOT` slots. When those lists are full, each list is [Merkleized separately](/part3/transition/epoch/#def_process_historical_summaries_update), and their roots are added to the ever-growing `state.historical_summaries` list within an [`HistoricalSummary`](/part3/containers/dependencies/#historicalsummary) container.

#### State list lengths

The following parameters set the sizes of some lists in the beacon chain state. Some lists have natural sizes, others such as the validator registry need an explicit maximum size [to guide SSZ serialisation](https://github.com/ethereum/consensus-specs/pull/1180).

| Name                           | Value                                 | Unit             | Duration      |
|--------------------------------|-----------------------------------    |------------------|---------------|
| `EPOCHS_PER_HISTORICAL_VECTOR` | `uint64(2**16)` (= 65,536)            | epochs           | ~0.8 years    |
| `EPOCHS_PER_SLASHINGS_VECTOR`  | `uint64(2**13)` (= 8,192)             | epochs           | ~36 days      |
| `HISTORICAL_ROOTS_LIMIT`       | `uint64(2**24)` (= 16,777,216)        | historical roots | ~52,262 years |
| `VALIDATOR_REGISTRY_LIMIT`     | `uint64(2**40)`<br/>(= 1,099,511,627,776) | validators       | -         |

##### `EPOCHS_PER_HISTORICAL_VECTOR`

This is the number of epochs of previous RANDAO mixes that are stored (one per epoch). Having access to past randao mixes allows historical shufflings to be recalculated. Since [Validator](/part3/containers/dependencies/#validator) records keep track of the activation and exit epochs of all past validators, we can reconstitute past committees as far back as we have the RANDAO values. This information can be used for slashing long-past attestations, for example. It is not clear how the value of this parameter [was decided](https://github.com/ethereum/consensus-specs/pull/1196).

##### `EPOCHS_PER_SLASHINGS_VECTOR`

In the epoch in which a misbehaving validator is slashed, its effective balance is added to an accumulator in the state. In this way, the `state.slashings` list tracks the total effective balance of all validators slashed during the last `EPOCHS_PER_SLASHINGS_VECTOR` epochs.

At a time `EPOCHS_PER_SLASHINGS_VECTOR` `//` `2` after being slashed, a further penalty is applied to the slashed validator, based on the total amount of value slashed during the 4096 epochs before and the 4096 epochs after it was originally slashed.

The idea of this is to disproportionately punish coordinated attacks, in which many validators break the slashing conditions around the same time, while only lightly penalising validators that get slashed by making a mistake. Early designs for Eth2 would always slash a validator's entire deposit.

See also [`PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX`](#proportional_slashing_multiplier).

##### `HISTORICAL_ROOTS_LIMIT`

Every [`SLOTS_PER_HISTORICAL_ROOT`](#slots_per_historical_root) slots, the list of block roots and the list of state roots in the beacon state are Merkleized and added to `state.historical_roots` list. Although `state.historical_roots` is in principle unbounded, all SSZ lists must have maximum sizes specified. The size `HISTORICAL_ROOTS_LIMIT` will be fine for the next few millennia, after which it will be somebody else's problem. The list grows at less than 10 KB per year.

Storing past roots like this allows Merkle proofs to be constructed about anything in the beacon chain's history if required.

##### `VALIDATOR_REGISTRY_LIMIT`

Every time the Eth1 deposit contract processes a deposit from a new validator (as identified by its public key), a new entry is appended to the `state.validators` list.

In the current design, validators are never removed from this list, even after exiting from being a validator. This is largely because there is nowhere yet to send a validator's remaining deposit and staking rewards, so they continue to need to be tracked in the beacon chain.

The maximum length of this list is `VALIDATOR_REGISTRY_LIMIT`, which is one trillion, so we ought to be OK for a while, especially given that the minimum deposit amount is 1 Ether.

#### Rewards and penalties

| Name | Value |
| - | - |
| `BASE_REWARD_FACTOR` | `uint64(2**6)` (= 64) |
| `WHISTLEBLOWER_REWARD_QUOTIENT` | `uint64(2**9)` (= 512) |
| `PROPOSER_REWARD_QUOTIENT` | `uint64(2**3)` (= 8) |
| `INACTIVITY_PENALTY_QUOTIENT` | `uint64(2**26)` (= 67,108,864) |
| `MIN_SLASHING_PENALTY_QUOTIENT` | `uint64(2**7)` (= 128) |
| `PROPORTIONAL_SLASHING_MULTIPLIER` | `uint64(1)` |
| `INACTIVITY_PENALTY_QUOTIENT_ALTAIR` | `uint64(3 * 2**24)` (= 50,331,648) |
| `MIN_SLASHING_PENALTY_QUOTIENT_ALTAIR` | `uint64(2**6)` (= 64) |
| `PROPORTIONAL_SLASHING_MULTIPLIER_ALTAIR` | `uint64(2)` |
| `INACTIVITY_PENALTY_QUOTIENT_BELLATRIX` | `uint64(2**24)` (= 16,777,216) |
| `MIN_SLASHING_PENALTY_QUOTIENT_BELLATRIX` | `uint64(2**5)` (= 32) |
| `PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX` | `uint64(3)` |

Note that there are similar constants with different values here.

  - The original beacon chain Phase&nbsp;0 constants have no suffix.
  - Constants updated in the Altair upgrade have the suffix `_ALTAIR`.
  - Constants updated in the Bellatrix upgrade have the suffix `_BELLATRIX`.

This is [explained](https://github.com/ethereum/consensus-specs/tree/v1.3.0/configs#forking) in the specs repo as follows:

> Variables are not replaced but extended with forks. This is to support syncing from one state to another over a fork boundary, without hot-swapping a config. Instead, for forks that introduce changes in a variable, the variable name is suffixed with the fork name.

##### `BASE_REWARD_FACTOR`

This is the big knob to turn to change the issuance rate of Eth2. Almost all validator rewards are calculated in terms of a "base reward per increment" which is [formulated as](/part3/transition/epoch/#def_get_base_reward_per_increment),

```code
  EFFECTIVE_BALANCE_INCREMENT * BASE_REWARD_FACTOR // integer_squareroot(get_total_active_balance(state))
```

Thus, the total validator rewards per epoch (the Eth2 issuance rate) could be tuned by increasing or decreasing `BASE_REWARD_FACTOR`.

The exception is proposer rewards for including slashing reports in blocks. However, these are more than offset by the amount of stake burnt, so do not increase the overall issuance rate.

##### `WHISTLEBLOWER_REWARD_QUOTIENT`

[TODO: link to some explanation of WB process]::

One reward that is not tied to the base reward is the whistleblower reward. This is an amount awarded to the proposer of a block containing one or more proofs that a proposer or attester has violated a slashing condition. The whistleblower reward is set at $\frac{1}{512}$ of the effective balance of the slashed validator.

The whistleblower reward comes from new issuance of Ether on the beacon chain, but is more than offset by the Ether burned due to slashing penalties.

##### `PROPOSER_REWARD_QUOTIENT`

`PROPOSER_REWARD_QUOTIENT` was removed in the Altair upgrade in favour of [`PROPOSER_WEIGHT`](/part3/config/constants/#incentivization-weights). It was used to apportion rewards between attesters and proposers when including attestations in blocks.

<a id="inactivity_penalty_quotient"></a>

##### `INACTIVITY_PENALTY_QUOTIENT_BELLATRIX`

This value supersedes `INACTIVITY_PENALTY_QUOTIENT` and `INACTIVITY_PENALTY_QUOTIENT_ALTAIR`.

If the beacon chain hasn't finalised a checkpoint for longer than [`MIN_EPOCHS_TO_INACTIVITY_PENALTY`](#min_epochs_to_inactivity_penalty) epochs, then it enters "leak" mode. In this mode, any validator that does not vote (or votes for an incorrect target) is penalised an amount each epoch of `(effective_balance * inactivity_score) // (` `INACTIVITY_SCORE_BIAS` ` * ` `INACTIVITY_PENALTY_QUOTIENT_BELLATRIX` `)`.

Since the Altair upgrade, `inactivity_score` has become a per-validator quantity, whereas previously validators were penalised by a globally calculated amount when they missed a duty during a leak. See [inactivity penalties](/part3/config/configuration/#inactivity-penalties) for more on the rationale for this and how this score is calculated per validator.

During a leak, no validators receive rewards, and they continue to accrue the normal penalties when they fail to fulfil duties. In addition, for epochs in which validators do not make a correct, timely target vote, they receive a leak penalty.

To examine the effect of the leak on a single validator's balance, assume that during a period of inactivity leak (non-finalisation) the validator is completely offline. At each epoch, the offline validator will be penalised an extra amount $nB / \alpha$, where $n$ is the number of epochs since the leak started, $B$ is the validator's effective balance, and $\alpha$ is the prevailing inactivity penalty quotient (currently `INACTIVITY_PENALTY_QUOTIENT_BELLATRIX`).

The effective balance $B$ will remain constant for a while, by design, during which time the total amount of the penalty after $n$ epochs would be $n(n+1)B / 2\alpha$. This is sometimes called the "quadratic leak" since it grows as $n^2$ to first order. If $B$ were continuously variable, the penalty would satisfy $\frac{dB}{dt}=-\frac{Bt}{\alpha}$, which can be solved to give $B(t)=B_0e^{-t^2/2\alpha}$. The actual behaviour is somewhere between these two (piecewise quadratic) since the effective balance is neither constant nor continuously variable but decreases in a step-wise fashion.

In the continuous approximation, the inactivity penalty quotient, $\alpha$, is the square of the time it takes to reduce the balance of a non-participating validator to $1 / \sqrt{e}$, or around 60.7% of its initial value. With the value of `INACTIVITY_PENALTY_QUOTIENT_BELLATRIX` at `2**24`, this equates to 4096 epochs, or 18.2 days.

The idea for the inactivity leak (aka the quadratic leak) was proposed in the original [Casper FFG paper](https://arxiv.org/abs/1710.09437). The problem it addresses is that, if a large fraction of the validator set were to go offline at the same time, it would not be possible to continue finalising checkpoints, since a majority vote from validators representing 2/3 of the total stake is required for finalisation.

In order to recover, the inactivity leak gradually reduces the stakes of validators who are not making attestations until, eventually, the participating validators control 2/3 of the remaining stake. They can then begin to finalise checkpoints once again.

This inactivity penalty mechanism is designed to protect the chain long-term in the face of catastrophic events (sometimes referred to as the ability to survive World War III). The result might be that the beacon chain could permanently split into two independent chains either side of a network partition, and this is assumed to be a reasonable outcome for any problem that can't be fixed in a few weeks. In this sense, the beacon chain formally prioritises availability over consistency. (You [can't have both](https://en.wikipedia.org/wiki/CAP_theorem).)

The value of `INACTIVITY_PENALTY_QUOTIENT` [was increased](https://github.com/ethereum/consensus-specs/commit/157f7e8ef4be3675543980e68581eb4b73284763) by a factor of four from `2**24` to `2**26` for the beacon chain launch, with the intention of penalising validators less severely in case of non-finalisation due to implementation problems in the early days. As it happens, there were no instances of non-finalisation during the eleven months of Phase&nbsp;0 of the beacon chain.

The value was decreased by one quarter in the Altair upgrade from `2**26` (`INACTIVITY_PENALTY_QUOTIENT`) to `3 * 2**24` (`INACTIVITY_PENALTY_QUOTIENT_ALTAIR`), and to its final value of `2**24` (`INACTIVITY_PENALTY_QUOTIENT_BELLATRIX`) in the Bellatrix upgrade. Decreasing the inactivity penalty quotient speeds up recovery of finalisation in the event of an inactivity leak.

<a id="min_slashing_penalty_quotient"></a>

##### `MIN_SLASHING_PENALTY_QUOTIENT_BELLATRIX`

When a validator is first convicted of a slashable offence, an initial penalty is applied. This is calculated as, `validator.effective_balance` ` // ` `MIN_SLASHING_PENALTY_QUOTIENT_BELLATRIX`.

Thus, the initial slashing penalty is between 0.5&nbsp;ETH and 1&nbsp;ETH depending on the validator's effective balance (which is between 16 and 32 Ether; note that effective balance is denominated in Gwei).

A further slashing penalty is applied later based on the total amount of balance slashed during a period of [`EPOCHS_PER_SLASHINGS_VECTOR`](#epochs_per_slashings_vector).

The value of `MIN_SLASHING_PENALTY_QUOTIENT` [was increased](https://github.com/ethereum/consensus-specs/commit/157f7e8ef4be3675543980e68581eb4b73284763) by a factor of four from `2**5` to `2**7` for the beacon chain launch, anticipating that unfamiliarity with the rules of Ethereum&nbsp;2.0 staking was likely to result in some unwary users getting slashed. In the event, a total of 157 validators were slashed during Phase&nbsp;0, all as a result of user error or misconfiguration as far as can be determined.

The value of this parameter was halved in the Altair upgrade from `2**7` (`MIN_SLASHING_PENALTY_QUOTIENT`) to `2**6` (`MIN_SLASHING_PENALTY_QUOTIENT_ALTAIR`), and set to its final value of `2**5` (`MIN_SLASHING_PENALTY_QUOTIENT_BELLATRIX`) in the Bellatrix upgrade.

<a id="proportional_slashing_multiplier"></a>

##### `PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX`

When a validator has been slashed, a further penalty is later applied to the validator based on how many other validators were slashed during a window of size [`EPOCHS_PER_SLASHINGS_VECTOR`](#epochs_per_slashings_vector) epochs centred on that slashing event (approximately 18 days before and after).

The proportion of the validator's remaining effective balance that will be subtracted [is calculated](/part3/transition/epoch/#slashings) as, `PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX` multiplied by the sum of the effective balances of the slashed validators in the window, divided by the total effective balance of all validators. The idea of this mechanism is to punish accidents lightly (in which only a small number of validators were slashed) and attacks heavily (where many validators coordinated to double vote).

To finalise conflicting checkpoints, at least a third of the balance must have voted for both. That's why the "natural" setting of `PROPORTIONAL_SLASHING_MULTIPLIER` is three: in the event of an attack that finalises conflicting checkpoints, the attackers lose their entire stake. This provides "the maximal minimum accountable safety margin".

However, for the initial stage of the beacon chain, Phase&nbsp;0, `PROPORTIONAL_SLASHING_MULTIPLIER` was set to one. It was increased to two at the Altair upgrade, and to its final value of three at the Bellatrix upgrade. The lower values provided some insurance against client bugs that might have caused mass slashings in the early days.

#### Max operations per block

<a id="max_proposer_slashings"></a>
<a id="max_attester_slashings"></a>
<a id="max_attestations"></a>
<a id="max_deposits"></a>
<a id="max_voluntary_exits"></a>
<a id="max_bls_to_execution_changes"></a>

| Name | Value |
| - | - |
| `MAX_PROPOSER_SLASHINGS`       | `2**4` (= 16)  |
| `MAX_ATTESTER_SLASHINGS`       | `2**1` (= 2)   |
| `MAX_ATTESTATIONS`             | `2**7` (= 128) |
| `MAX_DEPOSITS`                 | `2**4` (= 16)  |
| `MAX_VOLUNTARY_EXITS`          | `2**4` (= 16)  |
| `MAX_BLS_TO_EXECUTION_CHANGES` | `2**4` (= 16)  |

These parameters are used to size lists in the beacon block bodies for the purposes of SSZ serialisation, as well as constraining the maximum size of beacon blocks so that they can propagate efficiently, and avoid DoS attacks.

Some comments on the chosen values:

  - I have suggested [elsewhere](https://github.com/ethereum/consensus-specs/issues/2152) reducing `MAX_DEPOSITS` from sixteen to one to ensure that more validators must process deposits, which encourages them to run Eth1 clients.
  - At first sight, there looks to be a disparity between the number of proposer slashings and the number of attester slashings that may be included in a block. But note that an attester slashing (a) can be much larger than a proposer slashing, and (b) can result in many more validators getting slashed than a proposer slashing.
  - `MAX_ATTESTATIONS` is double the value of [`MAX_COMMITTEES_PER_SLOT`](#max_committees_per_slot). This allows there to be an empty slot (with no block proposal), yet still include all the attestations for the empty slot in the next slot. Since, ideally, each committee produces a single aggregate attestation, a block can hold two slots' worth of aggregate attestations.

#### Sync committee

<a id="sync_committee_size"></a>
<a id="epochs_per_sync_committee_period"></a>

| Name | Value | Unit | Duration |
| - | - | - | - |
| `SYNC_COMMITTEE_SIZE` | `uint64(2**9)` (= 512) | Validators | |
| `EPOCHS_PER_SYNC_COMMITTEE_PERIOD` | `uint64(2**8)` (= 256) | epochs | ~27 hours |

[TODO: Link to sync committees section]::

Sync committees were introduced by the Altair upgrade to allow light clients to quickly and trustlessly determine the head of the beacon chain.

Why did we need a new committee type? Couldn't this be built on top of existing committees, say committees 0 to 3 at a slot? After all, voting for the head of the chain is already one of their duties. The reason is that it is important for reducing the load on light clients that sync committees do not change very often. Standard committees change every slot; we need something much longer lived here.

Only a single sync committee is active at any one time, and contains a randomly selected subset of size `SYNC_COMMITTEE_SIZE` of the total validator set.

A sync committee does its duties (and receives rewards for doing so) for only `EPOCHS_PER_SYNC_COMMITTEE_PERIOD` epochs until the next committee takes over.

<!-- Number of validators -->

With 500,000 validators, the expected time between being selected for sync committee duty is around 37 months. The probability of being in the current sync committee would be $\frac{512}{500{,}000}$ per validator.

`SYNC_COMMITTEE_SIZE` is a [trade-off](https://github.com/ethereum/consensus-specs/pull/2130) between [security](https://notes.ethereum.org/iMxxlEkuQMiPkEL1S6SfbQ) (ensuring that enough honest validators are always present) and efficiency for light clients (ensuring that they do not have to handle too much computation). The value 512 is conservative in terms of safety. It would be catastrophic for trustless bridges to other protocols, for example, if a sync committee voted in an invalid block.

`EPOCHS_PER_SYNC_COMMITTEE_PERIOD` is around a day, and again is a trade-off between security (short enough that it's hard for an attacker to find and corrupt committee members) and efficiency (reducing the data load on light clients).

#### Execution

<a id="max_bytes_per_transaction"></a>
<a id="max_transactions_per_payload"></a>
<a id="bytes_per_logs_bloom"></a>
<a id="max_extra_data_bytes"></a>
<a id="max_withdrawals_per_payload"></a>

| Name | Value |
| - | - |
| `MAX_BYTES_PER_TRANSACTION` | `uint64(2**30)` (= 1,073,741,824) |
| `MAX_TRANSACTIONS_PER_PAYLOAD` | `uint64(2**20)` (= 1,048,576) |
| `BYTES_PER_LOGS_BLOOM` | `uint64(2**8)` (= 256) |
| `MAX_EXTRA_DATA_BYTES` | `2**5` (= 32) |
| `MAX_WITHDRAWALS_PER_PAYLOAD` | `uint64(2**4)` (= 16) |

These first four of these constants were introduced at the Bellatrix pre-Merge upgrade and are used only to size some fields within the [`ExecutionPayload`](/part3/containers/execution/#executionpayload) class.

The execution payload (formerly known as an Eth1 block) contains a list of up to `MAX_TRANSACTIONS_PER_PAYLOAD` normal Ethereum transactions. Each of these has size up to `MAX_BYTES_PER_TRANSACTION`. These constants are needed only because [SSZ list types](/part2/building_blocks/ssz/#lists) require a maximum size to be specified. They are set ludicrously large, but that's not a problem in practice.

`BYTES_PER_LOGS_BLOOM` and `MAX_EXTRA_DATA_BYTES` are a direct carry-over from Eth1 blocks as specified in the [Yellow Paper](https://ethereum.org/615606b8e1e1da72687e66dba79771e9/yellow-paper-berlin.pdf), being the size of a block's Bloom filter and the size of a block's extra data field respectively. The execution payload's extra data is analogous to a beacon block's graffiti - the block builder can set it to any value they choose.

`MAX_WITHDRAWALS_PER_PAYLOAD` was introduced at the [Capella upgrade](/part4/history/capella/). It is the maximum number of withdrawals that the consensus client will ask the execution client to include in an execution payload. As a consequence, the rate of withdrawals is limited to `MAX_WITHDRAWALS_PER_PAYLOAD` per slot.

#### Withdrawals processing

<a id="max_validators_per_withdrawals_sweep"></a>

| Name | Value |
| - | - |
| `MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP` | `16384` (= 2**14 ) |

This preset constant was introduced in the [Capella upgrade](/part4/history/capella/) to bound the amount of work each node must do when processing withdrawals.

The number of withdrawal transactions per block is bounded at [`MAX_WITHDRAWALS_PER_PAYLOAD`](#max_withdrawals_per_payload). But not all validators will be eligible for a withdrawal transaction, meaning that nodes might have to search indefinitely through the validator set to find enough withdrawals to include. Searching the validator set can be an expensive operation, therefore we [bound the search](/part3/transition/block/#def_get_expected_withdrawals), considering only `MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP` validators per block. If we find fewer withdrawable validators than `MAX_WITHDRAWALS_PER_PAYLOAD` then we make fewer withdrawal transactions.

The primary reason that a validator might not be withdrawable is that it still has an old `0x00` [BLS withdrawal credential](/part3/config/constants/#withdrawal-prefixes). At least in the early days of Capella this could have led to long stretches of the validator set that were not withdrawable, before many validators had updated their credentials.

Another reason might be having an effective balance lower than [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance). In the event of an [inactivity leak](/part2/incentives/inactivity/) this could also lead to long stretches of validators being non-withdrawable.

### Configuration <!-- /part3/config/configuration/ -->

#### Genesis Settings

Beacon chain genesis is long behind us. Nevertheless, the ability to spin-up testnets is useful in all sorts of scenarios, so the spec retains genesis functionality, now called [initialisation](/part3/initialise/#initialise-state).

The following parameters refer to the actual mainnet beacon chain genesis, and I'll explain them in that context. When starting up new testnets, these will of course be changed. For example, see the configuration file for the [Prater testnet](https://github.com/eth2-clients/eth2-networks/blob/274e71c7af8fb26f65b47016ffa6169079315e2c/shared/prater/config.yaml).

| Name | Value |
| - | - |
| `MIN_GENESIS_ACTIVE_VALIDATOR_COUNT` | `uint64(2**14)` (= 16,384) |
| `MIN_GENESIS_TIME` | `uint64(1606824000)` (Dec 1, 2020, 12pm UTC) |
| `GENESIS_FORK_VERSION` | `Version('0x00000000')` |
| `GENESIS_DELAY` | `uint64(604800)` (7 days) |

##### `MIN_GENESIS_ACTIVE_VALIDATOR_COUNT`

`MIN_GENESIS_ACTIVE_VALIDATOR_COUNT` is the minimum number of full validator stakes that must have been deposited before the beacon chain can start producing blocks. The number is chosen to ensure a degree of security. It allows for four 128 member committees per slot, rather than the 64 committees per slot eventually desired to support the fully operational data shards that were on the roadmap at that time (but no longer). Fewer validators means higher rewards per validator, so it is designed to attract early participants to get things bootstrapped.

`MIN_GENESIS_ACTIVE_VALIDATOR_COUNT` used to be much higher (65,536 = 2 million Ether staked), but was reduced when `MIN_GENESIS_TIME`, below, was added.

In the actual event of beacon chain genesis, there were 21,063 participating validators, comfortably exceeding the minimum necessary count.

##### `MIN_GENESIS_TIME`

`MIN_GENESIS_TIME` is the earliest date that the beacon chain can start.

Having a `MIN_GENESIS_TIME` allows us to start the chain with fewer validators than was previously thought necessary. The previous plan was to start the chain as soon as there were `MIN_GENESIS_ACTIVE_VALIDATOR_COUNT` validators staked. But there were concerns that with a lowish initial validator count, a single entity could form the majority of them and then act to prevent other validators from entering (a "[gatekeeper attack](https://github.com/ethereum/consensus-specs/pull/1467)"). A minimum genesis time allows time for all those who wish to make deposits to do so before they could be excluded by a gatekeeper attack.

The beacon chain actually started at 12:00:23 UTC on the 1st of December 2020. The extra 23 seconds comes from the timestamp of the first Eth1 block to meet the [genesis criteria](/part3/initialise/#genesis-state), [block 11320899](https://etherscan.io/block/11320899). I like to think of this as a little remnant of proof of work forever embedded in the beacon chain's history.

##### `GENESIS_FORK_VERSION`

Unlike Ethereum&nbsp;1.0, the beacon chain gives in-protocol versions to its forks. See the [Version custom type](/part3/config/types/#version) for more explanation.

`GENESIS_FORK_VERSION` is the fork version the beacon chain starts with at its "genesis" event: the point at which the chain first starts producing blocks. Nowadays, this value is used only when [computing](/part3/helper/misc/#compute_domain) the cryptographic domain for deposit messages and BLS credential change messages, which are valid across all forks.

Fork versions and timings for the [Altair](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/fork.md#configuration), [Bellatrix](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/fork.md#configuration), and [Capella](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/capella/fork.md#configuration) upgrades are defined in their respective specifications as follows.

<a id="altair_fork_version"></a>
<a id="altair_fork_epoch"></a>
<a id="bellatrix_fork_version"></a>
<a id="bellatrix_fork_epoch"></a>
<a id="capella_fork_version"></a>
<a id="capella_fork_epoch"></a>

| Name | Value |
| - | - |
| `ALTAIR_FORK_VERSION`    | `Version('0x01000000')` |
| `ALTAIR_FORK_EPOCH`      | `Epoch(74240)` (Oct 27, 2021, 10:56:23am UTC) |
| `BELLATRIX_FORK_VERSION` | `Version('0x02000000')` |
| `BELLATRIX_FORK_EPOCH`   | `Epoch(144896)` (Sept 6, 2022, 11:34:47am UTC) |
| `CAPELLA_FORK_VERSION`   | `Version('0x03000000')` |
| `CAPELLA_FORK_EPOCH`     | `Epoch(194048)` (April 12, 2023, 10:27:35pm UTC) |

##### `GENESIS_DELAY`

The `GENESIS_DELAY` is a grace period to allow nodes and node operators time to prepare for the genesis event. The genesis event cannot occur before [`MIN_GENESIS_TIME`](#min_genesis_time). If [`MIN_GENESIS_ACTIVE_VALIDATOR_COUNT`](#min_genesis_active_validator_count) validators are not registered sufficiently in advance of `MIN_GENESIS_TIME`, then Genesis will occur `GENESIS_DELAY` seconds after enough validators have been registered.

Seven days' notice was regarded as sufficient to allow client dev teams time to make a release once the genesis parameters were known, and for node operators to upgrade to that release. And, of course, to organise some parties. It was increased from 2 days over time due to lessons learned on some of the pre-genesis testnets.

#### Time parameters

| Name                                  | Value                     | Unit        | Duration   |
|---------------------------------------|---------------------------|-------------|------------|
| `SECONDS_PER_SLOT`                    | `uint64(12)`              | seconds     | 12 seconds |
| `SECONDS_PER_ETH1_BLOCK`              | `uint64(14)`              | seconds     | 14 seconds |
| `MIN_VALIDATOR_WITHDRAWABILITY_DELAY` | `uint64(2**8)` (= 256)    | epochs      | ~27 hours  |
| `SHARD_COMMITTEE_PERIOD`              | `uint64(2**8)` (= 256)    | epochs      | ~27 hours  |
| `ETH1_FOLLOW_DISTANCE`                | `uint64(2**11)` (= 2,048) | Eth1 blocks | ~8 hours   |

##### `SECONDS_PER_SLOT`

This was originally six seconds, but [is now twelve](https://github.com/ethereum/consensus-specs/pull/1428#issue-327424983), and has been [other values](https://github.com/ethereum/consensus-specs/pull/143/files#diff-51a43328a58414e132a744f3771f018cL42) in between.

Network delays are the main limiting factor in shortening the slot length. Three communication activities need to be accomplished within a slot, and it is supposed that four seconds is enough for the vast majority of nodes to have participated in each:

1. Blocks are proposed at the start of a slot and should have propagated to most of the network within the first four seconds.
2. At four seconds into a slot, committee members create and broadcast attestations, including attesting to this slot's block. During the next four seconds, these attestations are collected by aggregators in each committee.
3. At eight seconds into the slot, the aggregators broadcast their aggregate attestations which then have four seconds to reach the validator who is proposing the next block.

[TODO: find this discussion and link to it]::

There is a general intention to shorten the slot time in future, perhaps to [8 seconds](https://github.com/ethereum/consensus-specs/issues/1890#issue-638024803), if it proves possible to do this in practice. Or perhaps to lengthen it to [16 seconds](https://ethresear.ch/t/two-slot-proposer-builder-separation/10980?u=benjaminion).

Post-Merge, the time taken by the execution client to validate the execution payload contents (that is, the normal Ethereum transactions) is now on the critical path for validators during step 1, the first four seconds. In order for the validator to attest correctly, the beacon block must first be broadcast, propagated and received, then validated by the consensus client, and also validated by the execution client, all within that initial four-second window. In borderline cases, the extra time taken by execution validation can push the whole process beyond the four-second point at which attestations must be made. This can lead to voting incorrectly for an empty slot. See Adrian Sutton's article [Understanding Attestation Misses](https://symphonious.net/2022/09/25/understanding-attestation-misses/) for further explanation.

##### `SECONDS_PER_ETH1_BLOCK`

The assumed block interval on the Eth1 chain, used in conjunction with [`ETH1_FOLLOW_DISTANCE`](#eth1_follow_distance) when considering blocks on the Eth1 chain, either at genesis, or when voting on the deposit contract state.

The [average Eth1 block time](https://etherscan.io/chart/blocktime) since January 2020 has actually been nearer 13 seconds, but never mind. The net effect is that we will be going a little deeper back in the Eth1 chain than [`ETH1_FOLLOW_DISTANCE`](#eth1_follow_distance) would suggest, which ought to be safer.

##### `MIN_VALIDATOR_WITHDRAWABILITY_DELAY`

A validator can stop participating once it has made it through the exit queue. However, its stake remains locked for the duration of `MIN_VALIDATOR_WITHDRAWABILITY_DELAY`. This is to allow some time for any slashable behaviour to be detected and reported so that the validator can still be penalised (in which case the validator's withdrawable time is pushed [`EPOCHS_PER_SLASHINGS_VECTOR`](/part3/config/preset/#epochs_per_slashings_vector) into the future).

Once the `MIN_VALIDATOR_WITHDRAWABILITY_DELAY` period has passed, the validator becomes eligible for a full withdrawal of its stake and rewards on the next withdrawals sweep, as long as it has [`ETH1_ADDRESS_WITHDRAWAL_PREFIX`](/part3/config/constants/#eth1_address_withdrawal_prefix) (`0x01`) withdrawal credentials set. In any case, being in a "withdrawable" state means that a validator has now fully exited from the protocol.

##### `SHARD_COMMITTEE_PERIOD`

This really anticipates the implementation of data shards, which is no longer planned, at least in its originally envisaged form. The [idea is](https://github.com/ethereum/consensus-specs/issues/675#issuecomment-468159678) that it's bad for the stability of longer-lived committees if validators can appear and disappear very rapidly. Therefore, a validator cannot initiate a voluntary exit until `SHARD_COMMITTEE_PERIOD` epochs after it has been activated. However, it could still be ejected by slashing before this time.

##### `ETH1_FOLLOW_DISTANCE`

This is used to calculate the minimum depth of block on the Ethereum&nbsp;1 chain that can be considered by the Eth2 chain: it applies to the [Genesis](/part3/initialise/#initialise-state) process and the [processing of deposits](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#process-deposit) by validators.  The Eth1 chain depth is estimated by multiplying this value by the target average Eth1 block time, [`SECONDS_PER_ETH1_BLOCK`](#seconds_per_eth1_block).

The value of `ETH1_FOLLOW_DISTANCE` is not based on the expected depth of any reorgs of the Eth1 chain, which are rarely if ever more than 2-3 blocks deep. It is about providing time to respond to an incident on the Eth1 chain such as a consensus failure between clients.

This parameter [was increased](https://github.com/ethereum/consensus-specs/pull/2093/files) from 1024 to 2048 blocks for the beacon chain mainnet, to allow devs more time to respond if there were any trouble on the Eth1 chain.

The whole follow distance concept has been made redundant by the Merge and may be removed in a future upgrade, so that validators can make deposits and become active more-or-less instantly.

#### Validator Cycle

| Name | Value |
| - | - |
| `EJECTION_BALANCE` | `Gwei(2**4 * 10**9)` (= 16,000,000,000) |
| `MIN_PER_EPOCH_CHURN_LIMIT` | `uint64(2**2)` (= 4) |
| `CHURN_LIMIT_QUOTIENT` | `uint64(2**16)` (= 65,536) |

##### `EJECTION_BALANCE`

If a validator's effective balance falls to 16 Ether or below then it is exited from the system. This is most likely to happen as a result of the ["inactivity leak"](/part3/config/preset/#inactivity_penalty_quotient), which gradually reduces the balances of inactive validators in order to maintain the liveness of the beacon chain.

This mechanism is intended to protect stakers who no longer have access to their keys. If a validator has been offline for long enough to lose half of its balance, it is unlikely to be coming back. To save the staker from losing everything, we choose to eject the validator before its balance reaches zero.

Note that the dependence on effective balance means that the validator is queued for ejection as soon as its actual balance falls to 16.75 Ether.

##### `MIN_PER_EPOCH_CHURN_LIMIT`

Validators are allowed to exit the system and cease validating, and new validators may apply to join at any time. For [interesting reasons](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Exiting), a design decision was made to apply a rate-limit to entries (activations) and exits. Basically, it is important in proof of stake protocols that the validator set not change too quickly.

In the normal case, a validator is able to exit fairly swiftly: it just needs to wait [`MAX_SEED_LOOKAHEAD`](/part3/config/preset/#max_seed_lookahead) (currently four) epochs. However, if a large number of validators wishes to exit at the same time, a queue forms with a limited number of exits allowed per epoch. The minimum number of exits per epoch (the minimum "churn") is `MIN_PER_EPOCH_CHURN_LIMIT`, so that validators can always eventually exit. The actual allowed churn per epoch is [calculated](/part3/helper/accessors/#get_validator_churn_limit) in conjunction with `CHURN_LIMIT_QUOTIENT`.

The same applies to new validator activations, once a validator has been marked as eligible for activation.

The rate at which validators can exit is strongly related to the concept of weak subjectivity, and the weak subjectivity period.

[TODO: Link to weak subjectivity discussion when done]::

##### `CHURN_LIMIT_QUOTIENT`

This is used in conjunction with `MIN_PER_EPOCH_CHURN_LIMIT` to [calculate](/part3/helper/accessors/#get_validator_churn_limit) the actual number of validator exits and activations allowed per epoch. The number of exits allowed is `max(MIN_PER_EPOCH_CHURN_LIMIT, n // CHURN_LIMIT_QUOTIENT)`, where `n` is the number of active validators. The same applies to activations.

#### Inactivity penalties

| Name | Value | Description |
| - | - | - |
| `INACTIVITY_SCORE_BIAS` | `uint64(2**2)` (= 4) | score points per inactive epoch |
| `INACTIVITY_SCORE_RECOVERY_RATE` | `uint64(2**4)` (= 16) | score points per leak-free epoch |

##### `INACTIVITY_SCORE_BIAS`

If the beacon chain hasn't finalised an epoch for longer than [`MIN_EPOCHS_TO_INACTIVITY_PENALTY`](/part3/config/preset/#min_epochs_to_inactivity_penalty) epochs, then it enters "leak" mode. In this mode, any validator that does not vote (or votes for an incorrect target) is penalised an amount each epoch of `(effective_balance * inactivity_score) // (INACTIVITY_SCORE_BIAS * INACTIVITY_PENALTY_QUOTIENT_BELLATRIX)`. See [`INACTIVITY_PENALTY_QUOTIENT_BELLATRIX`](/part3/config/preset/#inactivity_penalty_quotient) for discussion of the inactivity leak itself.

The per-validator `inactivity-score` was introduced in the Altair upgrade. During Phase&nbsp;0, inactivity penalties were an increasing global amount applied to all validators that did not participate in an epoch, regardless of their individual track records of participation. So a validator that was able to participate for a significant fraction of the time nevertheless could be quite severely penalised due to the growth of the per-epoch inactivity penalty. Vitalik gives a simplified [example](https://github.com/ethereum/consensus-specs/issues/2125#issue-737768917): "if fully [off]line validators get leaked and lose 40% of their balance, someone who has been trying hard to stay online and succeeds at 90% of their duties would still lose 4% of their balance. Arguably this is unfair."

In addition, if many validators are able to participate intermittently, it indicates that whatever event has befallen the chain is potentially recoverable (unlike a permanent network partition, or a super-majority network fork, for example). The inactivity leak is intended to bring finality to irrecoverable situations, so prolonging the time to finality if it's not irrecoverable is likely a good thing.

Each validator has an individual inactivity score in the beacon state which is updated by [`process_inactivity_updates()`](/part3/transition/epoch/#def_process_inactivity_updates) as follows.

  - Every epoch, irrespective of the inactivity leak,
    - decrease the score by one when the validator makes a correct timely target vote, and
    - increase the score by `INACTIVITY_SCORE_BIAS` otherwise.
  - When _not_ in an inactivity leak
    - decrease every validator's score by `INACTIVITY_SCORE_RECOVERY_RATE`.

There is a floor of zero on the score. So, outside a leak, validators' scores will rapidly return to zero and stay there, since `INACTIVITY_SCORE_RECOVERY_RATE` is greater than `INACTIVITY_SCORE_BIAS`.

When in a leak, if $p$ is the participation rate between $0$ and $1$, and $\lambda$ is `INACTIVITY_SCORE_BIAS`, then the expected score after $N$ epochs is $\max (0, N((1-p)\lambda - p))$. For $\lambda = 4$ this is $\max (0, N(4 - 5p))$. So a validator that is participating 80% of the time or more can maintain a score that is bounded near zero. With less than 80% average participation, its score will increase unboundedly.

##### `INACTIVITY_SCORE_RECOVERY_RATE`

When not in an inactivity leak, validators' inactivity scores are reduced by `INACTIVITY_SCORE_RECOVERY_RATE` ` + ` `1` per epoch when they make a timely target vote, and by `INACTIVITY_SCORE_RECOVERY_RATE` ` - ` `INACTIVITY_SCORE_BIAS` when they don't. So, even for non-performing validators, scores decrease three times faster than they increase.

The new scoring system means that some validators will continue to be penalised due to the leak, even after finalisation starts again. This is [intentional](https://github.com/ethereum/consensus-specs/issues/2098). When the leak causes the beacon chain to finalise, at that point we have just 2/3 of the stake online. If we immediately stop the leak (as we used to), then the amount of stake online would remain close to 2/3 and the chain would be vulnerable to flipping in and out of finality as small numbers of validators come and go. We saw this behaviour on some of the testnets prior to launch. Continuing the leak after finalisation serves to increase the balances of participating validators to greater than 2/3, providing a margin that should help to prevent such behaviour.

See the section on the [Inactivity Leak](/part2/incentives/inactivity/) for some more analysis of the inactivity score and some graphs of its effect.

#### Transition settings

| Name | Value |
| - | - |
| `TERMINAL_TOTAL_DIFFICULTY` | `58750000000000000000000` |
| `TERMINAL_BLOCK_HASH` | `Hash32()` |
| `TERMINAL_BLOCK_HASH_ACTIVATION_EPOCH` | `FAR_FUTURE_EPOCH` |

These values are not used in the main beacon chain specification, but are used in the Bellatrix [fork choice](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/fork-choice.md) and [validator guide](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/validator.md) to determine the point of handover from proof of work to proof of stake for the execution chain.

All previous upgrades to the Ethereum proof of work chain took place at a pre-defined block height. That approach was deemed to be insecure for the Merge due to the irreversible dynamics of the switch to proof of stake. The rationale is given in the Security Considerations section of [EIP-3675](https://eips.ethereum.org/EIPS/eip-3675).

> Using a pre-defined block number for the hardfork is unsafe in this context due to the PoS fork choice taking priority during the transition.
>
> An attacker may use a minority of hash power to build a malicious chain fork that would satisfy the block height requirement. Then the first PoS block may be maliciously proposed on top of the PoW block from this adversarial fork, becoming the head and subverting the security of the transition.
>
> To protect the network from this attack scenario, difficulty accumulated by the chain (total difficulty) is used to trigger the upgrade.

Thus, the Bellatrix upgrade defined a terminal total difficulty (TTD) at which the Merge would take place. Each block on the Ethereum proof of work chain has a "difficulty" associated with it, which corresponds to the expected number of hashes it would take to mine it. The total difficulty is the monotonically increasing accumulated difficulty of all the blocks so far.

The first block to exceed `TERMINAL_TOTAL_DIFFICULTY` was Ethereum block number [15537393](https://etherscan.io/block/15537393). That block became the last canonical block to be produced under proof of work. The [next execution payload](https://etherscan.io/block/15537394) was included in the beacon chain at slot [4700013](https://beaconcha.in/slot/4700013), which was produced at 06:42:59 UTC on September the 15th, 2022.

`TERMINAL_BLOCK_HASH` and `TERMINAL_BLOCK_HASH_ACTIVATION_EPOCH` are present in case a need arose to manually select a particular proof of work fork to follow in case of trouble. `TERMINAL_BLOCK_HASH` would have been set in clients, by a manual override or a client update, to point to a specific proof of work block chosen by agreement to be the terminal block. In the event this functionality was not needed.

## Containers <!-- /part3/containers/ -->

### Preamble

We are about to see our first Python code in the executable spec. For specification purposes, these Container data structures are just Python data classes that are derived from the base SSZ `Container` class.

[SSZ](/part2/building_blocks/ssz/) is the serialisation and Merkleization format used everywhere in Ethereum&nbsp;2.0. It is not self-describing, so you need to know ahead of time what you are unpacking when deserialising. SSZ deals with basic types and composite types. Classes like the below are handled as SSZ containers, a composite type defined as an "ordered heterogeneous collection of values".

Client implementations in different languages will obviously use their own paradigms to represent these data structures.

Two notes directly from the spec:

  - The definitions are ordered topologically to facilitate execution of the spec.
  - Fields missing in container instantiations default to their [zero value](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#default-values).

### Misc dependencies <!-- /part3/containers/dependencies/ -->

#### `Fork`

```python
class Fork(Container):
    previous_version: Version
    current_version: Version
    epoch: Epoch  # Epoch of latest fork
```

Fork data is stored in the [BeaconState](/part3/containers/state/#beaconstate) to indicate the current and previous fork versions. The fork version gets incorporated into the cryptographic domain in order to invalidate messages from validators on other forks. The previous fork version and the epoch of the change are stored so that pre-fork messages can still be validated (at least until the next fork). This ensures continuity of attestations across fork boundaries.

Note that this is all about planned protocol forks (upgrades), and nothing to do with the fork-choice rule, or inadvertent forks due to errors in the state transition.

#### `ForkData`

```python
class ForkData(Container):
    current_version: Version
    genesis_validators_root: Root
```

`ForkData` is used only in [`compute_fork_data_root()`](/part3/helper/misc/#def_compute_fork_data_root). This is used when distinguishing between chains for the purpose of [peer-to-peer gossip](https://github.com/ethereum/consensus-specs/pull/1652), and for [domain separation](/part3/config/constants/#domain-types). By including both the current fork version and the genesis validators root, we can cleanly distinguish between, say, mainnet and a testnet. Even if they have the same fork history, the genesis validators roots will differ.

[`Version`](/part3/config/types/#version) is the datatype for a fork version number.

#### `Checkpoint`

```python
class Checkpoint(Container):
    epoch: Epoch
    root: Root
```

`Checkpoint`s are the points of justification and finalisation used by the [Casper FFG protocol](https://arxiv.org/pdf/1710.09437.pdf). Validators use them to create [`AttestationData`](#attestationdata) votes, and the status of recent checkpoints is recorded in [`BeaconState`](/part3/containers/state/#beaconstate).

As per the Casper paper, checkpoints contain a height, and a block root. In this implementation of Casper FFG, checkpoints occur whenever the slot number is a multiple of [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch), thus they correspond to `epoch` numbers. In particular, checkpoint $N$ is the first slot of epoch $N$. The [genesis block](/part3/initialise/#genesis-block) is checkpoint 0, and starts off both justified and finalised.

Thus, the `root` element here is the block root of the first block in the `epoch`. (This might be the block root of an earlier block if some slots have been skipped, that is, if there are no blocks for those slots.).

It is very common to talk about justifying and finalising epochs. This is not strictly correct: checkpoints are justified and finalised.

Once a checkpoint has been finalised, the slot it points to and all prior slots will never be reverted.

#### `Validator`

```python
class Validator(Container):
    pubkey: BLSPubkey
    withdrawal_credentials: Bytes32  # Commitment to pubkey for withdrawals
    effective_balance: Gwei  # Balance at stake
    slashed: boolean
    # Status epochs
    activation_eligibility_epoch: Epoch  # When criteria for activation were met
    activation_epoch: Epoch
    exit_epoch: Epoch
    withdrawable_epoch: Epoch  # When validator can withdraw funds
```

This is the data structure that stores most of the information about an individual validator, with only validators' balances and inactivity scores stored elsewhere.

Validators' actual balances are stored separately in the `BeaconState` structure, and only the slowly changing "[effective balance](/part2/incentives/balances/)" is stored here. This is because actual balances are liable to change quite frequently (at least every epoch, and sometimes more frequently): the Merkleization process used to calculate state roots means that only the parts that change need to be recalculated; the roots of unchanged parts can be cached. Separating out the validator balances potentially means that only 1/15th (8/121) as much data needs to be rehashed every epoch compared to storing them here, which is an important optimisation.

For similar reasons, validators' inactivity scores are stored outside validator records as well, as they are also updated every epoch.

A validator's record is [created](/part3/transition/block/#def_get_validator_from_deposit) when its deposit is first processed. Sending multiple deposits does not create multiple validator records: deposits with the same public key are aggregated in one record. Validator records never expire; they are stored permanently, even after the validator has exited the system. Thus, there is a 1:1 mapping between a validator's index in the list and the identity of the validator (validator records are only ever appended to the list).

Also stored in `Validator`:

  - `pubkey` serves as both the unique identity of the validator and the means of cryptographically verifying messages purporting to have been signed by it. The public key is stored raw, unlike in Eth1, where it is hashed to form the account address. This allows public keys to be aggregated for verifying aggregated attestations.
  - Depending on its [prefix](/part3/config/constants/#withdrawal-prefixes), `withdrawal_credentials` might specify an Eth1 account to which withdrawal transactions will be made, or it might be an old-style BLS commitment that needs to be updated before withdrawals can occur for that validator. The withdrawal credentials included in a validator's deposit data are not checked in any way by the consensus layer.
  - `effective_balance` is a topic of its own that we've [touched upon already](/part3/config/preset/#max_effective_balance), and will discuss more fully when we look at [effective balances updates](/part3/transition/epoch/#effective-balances-updates).
  - `slashed` indicates that a validator has been slashed, that is, punished for violating the slashing conditions. A validator can be slashed only once.
  - The remaining values are the epochs in which the validator changed, or is due to change state.

[TODO: link to validator lifecycle chapter]::

A detailed explanation of the stages in a validator's lifecycle is [here](https://notes.ethereum.org/@hww/lifecycle), and we'll be covering it in detail as we work through the beacon chain logic. But, in simplified form, progress is as follows:

  1. A 32&nbsp;ETH deposit has been made on the Ethereum&nbsp;1 chain. No validator record exists yet.
  2. The deposit is processed by the beacon chain at some slot. A validator record is created with all epoch fields set to `FAR_FUTURE_EPOCH`.
  3. At the end of the current epoch, the `activation_eligibility_epoch` is set to the next epoch.
  4. After the epoch `activation_eligibility_epoch` has been finalised, the validator is added to the activation queue by setting its `activation_epoch` appropriately, taking into account the per-epoch [churn limit](/part3/config/configuration/#min_per_epoch_churn_limit) and [`MAX_SEED_LOOKAHEAD`](/part3/config/preset/#max_seed_lookahead).
  5. On reaching `activation_epoch` the validator becomes active, and should carry out its duties.
  6. At any time after [`SHARD_COMMITTEE_PERIOD`](/part3/config/configuration/#shard_committee_period) epochs have passed, a validator may request a voluntary exit. `exit_epoch` is set according to the validator's position in the exit queue and [`MAX_SEED_LOOKAHEAD`](/part3/config/preset/#max_seed_lookahead), and `withdrawable_epoch` is set [`MIN_VALIDATOR_WITHDRAWABILITY_DELAY`](/part3/config/configuration/#min_validator_withdrawability_delay) epochs after that.
  7. From `exit_epoch` onward the validator is no longer active. There is no mechanism for exited validators to rejoin: exiting is permanent.
  8. After `withdrawable_epoch`, the validator's full stake can be withdrawn.

The above does not account for slashing or forced exits due to low balance.

#### `AttestationData`

```python
class AttestationData(Container):
    slot: Slot
    index: CommitteeIndex
    # LMD GHOST vote
    beacon_block_root: Root
    # FFG vote
    source: Checkpoint
    target: Checkpoint
```

The beacon chain relies on a combination of two different consensus mechanisms: LMD GHOST keeps the chain moving, and Casper FFG brings finalisation. These are documented in the [Gasper paper](https://arxiv.org/abs/2003.03052). Attestations from (committees of) validators are used to provide votes simultaneously for each of these consensus mechanisms.

This container is the fundamental unit of attestation data. It provides the following elements.

  - `slot`: each active validator should be making exactly one attestation per epoch. Validators have an assigned slot for their attestation, and it is recorded here for validation purposes.
  - `index`: there can be several committees active in a single slot. This is the number of the committee that the validator belongs to in that slot. It can be used to reconstruct the committee to check that the attesting validator is a member. Ideally, all (or the majority at least) of the attestations in a slot from a single committee will be identical, and can therefore be aggregated into a single aggregate attestation.
  - `beacon_block_root` is the validator's vote on the head block for that slot after locally running the LMD GHOST fork-choice rule. It may be the root of a block from a previous slot if the validator believes that the current slot is empty.
  - `source` is the validator's opinion of the best currently justified checkpoint for the Casper FFG finalisation process.
  - `target` is the validator's opinion of the block at the start of the current epoch, also for Casper FFG finalisation.

This `AttestationData` structure gets wrapped up into several other similar but distinct structures:

  - [`Attestation`](/part3/containers/operations/#attestation) is the form in which attestations normally make their way around the network. It is signed and aggregatable, and the list of validators making this attestation is compressed into a bitlist.
  - [`IndexedAttestation`](#indexedattestation) is used primarily for [attester slashing](/part3/containers/operations/#attesterslashing). It is signed and aggregated, with the list of attesting validators being an uncompressed list of indices.
  - [`PendingAttestation`](#pendingattestation). In Phase&nbsp;0, after having their validity checked during block processing, `PendingAttestation`s were stored in the beacon state pending processing at the end of the epoch. This was reworked in the Altair upgrade and `PendingAttestation`s are no longer used.

#### `IndexedAttestation`

```python
class IndexedAttestation(Container):
    attesting_indices: List[ValidatorIndex, MAX_VALIDATORS_PER_COMMITTEE]
    data: AttestationData
    signature: BLSSignature
```

This is one of the forms in which aggregated attestations &ndash; combined identical attestations from multiple validators in the same committee &ndash; are handled.

[`Attestation`](/part3/containers/operations/#attestation)s and `IndexedAttestation`s contain essentially the same information. The difference being that the list of attesting validators is stored uncompressed in `IndexedAttestation`s. That is, each attesting validator is referenced by its global validator index, and non-attesting validators are not included. To be [valid](/part3/helper/predicates/#is_valid_indexed_attestation), the validator indices must be unique and sorted, and the signature must be an aggregate signature from exactly the listed set of validators.

`IndexedAttestation`s are primarily used when reporting [attester slashing](/part3/containers/operations/#attesterslashing). An `Attestation` can be converted to an `IndexedAttestation` using [`get_indexed_attestation()`](/part3/helper/accessors/#def_get_indexed_attestation).

#### `PendingAttestation`

```python
class PendingAttestation(Container):
    aggregation_bits: Bitlist[MAX_VALIDATORS_PER_COMMITTEE]
    data: AttestationData
    inclusion_delay: Slot
    proposer_index: ValidatorIndex
```

`PendingAttestation`s were removed in the Altair upgrade and now appear only in the process for [upgrading the state](/part4/history/altair/) during the fork. The following is provided for historical reference.

Prior to Altair, `Attestation`s received in blocks were verified then temporarily stored in beacon state in the form of `PendingAttestation`s, pending further processing at the end of the epoch.

A `PendingAttestation` is an [`Attestation`](/part3/containers/operations/#attestation) minus the signature, plus a couple of fields related to reward calculation:

  - `inclusion_delay` is the number of slots between the attestation having been made and it being included in a beacon block by the block proposer. Validators are rewarded for getting their attestations included in blocks, but the reward used to decline in inverse proportion to the inclusion delay. This incentivised swift attesting and communicating by validators.
  - `proposer_index` is the block proposer that included the attestation. The block proposer gets a micro reward for every validator's attestation it includes, not just for the aggregate attestation as a whole. This incentivises efficient finding and packing of aggregations, since the number of aggregate attestations per block is capped.

Taken together, these rewards are designed to incentivise the whole network to collaborate to do efficient attestation aggregation (proposers want to include only well-aggregated attestations; validators want to get their attestations included, so will ensure that they get well aggregated).

This whole mechanism was replaced in the Altair upgrade by [`ParticipationFlags`](/part3/config/types/#participationflags).

#### `Eth1Data`

```python
class Eth1Data(Container):
    deposit_root: Root
    deposit_count: uint64
    block_hash: Hash32
```

Proposers include their view of the Ethereum&nbsp;1 chain in blocks, and this is how they do it. The beacon chain stores these votes up in the [beacon state](/part3/containers/state/#beaconstate) until there is a simple majority consensus, then the winner is committed to beacon state. This is to allow the [processing](/part3/transition/block/#deposits) of Eth1 deposits, and creates a simple "honest-majority" one-way bridge from Eth1 to Eth2. The 1/2 majority assumption for this (rather than 2/3 for committees) is considered safe as the number of validators voting each time is large: [`EPOCHS_PER_ETH1_VOTING_PERIOD`](/part3/config/preset/#epochs_per_eth1_voting_period) `*` [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch) = 64 `*` 32 = 2048.

  - `deposit_root` is the result of the [`get_deposit_root()`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/solidity_deposit_contract/deposit_contract.sol#L80) method of the Eth1 deposit contract after executing the Eth1 block being voted on - it's the root of the (incremental) [Merkle tree of deposits](/part2/deposits-withdrawals/contract/#get_deposit_root).
  - `deposit_count` is the number of deposits in the deposit contract at that point, the result of the [`get_deposit_count`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/solidity_deposit_contract/deposit_contract.sol#L97) method on the contract. This will be equal to or greater than (if there are pending unprocessed deposits) the value of `state.eth1_deposit_index`.
  - `block_hash` is the hash of the Eth1 block being voted for. This doesn't have any current use within the Eth2 protocol, but is "too potentially useful to not throw in there", to quote Danny Ryan.

#### `HistoricalBatch`

```python
class HistoricalBatch(Container):
    block_roots: Vector[Root, SLOTS_PER_HISTORICAL_ROOT]
    state_roots: Vector[Root, SLOTS_PER_HISTORICAL_ROOT]
```

The `HistoricalBatch` container has been superseded by [`HistoricalSummary`](/part3/containers/dependencies/#historicalsummary) in the [Capella upgrade](/part4/history/capella/). It remains in the spec since the `historical_roots` list remains in the [`BeaconState`](/part3/containers/state/#beaconstate), albeit it now frozen forever.

`HistoricalBatch` is no longer used anywhere in the state transition. However, applications validating pre-Capella data against the `historical_roots` list will need to use it.

See [`process_historical_summaries_update()`](/part3/transition/epoch/#def_process_historical_summaries_update) for more discussion of this change.

#### `DepositMessage`

```python
class DepositMessage(Container):
    pubkey: BLSPubkey
    withdrawal_credentials: Bytes32
    amount: Gwei
```

The basic information necessary to either add a validator to the registry, or to top up an existing validator's stake.

`pubkey` is the unique public key of the validator. If it is already present in the registry (the list of validators in beacon state) then `amount` is added to its balance. Otherwise, a new [`Validator`](#validator) entry is appended to the list and credited with `amount`.

See the [`Validator`](#validator) container for more on `withdrawal_credentials`.

There are two protections that `DepositMessages` get at different points.

   1. [`DepositData`](#depositdata) is included in beacon blocks as a [`Deposit`](/part3/containers/operations/#deposit), which adds a Merkle proof that the data has been registered with the Eth1 deposit contract.
   2. When the containing beacon block is processed, deposit messages are stored, pending processing at the end of the epoch, in the beacon state as [`DepositData`](#depositdata). This includes the pending validator's BLS signature so that the authenticity of the `DepositMessage` can be verified before a validator is added.

#### `DepositData`

```python
class DepositData(Container):
    pubkey: BLSPubkey
    withdrawal_credentials: Bytes32
    amount: Gwei
    signature: BLSSignature  # Signing over DepositMessage
```

A signed [`DepositMessage`](#depositmessage). The comment says that the signing is done over `DepositMessage`. What actually happens is that a `DepositMessage` is constructed from the first three fields; the root of that is combined with [`DOMAIN_DEPOSIT`](/part3/config/constants/#domain_deposit) in a [`SigningData`](#signingdata) object; finally the root of this is signed and included in `DepositData`.

#### `BeaconBlockHeader`

```python
class BeaconBlockHeader(Container):
    slot: Slot
    proposer_index: ValidatorIndex
    parent_root: Root
    state_root: Root
    body_root: Root
```

A standalone version of a beacon block header: [`BeaconBlock`](/part3/containers/blocks/#beaconblock)s contain their own header. It is identical to [`BeaconBlock`](/part3/containers/blocks/#beaconblock), except that `body` is replaced by `body_root`. It is `BeaconBlock`-lite.

`BeaconBlockHeader` is stored in beacon state to record the last processed block header. This is used to ensure that we proceed along a continuous chain of blocks that always point to their predecessor[^fn-its-a-blockchain-yo]. See [`process_block_header()`](/part3/transition/block/#def_process_block_header).

[^fn-its-a-blockchain-yo]: It's a blockchain, yo!

The [signed version](/part3/containers/envelopes/#signedbeaconblockheader) is used in [proposer slashings](/part3/containers/operations/#proposerslashing).

#### `SyncCommittee`

```python
class SyncCommittee(Container):
    pubkeys: Vector[BLSPubkey, SYNC_COMMITTEE_SIZE]
    aggregate_pubkey: BLSPubkey
```

Sync committees were introduced in the Altair upgrade to support light clients to the beacon chain protocol. The list of committee members for each of the current and next sync committees is stored in the beacon state. Members are updated every [`EPOCHS_PER_SYNC_COMMITTEE_PERIOD`](/part3/config/preset/#epochs_per_sync_committee_period) epochs by [`get_next_sync_committee()`](/part3/helper/accessors/#def_get_next_sync_committee).

Including the `aggregate_pubkey` of the sync committee is an [optimisation](https://github.com/ethereum/consensus-specs/commit/9c3d5982cfbe9a52b02e2bd028a873c9226a34c9) intended to save light clients some work when verifying the sync committee's signature. All the public keys of the committee members (including any duplicates) are aggregated into this single public key. If any signatures are missing from the [`SyncAggregate`](/part3/containers/operations/#syncaggregate), the light client can "de-aggregate" them by performing elliptic curve subtraction. As long as more than half of the committee contributed to the signature, then this will be faster than constructing the aggregate of participating members from scratch. If less than half contributed to the signature, the light client can start instead with the identity public key and use elliptic curve addition to aggregate those public keys that are present.

See also [`SYNC_COMMITTEE_SIZE`](/part3/config/preset/#sync_committee_size).

#### `SigningData`

```python
class SigningData(Container):
    object_root: Root
    domain: Domain
```

This is just a convenience container, used only in [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root) to calculate the hash tree root of an object along with a domain. The resulting root is the message data that gets signed with a BLS signature. The `SigningData` object itself is never stored or transmitted.

#### `Withdrawal`

```python
class Withdrawal(Container):
    index: WithdrawalIndex
    validator_index: ValidatorIndex
    address: ExecutionAddress
    amount: Gwei
```

A container for handling validator balance withdrawals from the consensus layer to the execution layer. The [`index`](/part3/config/types/#withdrawalindex) is a simple count of the total number of withdrawal transactions made since withdrawals were enabled in the [Capella upgrade](/part4/history/capella/).

As per the type definition of the `amount` field, the consensus layer denominates withdrawals in Gwei (as it does all Ether amounts), while the execution layer denominates withdrawals in Wei (as it does all Ether amounts). Care needs to be taken when dealing with withdrawal transactions not to end up a factor of $10^9$ wrong.

#### `HistoricalSummary`

```python
class HistoricalSummary(Container):
    """
    `HistoricalSummary` matches the components of the phase0 `HistoricalBatch`
    making the two hash_tree_root-compatible.
    """
    block_summary_root: Root
    state_summary_root: Root
```

This is part of the [double batched accumulator](https://ethresear.ch/t/double-batched-merkle-log-accumulator/571?u=benjaminion) mechanism implemented by [`process_historical_summaries_update()`](/part3/transition/epoch/#def_process_historical_summaries_update). It was introduced in the [Capella upgrade](/part4/history/capella/) and supersedes [`HistoricalBatch`] as the structure for storing roots of historical data.

The comment here is interesting. It reflects the invariant that the SSZ [hash tree root](/part2/building_blocks/merkleization/#the-hash-tree-root) of a container of objects is the same as the hash tree root of a container of the objects' hash tree roots - what I call [the magic of Merkleization](/part2/building_blocks/merkleization/#summaries-and-expansions).

The following code demonstrates this equivalence between the pre- and post-Capella constructions. It should [run](/appendices/running/) with no errors.

```python
from eth2spec.capella import mainnet
from eth2spec.capella.mainnet import *
from eth2spec.utils.ssz.ssz_typing import *

# Dummy data
roots = [Root('0x0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef')] * SLOTS_PER_HISTORICAL_ROOT
block_roots = state_roots = Vector[Root, SLOTS_PER_HISTORICAL_ROOT](*roots)

# Pre-Capella
historical_batch = HistoricalBatch(
    block_roots = block_roots,
    state_roots = state_roots)

# Post-Capella
historical_summary = HistoricalSummary(
    block_summary_root = hash_tree_root(block_roots),
    state_summary_root = hash_tree_root(state_roots))

assert(hash_tree_root(historical_batch) == hash_tree_root(historical_summary))
```

### Beacon operations <!-- /part3/containers/operations/ -->

The following are the various protocol messages that can be transmitted in a [block`](/part3/containers/blocks/#beaconblockbody) on the beacon chain.

For most of these, the proposer is rewarded either explicitly or implicitly for including the object in a block.

The proposer receives explicit in-protocol rewards for including the following in blocks:

  - `ProposerSlashing`s,
  - `AttesterSlashing`s,
  - `Attestation`s, and
  - `SyncAggregate`s.

Including `Deposit` objects in blocks is only implicitly rewarded, in that, if there are pending deposits that the block proposer does not include then the block is invalid, so the proposer receives no reward.

There is no direct reward for including `VoluntaryExit` objects. However, for each validator exited, rewards for the remaining validators increase very slightly, so it's still beneficial for proposers not to ignore `VoluntaryExit`s.

#### `ProposerSlashing`

```python
class ProposerSlashing(Container):
    signed_header_1: SignedBeaconBlockHeader
    signed_header_2: SignedBeaconBlockHeader
```

`ProposerSlashing`s may be included in blocks to prove that a validator has broken the rules and ought to be slashed. Proposers receive a reward for correctly submitting these.

In this case, the rule is that a validator may not propose two different blocks at the same height, and the payload is the signed headers of the two [blocks](/part3/containers/dependencies/#beaconblockheader) that evidence the crime. The signatures on the [`SignedBeaconBlockHeader`](/part3/containers/envelopes/#signedbeaconblockheader)s are checked to verify that they were both signed by the accused validator.

#### `AttesterSlashing`

```python
class AttesterSlashing(Container):
    attestation_1: IndexedAttestation
    attestation_2: IndexedAttestation
```

`AttesterSlashing`s may be included in blocks to prove that one or more validators in a committee has broken the rules and ought to be slashed. Proposers receive a reward for correctly submitting these.

The contents of the [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation)s are checked against the attester slashing conditions in [`is_slashable_attestation_data()`](/part3/helper/predicates/#def_is_slashable_attestation_data). If there is a violation, then any validator that attested to both `attestation_1` and `attestation_2` is slashed, see [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing).

`AttesterSlashing`s can be very large since they could in principle list the indices of all the validators in a committee. However, in contrast to proposer slashings, many validators can be slashed as a result of a single report.

#### `Attestation`

```python
class Attestation(Container):
    aggregation_bits: Bitlist[MAX_VALIDATORS_PER_COMMITTEE]
    data: AttestationData
    signature: BLSSignature
```

This is the form in which attestations make their way around the network. It is designed to be easily aggregatable: `Attestations` containing identical `AttestationData` can be combined into a single attestation by aggregating the signatures.

`Attestation`s contain the same information as [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation)s, but use knowledge of the validator committees at slots to compress the list of attesting validators down to a bitlist. Thus, `Attestation`s are at least 5 times smaller than `IndexedAttestation`s, and up to 35 times smaller (with 128 or 2048 validators per committee, respectively).

When a validator first broadcasts its attestation to the network, the `aggregation_bits` list will contain only a single bit set, and calling [`get_attesting_indices()`](/part3/helper/accessors/#def_get_attesting_indices) on it will return a list containing only a single entry, the validator's own index.

#### `Deposit`

```python
class Deposit(Container):
    proof: Vector[Bytes32, DEPOSIT_CONTRACT_TREE_DEPTH + 1]  # Merkle path to deposit root
    data: DepositData
```

This container is used to include deposit data from prospective validators in beacon blocks so that they can be processed into beacon state.

The `proof` is a Merkle proof constructed by the block proposer that the [`DepositData`](/part3/containers/dependencies/#depositdata) corresponds to the previously agreed deposit root of the Eth1 contract's deposit tree. It is verified in [`process_deposit()`](/part3/transition/block/#def_process_deposit) by [`is_valid_merkle_branch()`](/part3/helper/predicates/#def_is_valid_merkle_branch).

#### `VoluntaryExit`

```python
class VoluntaryExit(Container):
    epoch: Epoch  # Earliest epoch when voluntary exit can be processed
    validator_index: ValidatorIndex
```

Voluntary exit messages are how a validator signals that it wants to cease being a validator. Blocks containing `VoluntaryExit` data for an epoch later than the current epoch are invalid, so nodes should buffer or ignore any future-dated exits they see.

`VoluntaryExit` objects are never used naked; they are always wrapped up into a [`SignedVoluntaryExit`](/part3/containers/envelopes/#signedvoluntaryexit) object.

#### `SyncAggregate`

```python
class SyncAggregate(Container):
    sync_committee_bits: Bitvector[SYNC_COMMITTEE_SIZE]
    sync_committee_signature: BLSSignature
```

The prevailing sync committee is stored in the beacon state, so the `SyncAggregate`s included in blocks need only use a bit vector to indicate which committee members signed off on the message.

The `sync_committee_signature` is the aggregate signature of all the validators referenced in the bit vector over the block root of the previous slot.

`SyncAggregate`s are handled by [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate).

#### `BLSToExecutionChange`

```python
class BLSToExecutionChange(Container):
    validator_index: ValidatorIndex
    from_bls_pubkey: BLSPubkey
    to_execution_address: ExecutionAddress
```

The [Capella upgrade](/part4/history/capella/) gives validators that have old style [BLS withdrawal credentials](/part3/config/constants/#withdrawal-prefixes) a one-time opportunity to update them to Eth1 withdrawal credentials.

To make this change, a staker needs to broadcast a [signed message](/part3/containers/envelopes/#signedblstoexecutionchange) containing this information via a consensus node. It will eventually be included in a block at which point nodes will verify it and update their validator registries. The `from_bls_pubkey` is verified against the validator's existing withdrawal credential, and the message's signature is verified against the `from_bls_pubkey`.

### Beacon blocks <!-- /part3/containers/blocks/ -->

#### `BeaconBlockBody`

```python
class BeaconBlockBody(Container):
    randao_reveal: BLSSignature
    eth1_data: Eth1Data  # Eth1 data vote
    graffiti: Bytes32  # Arbitrary data
    # Operations
    proposer_slashings: List[ProposerSlashing, MAX_PROPOSER_SLASHINGS]
    attester_slashings: List[AttesterSlashing, MAX_ATTESTER_SLASHINGS]
    attestations: List[Attestation, MAX_ATTESTATIONS]
    deposits: List[Deposit, MAX_DEPOSITS]
    voluntary_exits: List[SignedVoluntaryExit, MAX_VOLUNTARY_EXITS]
    sync_aggregate: SyncAggregate  # [New in Altair]
    # Execution
    execution_payload: ExecutionPayload  # [New in Bellatrix]
    # Capella operations
    bls_to_execution_changes: List[SignedBLSToExecutionChange, MAX_BLS_TO_EXECUTION_CHANGES]  # [New in Capella]
```

The two fundamental data structures for nodes are the `BeaconBlock` and the `BeaconState`. The `BeaconBlock` is how the leader (the chosen proposer in a slot) communicates network updates to all the other validators, and those validators update their own `BeaconState` by applying `BeaconBlock`s. The idea is that (eventually) all validators on the network come to agree on the same `BeaconState`.

Validators are randomly selected to propose beacon blocks, and there ought to be exactly one beacon block per slot if things are running correctly. If a validator is offline, or misses its slot, or proposes an invalid block, or has its block orphaned, then a slot can be empty.

The following objects are always present in a valid beacon block.

  - `randao_reveal`: the block is invalid if the RANDAO reveal does not verify correctly against the proposer's public key. This is the block proposer's contribution to the beacon chain's randomness. The proposer generates it by signing the current epoch number (combined with [`DOMAIN_RANDAO`](/part3/config/constants/#domain_randao)) with its private key. To the best of anyone's knowledge, the result is indistinguishable from random. This gets [mixed into](/part3/transition/block/#randao) the beacon state RANDAO.
  - See [Eth1Data](/part3/containers/dependencies/#eth1data) for `eth1_data`. In principle, this is mandatory, but it is not checked, and there is no penalty for making it up.
  - `graffiti` is left free for the proposer to insert whatever data it wishes. It has no protocol level significance. It can be left as zero; most clients set the client name and version string as their own default graffiti value.
  - `sync_aggregate` is a record of which validators in the current sync committee voted for the chain head in the previous slot.
  - `execution_payload` is what was known as an Eth1 block pre-Merge. Ethereum transactions are now included within beacon blocks in the form of an [`ExecutionPayload`](/part3/containers/execution/#executionpayload) structure.

Deposits are a special case. They are mandatory only if there are pending deposits to be processed. There is no explicit reward for including deposits, except that a block is invalid without any that ought to be there.

  - `deposits`: if the block does not contain either all the outstanding [`Deposit`](/part3/containers/operations/#deposit)s, or [`MAX_DEPOSITS`](/part3/config/preset/#max_deposits) of them in deposit order, then it is [invalid](/part3/transition/block/#operations).

Including any of the remaining objects is optional. They are handled, if present, in the [`process_operations()`](/part3/transition/block/#def_process_operations) function.

The proposer earns rewards for including any of the following. Rewards for attestations and sync aggregates are available every slot. Slashings, however, are very rare.

  - `proposer_slashings`: up to [`MAX_PROPOSER_SLASHINGS`](/part3/config/preset/#max_proposer_slashings) [`ProposerSlashing`](/part3/containers/operations/#proposerslashing) objects may be included.
  - `attester_slashings`: up to [`MAX_ATTESTER_SLASHINGS`](/part3/config/preset/#max_attester_slashings) [`AttesterSlashing`](/part3/containers/operations/#attesterslashing) objects may be included.
  - `attestations`: up to [`MAX_ATTESTATIONS`](/part3/config/preset/#max_attestations) (aggregated) [`Attestation`](/part3/containers/operations/#attestation) objects may be included. The block proposer is incentivised to include well-packed aggregate attestations, as it receives a micro reward for each unique attestation. In a perfect world, with perfectly aggregated attestations, `MAX_ATTESTATIONS` would be equal to `MAX_COMMITTEES_PER_SLOT`; in our configuration it is double. This provides capacity in blocks to catch up with attestations after skip slots, and also room to include some imperfectly aggregated attestations.

Including voluntary exits and BLS to execution changes is optional, and there are no explicit rewards for doing so.

  - `voluntary_exits`: up to [`MAX_VOLUNTARY_EXITS`](/part3/config/preset/#max_voluntary_exits) [`SignedVoluntaryExit`](/part3/containers/envelopes/#signedvoluntaryexit) objects may be included.
  - `bls_to_execution_changes`: up to [`MAX_BLS_TO_EXECUTION_CHANGES`](/part3/config/preset/#max_bls_to_execution_changes) [`SignedBLSToExecutionChange`](/part3/containers/envelopes/#signedblstoexecutionchange) objects may be included.

#### `BeaconBlock`

```python
class BeaconBlock(Container):
    slot: Slot
    proposer_index: ValidatorIndex
    parent_root: Root
    state_root: Root
    body: BeaconBlockBody
```

`BeaconBlock` just adds some blockchain paraphernalia to [`BeaconBlockBody`](#beaconblockbody). It is identical to [`BeaconBlockHeader`](/part3/containers/dependencies/#beaconblockheader), except that the `body_root` is replaced by the actual block `body`.

`slot` is the slot the block is proposed for.

`proposer_index` was [added](https://github.com/ethereum/consensus-specs/pull/1626) to avoid a potential [DoS vector](https://github.com/ethereum/consensus-specs/issues/1601#issue-556546908), and to allow clients without full access to the state to still know [useful things](https://github.com/ethereum/consensus-specs/pull/1626#pullrequestreview-372265515).

`parent_root` is used to make sure that this block is a direct child of the last block we processed.

In order to calculate `state_root`, the proposer is expected to run the state transition with the block before propagating it. After the beacon node has processed the block, the state roots are compared to ensure they match. This is the mechanism for tying the whole system together and making sure that all validators and beacon nodes are always working off the same version of state (in the absence of short-term forks).

If any of these is incorrect, then the block is invalid with respect to the current beacon state and will be ignored.

### Beacon state <!-- /part3/containers/state/ -->

#### `BeaconState`

```python
class BeaconState(Container):
    # Versioning
    genesis_time: uint64
    genesis_validators_root: Root
    slot: Slot
    fork: Fork
    # History
    latest_block_header: BeaconBlockHeader
    block_roots: Vector[Root, SLOTS_PER_HISTORICAL_ROOT]
    state_roots: Vector[Root, SLOTS_PER_HISTORICAL_ROOT]
    historical_roots: List[Root, HISTORICAL_ROOTS_LIMIT]  # Frozen in Capella, replaced by historical_summaries
    # Eth1
    eth1_data: Eth1Data
    eth1_data_votes: List[Eth1Data, EPOCHS_PER_ETH1_VOTING_PERIOD * SLOTS_PER_EPOCH]
    eth1_deposit_index: uint64
    # Registry
    validators: List[Validator, VALIDATOR_REGISTRY_LIMIT]
    balances: List[Gwei, VALIDATOR_REGISTRY_LIMIT]
    # Randomness
    randao_mixes: Vector[Bytes32, EPOCHS_PER_HISTORICAL_VECTOR]
    # Slashings
    slashings: Vector[Gwei, EPOCHS_PER_SLASHINGS_VECTOR]  # Per-epoch sums of slashed effective balances
    # Participation
    previous_epoch_participation: List[ParticipationFlags, VALIDATOR_REGISTRY_LIMIT]  # [Modified in Altair]
    current_epoch_participation: List[ParticipationFlags, VALIDATOR_REGISTRY_LIMIT]  # [Modified in Altair]
    # Finality
    justification_bits: Bitvector[JUSTIFICATION_BITS_LENGTH]  # Bit set for every recent justified epoch
    previous_justified_checkpoint: Checkpoint
    current_justified_checkpoint: Checkpoint
    finalized_checkpoint: Checkpoint
    # Inactivity
    inactivity_scores: List[uint64, VALIDATOR_REGISTRY_LIMIT]  # [New in Altair]
    # Sync
    current_sync_committee: SyncCommittee  # [New in Altair]
    next_sync_committee: SyncCommittee  # [New in Altair]
    # Execution
    latest_execution_payload_header: ExecutionPayloadHeader  # [New in Bellatrix]
    # Withdrawals
    next_withdrawal_index: WithdrawalIndex  # [New in Capella]
    next_withdrawal_validator_index: ValidatorIndex  # [New in Capella]
    # Deep history valid from Capella onwards
    historical_summaries: List[HistoricalSummary, HISTORICAL_ROOTS_LIMIT]  # [New in Capella]
```

All roads lead to the `BeaconState`. Maintaining this data structure is the sole purpose of all the apparatus in all the spec documents. This state is the focus of consensus among the beacon nodes; it is what everybody, eventually, must agree on.

The beacon chain's state is monolithic: everything is bundled into a single state object (sometimes referred to as the "[God object](https://github.com/ethereum/consensus-specs/issues/582#issuecomment-461591281)"). Some [have argued](https://github.com/ethereum/consensus-specs/issues/582) for more granular approaches that might be more efficient, but at least the current approach is simple.

Let's break this thing down.

<a id="genesis_validators_root"></a>

```code
    # Versioning
    genesis_time: uint64
    genesis_validators_root: Root
    slot: Slot
    fork: Fork
```

How do we know which chain we're on, and where we are on it? The information here ought to be sufficient. A continuous path back to the genesis block would also suffice.

`genesis_validators_root` is calculated at [Genesis time](/part3/initialise/#initialisation) (when the chain starts) and is fixed for the life of the chain. This, combined with the `fork` identifier, should serve to uniquely identify the chain that we are on.

`genesis_time` is used by the [fork choice rule](/part3/forkchoice/) to work out what slot we're in, and (since The Merge) to [validate execution payloads](/part3/transition/block/#process_execution_payload).

The values of these two fields is fixed for the life of the chain. For the mainnet beacon chain they have the following values:

|||
|-|-|
| `genesis_time` | 1606824023 |
| `genesis_validators_root` | `0x4b363db9`<wbr/>`4e286120`<wbr/>`d76eb905`<wbr/>`340fdd4e`<wbr/>`54bfe9f0`<wbr/>`6bf33ff6`<wbr/>`cf5ad27f`<wbr/>`511bfe95` |

The `fork` [object](/part3/containers/dependencies/#fork) is manually updated as part of beacon chain upgrades, also called hard forks. This invalidates blocks and attestations from validators not following the new fork.

Since the Capella fork, the `fork` field has contained the following values:

|||
|-|-|
| `previous_version` | `0x02000000` |
| `current_version` | `0x03000000` |
| `epoch` | 194048 |

Historical info on fork versions and upgrade timing is in the [Upgrade History](/part4/history/) chapter.

```code
    # History
    latest_block_header: BeaconBlockHeader
    block_roots: Vector[Root, SLOTS_PER_HISTORICAL_ROOT]
    state_roots: Vector[Root, SLOTS_PER_HISTORICAL_ROOT]
    historical_roots: List[Root, HISTORICAL_ROOTS_LIMIT]  # Frozen in Capella, replaced by historical_summaries
```

`latest_block_header` is only used to make sure that the next block we process is a direct descendent of the previous block. It's a blockchain thing.

Past `block_roots` and `state_roots` are stored in the lists here until the lists are full. Before the Capella upgrade, once the lists were full, they were Merkleized together and the root appended to `historical_roots`. Since Capella, they are now Merkleized separately and appended to `historical_summaries` (see below). The `historical_roots` list is now frozen and continues to exist only to allow proofs to be made against pre-Capella blocks and states.

```code
    # Eth1
    eth1_data: Eth1Data
    eth1_data_votes: List[Eth1Data, EPOCHS_PER_ETH1_VOTING_PERIOD * SLOTS_PER_EPOCH]
    eth1_deposit_index: uint64
```

`eth1_data` is the latest agreed upon state of the Eth1 chain and deposit contract. `eth1_data_votes` accumulates [`Eth1Data`](/part3/containers/dependencies/#eth1data) from blocks until there is an overall majority in favour of one Eth1 state. If a majority is not achieved by the time the list is full then it is cleared down and voting starts again from scratch. `eth1_deposit_index` is the total number of deposits that have been processed by the beacon chain (which is greater than or equal to the number of validators, as a deposit can top up the balance of an existing validator).

<a id="registry"></a>

```code
    # Registry
    validators: List[Validator, VALIDATOR_REGISTRY_LIMIT]
    balances: List[Gwei, VALIDATOR_REGISTRY_LIMIT]
```

The registry of [`Validator`](/part3/containers/dependencies/#validator)s and their balances. The `balances` list is separated out as it changes much more frequently than the `validators` list. Roughly speaking, balances of active validators are updated at least once per epoch, while the `validators` list has only minor updates per epoch. When combined with [SSZ tree hashing](/part2/building_blocks/merkleization/), this results in a big saving in the amount of data to be rehashed on registry updates. See also validator inactivity scores under [Inactivity](#inactivity) which we treat similarly.

```code
    # Randomness
    randao_mixes: Vector[Bytes32, EPOCHS_PER_HISTORICAL_VECTOR]
```

Past randao mixes are stored in a fixed-size circular list for [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector) epochs (~290 days). These can be used to recalculate past committees, which allows slashing of historical attestations. See [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector) for more information.

```code
    # Slashings
    slashings: Vector[Gwei, EPOCHS_PER_SLASHINGS_VECTOR]
```

A fixed-size circular list of past slashed amounts. Each epoch, the total effective balance of all validators slashed in that epoch is stored as an entry in this list. When the final slashing penalty for a slashed validator is calculated, it is [weighted](/part3/transition/epoch/#slashings) with the sum of this list. This mechanism is designed to less heavily penalise one-off slashings that are most likely accidental, and more heavily penalise mass slashings during a window of time, which are more likely to be a coordinated attack.

```code
    # Participation
    previous_epoch_participation: List[ParticipationFlags, VALIDATOR_REGISTRY_LIMIT]  # [Modified in Altair]
    current_epoch_participation: List[ParticipationFlags, VALIDATOR_REGISTRY_LIMIT]  # [Modified in Altair]
```

These lists record which validators participated in attesting during the current and previous epochs by recording [flags](/part3/config/constants/#participation-flag-indices) for timely votes for the correct source, the correct target and the correct head. We store two epochs' worth since Validators have up to 32 slots to include a correct target vote. The flags are used to calculate finality and to assign rewards at the end of epochs.

Previously, during Phase&nbsp;0, we stored two epochs' worth of actual attestations in the state and processed them en masse at the end of epochs. This was slow, and was thought to be contributing to observed late block production in the first slots of epochs. The change to the new scheme was implemented in the Altair upgrade under the title of [Accounting Reforms](https://github.com/ethereum/consensus-specs/pull/2176).

```code
    # Finality
    justification_bits: Bitvector[JUSTIFICATION_BITS_LENGTH]
    previous_justified_checkpoint: Checkpoint
    current_justified_checkpoint: Checkpoint
    finalized_checkpoint: Checkpoint
```

Ethereum&nbsp;2.0 uses the [Casper FFG](https://arxiv.org/pdf/1710.09437.pdf) finality mechanism, with a [k-finality](https://docs.google.com/presentation/d/1MZ-E6TVwomt4rqz-P2Bd_X3DFUW9fWDQkxUP_QJhkyw/edit#slide=id.g621d74a5e7_0_159) optimisation, where k&nbsp;=&nbsp;2. The above objects in the state are the data that need to be tracked in order to apply the finality rules.

  - `justification_bits` is only four bits long. It tracks the justification status of the last four epochs: 1 if justified, 0 if not. This is used when [calculating](/part3/transition/epoch/#justification-and-finalization) whether we can finalise an epoch.
  - Outside the finality calculations, `previous_justified_checkpoint` and `current_justified_checkpoint` are used to [filter](/part3/helper/accessors/#get_attestation_participation_flag_indices) attestations: valid blocks include only attestations with a source checkpoint that matches the justified checkpoint in the state for the attestation's epoch.
  - `finalized_checkpoint`: the network has agreed that the beacon chain state at or before that epoch will never be reverted. So, for one thing, the fork choice rule doesn't need to go back any further than this. The Casper FFG mechanism is specifically constructed so that two conflicting finalized checkpoints cannot be created without at least one third of validators being slashed.

<a id="inactivity"></a>

```code
    # Inactivity
    inactivity_scores: List[uint64, VALIDATOR_REGISTRY_LIMIT]  # [New in Altair]
```

This is logically part of "Registry", above, and would be better placed there. It is a per-validator record of [inactivity scores](/part3/config/configuration/#inactivity-penalties) that is updated every epoch. This list is stored outside the main list of Validator objects since it is updated very frequently. See the [Registry](#registry) for more explanation.

```code
    # Sync
    current_sync_committee: SyncCommittee  # [New in Altair]
    next_sync_committee: SyncCommittee  # [New in Altair]
```

Sync committees were introduced in the Altair upgrade. The next sync committee is calculated and stored so that participating validators can prepare in advance by subscribing to the required p2p subnets.

```code
    # Execution
    latest_execution_payload_header: ExecutionPayloadHeader  # [New in Bellatrix]
```

Since the Merge, the [header](/part3/containers/execution/#executionpayloadheader) of the most recent execution payload is cached in the beacon state. This serves two functions for now, though possibly more in future. First, it allows the chain to check whether the Merge has been completed or not. See [`is_merge_transition_complete()`](/part3/helper/predicates/#is_merge_transition_complete). Second, it allows the beacon chain to check that the execution chain is unbroken when processing a new execution payload. See [`process_execution_payload()`](/part3/transition/block/#process_execution_payload).

<a id="withdrawals"></a>

```code
    # Withdrawals
    next_withdrawal_index: WithdrawalIndex  # [New in Capella]
    next_withdrawal_validator_index: ValidatorIndex  # [New in Capella]
```

Automatic validator balance withdrawals were added in the [Capella upgrade](/part4/history/capella/). The `next_withdrawal_index` maintains a count of the total number of withdrawal transactions performed so far, while `next_withdrawal_validator_index` cycles through the validator registry to keep track of which validator should be considered for a withdrawal next. Validators are considered for withdrawals consecutively in order of their validator indices, and the withdrawals sweep wraps round to zero after considering the highest-numbered validator. A maximum of [`MAX_WITHDRAWALS_PER_PAYLOAD`](/part3/config/preset/#max_withdrawals_per_payload) withdrawals may be made per block.

```code
    # Deep history valid from Capella onwards
    historical_summaries: List[HistoricalSummary, HISTORICAL_ROOTS_LIMIT]  # [New in Capella]
```

Hash tree roots of `state.block_roots` and `state.state_roots` are periodically added to `historical_summaries` every [`SLOTS_PER_HISTORICAL_ROOT`](/part3/config/preset/#slots_per_historical_root) slots as part of the protocol's [double batched accumulator](https://ethresear.ch/t/double-batched-merkle-log-accumulator/571?u=benjaminion). The work is done by [`process_historical_summaries_update()`](/part3/transition/epoch/#def_process_historical_summaries_update).

The `state.historical_summaries` list was introduced in the Capella upgrade and functionally replaces the `state.historical_roots` list that's now frozen (see above). It uses the [`HistoricalSummary`](/part3/containers/dependencies/#historicalsummary) container, which is twice as big as a `Root` type (64 bytes per item rather than 32). The list will effectively grow without bound ([`HISTORICAL_ROOTS_LIMIT`](/part3/config/preset/#historical_roots_limit) is _large_), but at a rate of only 20&nbsp;KB per year. Keeping this data is useful for light clients, and also allows Merkle proofs to be created against past states, for example [historical deposit data](https://github.com/ethereum/consensus-specs/issues/1343#issuecomment-521453223).

#### Historical Note

There was a period during which beacon state was split into "crystallized state" and "active state". The active state was constantly changing; the crystallized state changed only once per epoch (or what passed for epochs back then). Separating out the fast-changing state from the slower-changing state was an attempt to avoid having to constantly rehash the whole state every slot. With the introduction of [SSZ tree hashing](/part2/building_blocks/merkleization/), this was [no longer necessary](https://github.com/ethereum/consensus-specs/pull/122#issuecomment-437170249), as the roots of the slower changing parts could simply be cached, which was a nice simplification. There remains an echo of this approach, however, in the splitting out of validator balances and inactivity scores into different structures withing the beacon state.

### Execution <!-- /part3/containers/execution/ -->

#### `ExecutionPayload`

```python
class ExecutionPayload(Container):
    # Execution block header fields
    parent_hash: Hash32
    fee_recipient: ExecutionAddress  # 'beneficiary' in the yellow paper
    state_root: Bytes32
    receipts_root: Bytes32
    logs_bloom: ByteVector[BYTES_PER_LOGS_BLOOM]
    prev_randao: Bytes32  # 'difficulty' in the yellow paper
    block_number: uint64  # 'number' in the yellow paper
    gas_limit: uint64
    gas_used: uint64
    timestamp: uint64
    extra_data: ByteList[MAX_EXTRA_DATA_BYTES]
    base_fee_per_gas: uint256
    # Extra payload fields
    block_hash: Hash32  # Hash of execution block
    transactions: List[Transaction, MAX_TRANSACTIONS_PER_PAYLOAD]
    withdrawals: List[Withdrawal, MAX_WITHDRAWALS_PER_PAYLOAD]  # [New in Capella]
```

Since the Merge, blocks on the beacon chain contain Ethereum transaction data, formerly known as Eth1 blocks, and now called execution payloads.

This is a significant change, and is what led to the name "The Merge".

  - Pre-Merge, there were two types of block in the Ethereum system:
    - Eth1 blocks contained users' transactions and were gossiped between Eth1 nodes;
    - Eth2 blocks (beacon blocks) contained only consensus information and were gossiped between Eth2 nodes.
  - Post-Merge, there is only one kind of block, the merged beacon block:
    - Beacon blocks contain execution payloads that in turn contain users' transactions. These blocks are gossiped only between consensus (Eth2) nodes.

The `ExecutionPayload` is contained in the [`BeaconBlock`](/part3/containers/blocks/#beaconblock) structure.

The fields of `ExecutionPayload` mostly reflect the old structure of Eth1 blocks as described in Ethereum's [Yellow Paper](https://ethereum.org/615606b8e1e1da72687e66dba79771e9/yellow-paper-berlin.pdf)[^fn-yellow-paper-berlin], section 4.3. Differences from the Eth1 block structure are noted in the comments.

[^fn-yellow-paper-berlin]: This is intended to be a permalink to the Yellow Paper's "Berlin" edition, a pre-Merge version of the YP. At the time of writing, the YP has not been updated for the "London" upgrade and is therefore missing the [EIP-1559](https://eips.ethereum.org/EIPS/eip-1559) field `base_fee_per_gas`.

The execution payload differs from an old Eth1 block in the following respects:

  - `ommersHash` (also known as `uncle_hashes`), `difficulty`, `mixHash` and `nonce` were not carried over from Eth1 blocks as they were specific to the proof of work mechanism.
  - `fee_recipient` is the Ethereum account address that will receive the unburnt portion of the transaction fees (the priority fees). This has been called various things at various times: the original Yellow Paper calls it `beneficiary`; [EIP-1559](https://eips.ethereum.org/EIPS/eip-1559) calls it `author`. In any case, the proposer of the block sets the `fee_recipient` to specify where the appropriate transaction fees for the block are to be sent. Under proof of work this was the same address as the `COINBASE` address that received the block reward. Under proof of stake, the block reward is credited to the validator's beacon chain balance, and the transaction fees are credited to the `fee_recipient` Ethereum address.
  - `prev_randao` replaces `difficulty`. The Eth1 chain did not have access to good quality randomness. Sometimes the block hash or difficulty of the block were used to seed randomness, but these were low quality. The `prev_randao` field gives the execution layer access to the beacon chain's randomness. This is [better](/part2/building_blocks/randomness/), but still not of cryptographic quality.
  - `block_number` in the execution layer is the block height in that chain, picking up from the Eth1 block height at the Merge. It increments by one for every beacon block produced. The beacon chain itself does not track block height, only slot number, which can differ from block height due to empty slots.
  - The execution payload `block_hash` is included. The consensus layer does not know how to calculate the root hashes of execution blocks, but needs access to them when checking that the execution chain is unbroken during [execution payload processing](/part3/transition/block/#process_execution_payload).
  - Despite being flagged in the comments as an "extra payload field", a list of transactions was always part of Eth1 blocks. However, the list of ommers/uncles is no longer present.

Individual transactions are represented by the [Transaction](/part3/config/types/#transaction) custom type. There can be up to [`MAX_TRANSACTIONS_PER_PAYLOAD`](/part3/config/preset/#max_transactions_per_payload) of them in a single execution payload. The values of `MAX_BYTES_PER_TRANSACTION` and `MAX_TRANSACTIONS_PER_PAYLOAD` are huge, and suggest that an execution payload could be up to a petabyte in size. These sizes are specified only because [SSZ `List`](/part2/building_blocks/ssz/#lists) types require them. They will occupy only the minimum necessary space in practice.

The `withdrawals` field was added in the [Capella upgrade](/part4/history/capella/). Withdrawal transactions are unusual in that they affect state on both the consensus side and the execution side. Uniquely, withdrawal transactions are the only data that's generated by the consensus layer but only communicated between nodes in execution payloads.

#### `ExecutionPayloadHeader`

```python
class ExecutionPayloadHeader(Container):
    # Execution block header fields
    parent_hash: Hash32
    fee_recipient: ExecutionAddress
    state_root: Bytes32
    receipts_root: Bytes32
    logs_bloom: ByteVector[BYTES_PER_LOGS_BLOOM]
    prev_randao: Bytes32
    block_number: uint64
    gas_limit: uint64
    gas_used: uint64
    timestamp: uint64
    extra_data: ByteList[MAX_EXTRA_DATA_BYTES]
    base_fee_per_gas: uint256
    # Extra payload fields
    block_hash: Hash32  # Hash of execution block
    transactions_root: Root
    withdrawals_root: Root  # [New in Capella]
```

The same as [`ExecutionPayload`](#executionpayload) but with the transactions represented only by their root. By the [magic of Merkleization](/part2/building_blocks/merkleization/#summaries-and-expansions), the [hash tree root](/part2/building_blocks/merkleization/#the-hash-tree-root) of an `ExecutionPayloadHeader` will be the same as the hash tree root of its corresponding `ExecutionPayload`.

The most recent `ExecutionPayloadHeader` is stored in the [beacon state](/part3/containers/state/#beacon-state).

### Signed envelopes <!-- /part3/containers/envelopes/ -->

The following are just wrappers for more basic types, with an added signature.

#### `SignedVoluntaryExit`

```python
class SignedVoluntaryExit(Container):
    message: VoluntaryExit
    signature: BLSSignature
```

A voluntary exit is currently signed with the validator's online signing key.

There has been some discussion about [changing this](https://github.com/ethereum/consensus-specs/issues/1578) to also allow signing of a voluntary exit with the validator's offline withdrawal key. The introduction of multiple types of [withdrawal credential](/part3/config/constants/#withdrawal-prefixes) makes this more complex, however, and it is no longer likely to be practical.

#### `SignedBeaconBlock`

```python
class SignedBeaconBlock(Container):
    message: BeaconBlock
    signature: BLSSignature
```

`BeaconBlock`s are signed by the block proposer and unwrapped for block processing.

This signature is what makes proposing a block "accountable". If two correctly signed conflicting blocks turn up, the signatures guarantee that the same proposer produced them both, and is therefore subject to being slashed. This is also why stakers need to closely guard their signing keys.

#### `SignedBeaconBlockHeader`

```python
class SignedBeaconBlockHeader(Container):
    message: BeaconBlockHeader
    signature: BLSSignature
```

This is used only when reporting proposer slashing, within a [`ProposerSlashing`](/part3/containers/operations/#proposerslashing) object.

Through the magic of [SSZ hash tree roots](/part2/building_blocks/merkleization/), a valid signature for a `SignedBeaconBlock` is also a valid signature for a `SignedBeaconBlockHeader`. Proposer slashing makes use of this to save space in slashing reports.

#### `SignedBLSToExecutionChange`

```python
class SignedBLSToExecutionChange(Container):
    message: BLSToExecutionChange
    signature: BLSSignature
```

A message [requesting a change](/part3/containers/operations/#blstoexecutionchange) from BLS withdrawal credentials to Eth1 withdrawal credentials.

Uniquely, this message is signed with the validator's withdrawal key rather than its usual signing key. Only validators that made deposits with `0x00` [BLS credentials](/part3/config/constants/#withdrawal-prefixes) have a withdrawal key, and it will usually be different from the signing key (although it may be [derived from the same mnemonic](https://eips.ethereum.org/EIPS/eip-2334#validator-keys)).

## Helper Functions <!-- /part3/helper/ -->

### Preamble

> _Note_: The definitions below are for specification purposes and are not necessarily optimal implementations.

This note in the spec is super important for implementers! There are many, many optimisations of the below routines that are being used in practice; a naive implementation would be impractically slow for mainnet configurations. As long as the optimised code produces identical results to the code here, then all is fine.

### Math <!-- /part3/helper/math/ -->

#### `integer_squareroot`

<a id="def_integer_squareroot"></a>

```python
def integer_squareroot(n: uint64) -> uint64:
    """
    Return the largest integer ``x`` such that ``x**2 <= n``.
    """
    x = n
    y = (x + 1) // 2
    while y < x:
        x = y
        y = (x + n // x) // 2
    return x
```

Validator rewards scale with the reciprocal of the square root of the total active balance of all validators. This is calculated in [`get_base_reward_per_increment()`](/part3/transition/epoch/#def_get_base_reward_per_increment).

In principle `integer_squareroot` is also used in [`get_attestation_participation_flag_indices()`](/part3/helper/accessors/#def_get_attestation_participation_flag_indices), to specify the maximum delay for source votes to receive a reward. But this is just the constant, `integer_squareroot(SLOTS_PER_EPOCH)`, which is `5`.

[Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method) is used which has pretty good convergence properties, but implementations may use any method that gives identical results.

|||
|-|------|
| Used&nbsp;by | [`get_base_reward_per_increment()`](/part3/transition/epoch/#def_get_base_reward_per_increment), [`get_attestation_participation_flag_indices()`](/part3/helper/accessors/#def_get_attestation_participation_flag_indices) |

#### `xor`

<a id="def_xor"></a>

```python
def xor(bytes_1: Bytes32, bytes_2: Bytes32) -> Bytes32:
    """
    Return the exclusive-or of two 32-byte strings.
    """
    return Bytes32(a ^ b for a, b in zip(bytes_1, bytes_2))
```

The bitwise `xor` of two 32-byte quantities is defined here in Python terms.

This is used only in [`process_randao()`](/part3/transition/block/#def_process_randao) when mixing in the new randao reveal.

Fun fact: if you `xor` two `byte` types in Java, the result is a 32 bit (signed) integer. This is one reason  we need to define the "obvious" here. But mainly, because the spec is executable, we need to tell Python what it doesn't already know.

|||
|-|------|
| Used&nbsp;by | [`process_randao()`](/part3/transition/block/#def_process_randao) |

#### `uint_to_bytes`

<a id="def_uint_to_bytes"></a>

> `def uint_to_bytes(n: uint) -> bytes` is a function for serializing the `uint` type object to bytes in ``ENDIANNESS``-endian. The expected length of the output is the byte-length of the `uint` type.

For the most part, integers are integers and bytes are bytes, and they don't mix much. But there are a few places where we need to convert from integers to bytes:

  - several times in the [`compute_shuffled_index()`](/part3/helper/misc/#def_compute_shuffled_index) algorithm;
  - in [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index) for selecting a proposer weighted by stake;
  - in [`get_seed()`](/part3/helper/accessors/#def_get_seed) to mix the epoch number into the randao mix;
  - in [`get_beacon_proposer_index()`](/part3/helper/accessors/#def_get_beacon_proposer_index) to mix the slot number into the per-epoch randao seed; and
  - in [`get_next_sync_committee_indices()`](/part3/helper/accessors/#def_get_next_sync_committee_indices).

You'll note that in every case, the purpose of the conversion is for the integer to form part of a byte string that is hashed to create (pseudo-)randomness.

The result of this conversion is dependent on our arbitrary choice of endianness, that is, how we choose to represent integers as strings of bytes. For Eth2, we have chosen little-endian: see the discussion of [`ENDIANNESS`](/part3/config/constants/#endianness) for more background.

The `uint_to_bytes()` function is not given an explicit implementation in the specification, which is unusual. This [to avoid](https://github.com/ethereum/consensus-specs/pull/1935) exposing the innards of the Python SSZ implementation (of `uint`) to the rest of the spec. When running the spec as an executable, it uses the definition in the [SSZ utilities](https://github.com/ethereum/consensus-specs/blob/fb34e162ef3476f2dd5d7dc6ebfc51c626608ffa/tests/core/pyspec/eth2spec/utils/ssz/ssz_impl.py#L16).

|||
|-|------|
| Used&nbsp;by | [`compute_shuffled_index()`](/part3/helper/misc/#def_compute_shuffled_index), [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index), [`get_seed()`](/part3/helper/accessors/#def_get_seed), [`get_beacon_proposer_index()`](/part3/helper/accessors/#def_get_beacon_proposer_index), [`get_next_sync_committee_indices()`](/part3/helper/accessors/#def_get_next_sync_committee_indices) |
| See&nbsp;also | [`ENDIANNESS`](/part3/config/constants/#endianness), [SSZ utilities](https://github.com/ethereum/consensus-specs/blob/fb34e162ef3476f2dd5d7dc6ebfc51c626608ffa/tests/core/pyspec/eth2spec/utils/ssz/ssz_impl.py#L16) |

#### `bytes_to_uint64`

<a id="def_bytes_to_uint64"></a>

```python
def bytes_to_uint64(data: bytes) -> uint64:
    """
    Return the integer deserialization of ``data`` interpreted as ``ENDIANNESS``-endian.
    """
    return uint64(int.from_bytes(data, ENDIANNESS))
```

`bytes_to_uint64()` is the inverse of [`uint_to_bytes()`](#def_uint_to_bytes), and is used by the [shuffling algorithm](/part3/helper/misc/#compute_shuffled_index) to create a random index from the output of a hash.

It is also used in the validator specification when selecting validators to aggregate [attestations](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#aggregation-selection), and [sync committee messages](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/validator.md#aggregation-selection).

`int.from_bytes` is a [built-in](https://docs.python.org/3/library/stdtypes.html#int.from_bytes) Python&nbsp;3 method. The `uint64` cast is provided by the spec's SSZ implementation.

|||
|-|------|
| Used&nbsp;by | [`compute_shuffled_index`](/part3/helper/misc/#def_compute_shuffled_index) |
| See&nbsp;also | [attestation aggregator selection](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#aggregation-selection), [sync committee aggregator selection](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/validator.md#aggregation-selection) |

### Crypto <!-- /part3/helper/crypto/ -->

#### `hash`

> `def hash(data: bytes) -> Bytes32` is SHA256.

SHA256 was [chosen](https://github.com/ethereum/consensus-specs/pull/779) as the protocol's base hash algorithm for easier cross-chain interoperability: many other chains use SHA256, and Eth1 has a SHA256 precompile.

There was a lot of [discussion](https://github.com/ethereum/consensus-specs/issues/612) about this choice early in the design process. The [original plan](https://github.com/ethereum/consensus-specs/pull/11) had been to use the BLAKE2b-512 hash function &ndash; that being a modern hash function that's faster than SHA3 &ndash; and to move to a STARK/SNARK friendly hash function at some point (such as [MiMC](https://ethresear.ch/t/hash-based-vdfs-mimc-and-starks/2337?u=benjaminion)). However, to keep interoperability with Eth1, in particular for the implementation of the deposit contract, the hash function was [changed to Keccak256](https://github.com/ethereum/consensus-specs/issues/151). Finally, we [settled on SHA256](https://github.com/ethereum/consensus-specs/pull/779) as having even broader compatibility.

The hash function serves two purposes within the protocol. The main use, computationally, is in [Merkleization](/part2/building_blocks/merkleization/), the computation of hash tree roots, which is ubiquitous in the protocol. Its other use is to harden the randomness used in various places.

|||
|-|------|
| Used&nbsp;by | [`hash_tree_root`](#def_hash_tree_root), [`is_valid_merkle_branch()`](/part3/helper/predicates/#def_is_valid_merkle_branch), [`compute_shuffled_index()`](/part3/helper/misc/#def_compute_shuffled_index), [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index), [`get_seed()`](/part3/helper/accessors/#def_get_seed), [`get_beacon_proposer_index()`](/part3/helper/accessors/#def_get_beacon_proposer_index), [`get_next_sync_committee_indices()`](/part3/helper/accessors/#def_get_next_sync_committee_indices), [`process_randao()`](/part3/transition/block/#def_process_randao) |

#### `hash_tree_root`

<a id="def_hash_tree_root"></a>

> `def hash_tree_root(object: SSZSerializable) -> Root` is a function for hashing objects into a single root by utilizing a hash tree structure, as defined in the [SSZ spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md#merkleization).

The development of the tree hashing process was transformational for the Ethereum&nbsp;2.0 specification, and it is now used everywhere.

The naive way to create a digest of a data structure is to [serialise](https://en.wikipedia.org/wiki/Serialization) it and then just run a hash function over the result. In tree hashing, the basic idea is to treat each element of an ordered, compound data structure as the leaf of a Merkle tree, recursively if necessary until a primitive type is reached, and to return the [Merkle root](https://en.wikipedia.org/wiki/Merkle_tree) of the resulting tree.

At first sight, this all looks quite inefficient. Twice as much data needs to be hashed when tree hashing, and actual speeds are [4-6 times slower](https://github.com/ethereum/consensus-specs/pull/120) compared with the linear hash. However, it is good for [supporting light clients](https://github.com/ethereum/consensus-specs/issues/54), because it allows Merkle proofs to be constructed easily for subsets of the full state.

The breakthrough insight was realising that much of the re-hashing work can be cached: if part of the state data structure has not changed, that part does not need to be re-hashed: the whole subtree can be replaced with its cached hash. This turns out to be a huge efficiency boost, allowing the previous design, with cumbersome separate crystallised and active state, to be [simplified](https://github.com/ethereum/consensus-specs/pull/122) into a single state object.

Merkleization, the process of calculating the `hash_tree_root()` of an object, is defined in the [SSZ specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/ssz/simple-serialize.md), and explained further in the [section on SSZ](/part2/building_blocks/ssz/).

#### BLS signatures

See the main write-up on [BLS Signatures](/part2/building_blocks/signatures/) for a more in-depth exploration of this topic.

> The [IETF BLS signature draft standard v4](https://tools.ietf.org/html/draft-irtf-cfrg-bls-signature-04) with ciphersuite `BLS_SIG_BLS12381G2_XMD:SHA-256_SSWU_RO_POP_` defines the following functions:
>
>   - `def Sign(privkey: int, message: Bytes) -> BLSSignature`
>   - `def Verify(pubkey: BLSPubkey, message: Bytes, signature: BLSSignature) -> bool`
>   - `def Aggregate(signatures: Sequence[BLSSignature]) -> BLSSignature`
>   - `def FastAggregateVerify(pubkeys: Sequence[BLSPubkey], message: Bytes, signature: BLSSignature) -> bool`
>   - `def AggregateVerify(pubkeys: Sequence[BLSPubkey], messages: Sequence[Bytes], signature: BLSSignature) -> bool`
>   - `def KeyValidate(pubkey: BLSPubkey) -> bool`
>
> The above functions are accessed through the `bls` module, e.g. `bls.Verify`.

The detailed specification of the cryptographic functions underlying Ethereum&nbsp;2.0's BLS signing scheme is delegated to the draft IRTF standard[^fn-ietf-irtf-1] as described in the spec. This includes specifying the elliptic curve BLS12-381 as our domain of choice.

[^fn-ietf-irtf-1]: This document does not have the full force of an IETF standard. For one thing, it remains a draft (that is now expired), for another it is an IRTF document, meaning that it is from a research group rather than being on the IETF standards track. [Some context](https://mailarchive.ietf.org/arch/msg/ietf/A8MaBwNpbWf_DJoWj0sRROIml3Y/) from Brian Carpenter, former IETF chair,
    > I gather that you are referring to an issue in draft-irtf-cfrg-bls-signature-04. That is not even an IETF draft; it's an IRTF draft, apparently being discussed in an IRTF Research Group. So it is not even remotely under consideration to become an IETF standard...

Our intention in conforming to the in-progress standard is to provide for maximal interoperability with other chains, applications, and cryptographic libraries. Ethereum Foundation researchers and Eth2 developers had input to the [development](https://github.com/cfrg/draft-irtf-cfrg-bls-signature) of the standard. Nevertheless, there were some challenges involved in trying to keep up as the standard evolved. For example, the [Hashing to Elliptic Curves](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-hash-to-curve-09) standard was still changing [rather late](https://hackmd.io/@benjaminion/BkdbG45II#Multiclient-testnet-discussion) in the beacon chain testing phase. In the end, everything worked out fine.

The following two functions are described in the separate [BLS Extensions](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/bls.md) document, but included here for convenience.

#### `eth_aggregate_pubkeys`

<a id="def_eth_aggregate_pubkeys"></a>

```python
def eth_aggregate_pubkeys(pubkeys: Sequence[BLSPubkey]) -> BLSPubkey:
    """
    Return the aggregate public key for the public keys in ``pubkeys``.

    NOTE: the ``+`` operation should be interpreted as elliptic curve point addition, which takes as input
    elliptic curve points that must be decoded from the input ``BLSPubkey``s.
    This implementation is for demonstrative purposes only and ignores encoding/decoding concerns.
    Refer to the BLS signature draft standard for more information.
    """
    assert len(pubkeys) > 0
    # Ensure that the given inputs are valid pubkeys
    assert all(bls.KeyValidate(pubkey) for pubkey in pubkeys)

    result = copy(pubkeys[0])
    for pubkey in pubkeys[1:]:
        result += pubkey
    return result
```

Stand-alone aggregation of public keys is not defined by the BLS signature standard. In the standard, public keys are aggregated only in the context of performing an aggregate signature verification via `AggregateVerify()` or `FastAggregateVerify()`.

The `eth_aggregate_pubkeys()` function was added in the Altair upgrade to implement an [optimisation](/part3/containers/dependencies/#synccommittee) for light clients when verifying the signatures on `SyncAggregate`s.

|||
|-|------|
| Used&nbsp;by | [`get_next_sync_committee()`](/part3/helper/accessors/#def_get_next_sync_committee) |
| Uses | [`bls.KeyValidate()`](#bls-signatures) |

#### `eth_fast_aggregate_verify`

<a id="def_eth_fast_aggregate_verify"></a>

```python
def eth_fast_aggregate_verify(pubkeys: Sequence[BLSPubkey], message: Bytes32, signature: BLSSignature) -> bool:
    """
    Wrapper to ``bls.FastAggregateVerify`` accepting the ``G2_POINT_AT_INFINITY`` signature when ``pubkeys`` is empty.
    """
    if len(pubkeys) == 0 and signature == G2_POINT_AT_INFINITY:
        return True
    return bls.FastAggregateVerify(pubkeys, message, signature)
```

The specification of `FastAggregateVerify()` [in the BLS signature standard](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-bls-signature-04#section-3.3.4) returns `INVALID` if there are zero public keys given.

This function was introduced in Altair to handle [`SyncAggregate`](/part3/containers/operations/#syncaggregate)s that no sync committee member had signed off on, in which case the [`G2_POINT_AT_INFINITY`](/part3/config/constants/#g2_point_at_infinity) can be considered a "correct" signature (in our case, but not according to the standard).

The networking and validator specs were later clarified to require that `SyncAggregates` have [at least one signature](https://github.com/ethereum/consensus-specs/pull/2528). But this requirement is not enforced in the consensus layer (in [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate)), so we need to retain this `eth_fast_aggregate_verify()` wrapper to allow the empty signature to be valid.

|||
|-|------|
| Used&nbsp;by | [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) |
| Uses | [`FastAggregateVerify()`](#bls-signatures) |
| See&nbsp;also | [`G2_POINT_AT_INFINITY`](/part3/config/constants/#g2_point_at_infinity) |

### Predicates <!-- /part3/helper/predicates/ -->

#### `is_active_validator`

<a id="def_is_active_validator"></a>

```python
def is_active_validator(validator: Validator, epoch: Epoch) -> bool:
    """
    Check if ``validator`` is active.
    """
    return validator.activation_epoch <= epoch < validator.exit_epoch
```

Validators don't explicitly track their own state (eligible for activation, active, exited, withdrawable - the sole exception being whether they have been slashed or not). Instead, a validator's state is calculated by looking at the fields in the [`Validator`](/part3/containers/dependencies/#validator) record that store the epoch numbers of state transitions.

In this case, if the validator was activated in the past and has not yet exited, then it is active.

This is used a few times in the spec, most notably in [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices) which returns a list of all active validators at an epoch.

|||
|-|------|
| Used&nbsp;by | [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices), [`get_eligible_validator_indices()`](/part3/transition/epoch/#def_get_eligible_validator_indices), [`process_registry_updates()`](/part3/transition/epoch/#def_process_registry_updates), [`process_voluntary_exit()`](/part3/transition/block/#def_process_voluntary_exit) |
| See&nbsp;also | [`Validator`](/part3/containers/dependencies/#validator) |

#### `is_eligible_for_activation_queue`

<a id="def_is_eligible_for_activation_queue"></a>

```python
def is_eligible_for_activation_queue(validator: Validator) -> bool:
    """
    Check if ``validator`` is eligible to be placed into the activation queue.
    """
    return (
        validator.activation_eligibility_epoch == FAR_FUTURE_EPOCH
        and validator.effective_balance == MAX_EFFECTIVE_BALANCE
    )
```

When a deposit is [processed](/part3/transition/block/#deposits) with a previously unseen public key, a new [`Validator`](/part3/containers/dependencies/#validator) record is created with all the state-transition fields set to the default value of [`FAR_FUTURE_EPOCH`](/part3/config/constants/#far_future_epoch).

It is possible to deposit any amount over [`MIN_DEPOSIT_AMOUNT`](/part3/config/preset/#min_deposit_amount) (currently 1 Ether) into the deposit contract. However, validators do not become eligible for activation until their effective balance is equal to [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance), which corresponds to an actual balance of 32 Ether or more.

This predicate [is used](/part3/transition/epoch/#registry-updates) during epoch processing to find validators that have acquired the minimum necessary balance, but have not yet been added to the queue for activation. These validators are then marked as eligible for activation by setting the `validator.activation_eligibility_epoch` to the next epoch.

|||
|-|------|
| Used&nbsp;by | [`process_registry_updates()`](/part3/transition/epoch/#def_process_registry_updates) |
| See&nbsp;also | [`Validator`](/part3/containers/dependencies/#validator), [`FAR_FUTURE_EPOCH`](/part3/config/constants/#far_future_epoch), [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) |

#### `is_eligible_for_activation`

<a id="def_is_eligible_for_activation"></a>

```python
def is_eligible_for_activation(state: BeaconState, validator: Validator) -> bool:
    """
    Check if ``validator`` is eligible for activation.
    """
    return (
        # Placement in queue is finalized
        validator.activation_eligibility_epoch <= state.finalized_checkpoint.epoch
        # Has not yet been activated
        and validator.activation_epoch == FAR_FUTURE_EPOCH
    )
```

A validator that `is_eligible_for_activation()` has had its `activation_eligibility_epoch` [set](/part3/transition/epoch/#registry-updates), but its `activation_epoch` is not yet set.

To avoid any ambiguity or confusion on the validator side about its state, we wait until its eligibility activation epoch has been finalised before [adding it to the activation queue](/part3/transition/epoch/#registry-updates) by setting its `activation_epoch`. Otherwise, it might at one point become active, and then the beacon chain could flip to a fork in which it is not active. This could happen if the latter fork had fewer blocks and had thus processed fewer deposits.

Note that `state.finalized_checkpoint.epoch` does not mean that all of the slots in that epoch are finalised. We finalise checkpoints, not epochs, so only the first slot (the checkpoint) of that epoch is finalised. This is accounted for in [`process_registry_updates()`](/part3/transition/epoch/#def_process_registry_updates) by adding one to the current epoch when setting the `validator.activation_eligibility_epoch`, so that we can be sure that the block containing the deposit has been finalised.[^fn-activation-eligibility]

[^fn-activation-eligibility]: I'd have preferred not adding the one there, and using `<`, here. But it is what it is.

|||
|-|------|
| Used&nbsp;by | [`process_registry_updates()`](/part3/transition/epoch/#def_process_registry_updates) |
| See&nbsp;also | [`Validator`](/part3/containers/dependencies/#validator), [`FAR_FUTURE_EPOCH`](/part3/config/constants/#far_future_epoch) |

#### `is_slashable_validator`

<a id="def_is_slashable_validator"></a>

```python
def is_slashable_validator(validator: Validator, epoch: Epoch) -> bool:
    """
    Check if ``validator`` is slashable.
    """
    return (not validator.slashed) and (validator.activation_epoch <= epoch < validator.withdrawable_epoch)
```

Validators can be slashed only once: the flag [`validator.slashed`](/part3/containers/dependencies/#validator) is [set](/part3/helper/mutators/#def_slash_validator) when the first correct slashing report for the validator is processed.

An unslashed validator remains eligible to be slashed from when it becomes active right up until it becomes withdrawable. This is [`MIN_VALIDATOR_WITHDRAWABILITY_DELAY`](/part3/config/configuration/#min_validator_withdrawability_delay) epochs (around 27 hours) after it has exited from being a validator and ceased validation duties.

|||
|-|------|
| Used&nbsp;by | [`process_proposer_slashing()`](/part3/transition/block/#def_process_proposer_slashing), [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing) |
| See&nbsp;also | [`Validator`](/part3/containers/dependencies/#validator) |

#### `is_slashable_attestation_data`

<a id="def_is_slashable_attestation_data"></a>

```python
def is_slashable_attestation_data(data_1: AttestationData, data_2: AttestationData) -> bool:
    """
    Check if ``data_1`` and ``data_2`` are slashable according to Casper FFG rules.
    """
    return (
        # Double vote
        (data_1 != data_2 and data_1.target.epoch == data_2.target.epoch) or
        # Surround vote
        (data_1.source.epoch < data_2.source.epoch and data_2.target.epoch < data_1.target.epoch)
    )
```

This predicate is used by [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing) to check that the two sets of alleged conflicting attestation data in an [`AttesterSlashing`](/part3/containers/operations/#attesterslashing) do in fact qualify as slashable.

There are two ways for validators to get slashed under Casper FFG:

  1. A double vote: voting more than once for the same target epoch, or
  2. A surround vote: the source&ndash;target interval of one attestation entirely contains the source&ndash;target interval of a second attestation from the same validator or validators. The reporting block proposer needs to take care to order the `IndexedAttestation`s within the `AttesterSlashing` object so that the first set of votes surrounds the second. (The opposite ordering also describes a slashable offence, but is not checked for here, so the order of the arguments matters.)

It is far from obvious, but this predicate also enforces [LMD GHOST slashing](/part2/consensus/lmd_ghost/#attester-slashing) for attestation equivocation. The [`AttestationData`](/part3/containers/dependencies/#attestationdata) objects contain the LMD GHOST head vote (`beacon_block_root`) as well as the Casper FFG votes. So, the Casper FFG checkpoint votes might be identical and non-slashable, but if the LMD GHOST vote differs between the two attestations then it will be deemed slashable.

|||
|-|------|
| Used&nbsp;by | [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing) |
| See&nbsp;also | [`AttestationData`](/part3/containers/dependencies/#attestationdata), [`AttesterSlashing`](/part3/containers/operations/#attesterslashing) |

#### `is_valid_indexed_attestation`

<a id="def_is_valid_indexed_attestation"></a>

```python
def is_valid_indexed_attestation(state: BeaconState, indexed_attestation: IndexedAttestation) -> bool:
    """
    Check if ``indexed_attestation`` is not empty, has sorted and unique indices and has a valid aggregate signature.
    """
    # Verify indices are sorted and unique
    indices = indexed_attestation.attesting_indices
    if len(indices) == 0 or not indices == sorted(set(indices)):
        return False
    # Verify aggregate signature
    pubkeys = [state.validators[i].pubkey for i in indices]
    domain = get_domain(state, DOMAIN_BEACON_ATTESTER, indexed_attestation.data.target.epoch)
    signing_root = compute_signing_root(indexed_attestation.data, domain)
    return bls.FastAggregateVerify(pubkeys, signing_root, indexed_attestation.signature)
```

`is_valid_indexed_attestation()` is used in [attestation processing](/part3/transition/block/#attestations) and [attester slashing](/part3/transition/block/#attester-slashings).

[IndexedAttestation](/part3/containers/dependencies/#indexedattestation)s differ from [Attestation](/part3/containers/operations/#attestation)s in that the latter record the contributing validators in a bitlist and the former explicitly list the global indices of the contributing validators.

An [IndexedAttestation](/part3/containers/dependencies/#indexedattestation) passes this validity test only if all the following apply.

 1. There is at least one validator index present.
 2. The list of validators contains no duplicates (the Python `set` function performs deduplication).
 3. The indices of the validators are sorted. (It is not clear to me why this is required. It's used in the duplicate check here, but that could just be replaced by checking the set size.)
 4. Its aggregated signature verifies against the aggregated public keys of the listed validators.

Verifying the signature uses the magic of [aggregated BLS signatures](https://hackmd.io/@benjaminion/bls12-381#Aggregation). The indexed attestation contains a BLS signature that is supposed to be the combined individual signatures of each of the validators listed in the attestation. This is verified by passing it to `bls.FastAggregateVerify()` along with the list of public keys from the same validators. The verification succeeds only if exactly the same set of validators signed the message (`signing_root`) as appear in the list of public keys. Note that [`get_domain()`](/part3/helper/accessors/#def_get_domain) mixes in the fork version, so that attestations are not valid across forks.

No check is done here that the `attesting_indices` (which are the global validator indices) are all members of the correct committee for this attestation. In [`process_attestation()`](/part3/transition/block/#def_process_attestation) they must be, by construction. In [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing) it doesn't matter: _any_ validator signing conflicting attestations is liable to be slashed.

|||
|-|------|
| Used&nbsp;by | [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing), [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| Uses | [`get_domain()`](/part3/helper/accessors/#def_get_domain), [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root), [`bls.FastAggregateVerify()`](/part2/building_blocks/signatures/#bls-library-functions) |
| See&nbsp;also | [IndexedAttestation](/part3/containers/dependencies/#indexedattestation), [Attestation](/part3/containers/operations/#attestation) |

#### `is_valid_merkle_branch`

<a id="def_is_valid_merkle_branch"></a>

```python
def is_valid_merkle_branch(leaf: Bytes32, branch: Sequence[Bytes32], depth: uint64, index: uint64, root: Root) -> bool:
    """
    Check if ``leaf`` at ``index`` verifies against the Merkle ``root`` and ``branch``.
    """
    value = leaf
    for i in range(depth):
        if index // (2**i) % 2:
            value = hash(branch[i] + value)
        else:
            value = hash(value + branch[i])
    return value == root
```

This is the classic algorithm for [verifying a Merkle branch](https://blog.ethereum.org/2015/11/15/merkling-in-ethereum/) (also called a Merkle proof). Nodes are iteratively hashed as the tree is traversed from leaves to root. The bits of `index` select whether we are the right or left child of our parent at each level. The result should match the given `root` of the tree.

In this way we prove that we know that `leaf` is the value at position `index` in the list of leaves, and that we know the whole structure of the rest of the tree, as summarised in `branch`.

We use this function in [`process_deposit()`](/part3/transition/block/#def_process_deposit) to check whether the deposit data we've received is correct or not. Based on the deposit data they have seen, Eth2 clients build a replica of the Merkle tree of deposits in the [deposit contract](/part2/deposits-withdrawals/contract/). The proposer of the block that includes the deposit constructs the Merkle proof using its view of the deposit contract, and all other nodes use `is_valid_merkle_branch()` to check that their view matches the proposer's. If any deposit fails Merkle branch verification then the entire block is invalid.

|||
|-|------|
| Used&nbsp;by | [`process_deposit()`](/part3/transition/block/#def_process_deposit) |

#### `is_merge_transition_complete`

<a id="def_is_merge_transition_complete"></a>

```python
def is_merge_transition_complete(state: BeaconState) -> bool:
    return state.latest_execution_payload_header != ExecutionPayloadHeader()
```

A simple test for whether the given beacon state is pre- or post-Merge. If the `latest_execution_payload_header` in the state is the default `ExecutionPayloadHeader` then the chain is pre-Merge, otherwise it is post-Merge. Upgrades normally occur at a predetermined block height (or epoch number on the beacon chain), and that's the usual way to test for them. The block height of the Merge, however, was unknown ahead of time, so a different kind of test was required.

Although the mainnet beacon chain is decidedly post-Merge now, this remains useful for syncing nodes from pre-Merge starting points.

This function was added in the Bellatrix pre-Merge upgrade.

|||
|-|------|
| Used&nbsp;by | [`process_execution_payload()`](/part3/transition/block/#def_process_execution_payload), [`is_merge_transition_block()`](#def_is_merge_transition_block), [`is_execution_enabled()`](#def_is_execution_enabled) |
| See&nbsp;also | [`ExecutionPayloadHeader`](/part3/containers/execution/#executionpayloadheader) |

#### `is_merge_transition_block`

<a id="def_is_merge_transition_block"></a>

```python
def is_merge_transition_block(state: BeaconState, body: BeaconBlockBody) -> bool:
    return not is_merge_transition_complete(state) and body.execution_payload != ExecutionPayload()
```

If the Merge transition is not complete (meaning that the beacon state still has the default execution payload header in it), yet our block has a non-default execution payload, then this must be the first block we've seen with an execution payload. It is therefore the Merge transition block.

[TODO - link to Merge transition info]::

This function was added in the Bellatrix pre-Merge upgrade.

|||
|-|------|
| Uses | [`is_merge_transition_complete()`](#def_is_merge_transition_complete) |
| Used&nbsp;by | [`is_execution_enabled()`](#def_is_execution_enabled), [`on_block()`](/part3/forkchoice/bellatrix/#on_block) (Bellatrix version) |
| See&nbsp;also | [`ExecutionPayload`](/part3/containers/execution/#executionpayload) |

#### `is_execution_enabled`

<a id="def_is_execution_enabled"></a>

```python
def is_execution_enabled(state: BeaconState, body: BeaconBlockBody) -> bool:
    return is_merge_transition_block(state, body) or is_merge_transition_complete(state)
```

If the block that we have is the first block with an execution payload (the Merge transition block), or we know from the state that we have previously seen a block with an execution payload then execution is enabled, the execution and consensus chains have Merged.

This function was added in the Bellatrix pre-Merge upgrade.

|||
|-|------|
| Uses | [`is_merge_transition_block()`](#def_is_merge_transition_block), [`is_merge_transition_complete()`](#def_is_merge_transition_complete) |
| Used&nbsp;by | [`process_block()`](/part3/transition/block/#def_process_block) |

#### `has_eth1_withdrawal_credential`

<a id="def_has_eth1_withdrawal_credential"></a>

```python
def has_eth1_withdrawal_credential(validator: Validator) -> bool:
    """
    Check if ``validator`` has an 0x01 prefixed "eth1" withdrawal credential.
    """
    return validator.withdrawal_credentials[:1] == ETH1_ADDRESS_WITHDRAWAL_PREFIX
```

Only validators that have [Eth1 withdrawal credentials](/part3/config/constants/#withdrawal-prefixes) are eligible for balance withdrawals of any sort.

|||
|-|------|
| Used&nbsp;by | [`is_fully_withdrawable_validator()`](#def_is_fully_withdrawable_validator), [`is_partially_withdrawable_validator()`](#is_partially_withdrawable_validator) |
| See&nbsp;also | [`ETH1_ADDRESS_WITHDRAWAL_PREFIX`](/part3/config/constants/#eth1_address_withdrawal_prefix) |

#### `is_fully_withdrawable_validator`

<a id="def_is_fully_withdrawable_validator"></a>

```python
def is_fully_withdrawable_validator(validator: Validator, balance: Gwei, epoch: Epoch) -> bool:
    """
    Check if ``validator`` is fully withdrawable.
    """
    return (
        has_eth1_withdrawal_credential(validator)
        and validator.withdrawable_epoch <= epoch
        and balance > 0
    )
```

A validator is fully withdrawable only when (a) it has an [Eth1 withdrawal credential](/part3/config/constants/#withdrawal-prefixes) to make the withdrawal to, (b) it has become withdrawable, meaning that its exit has been processed and it has passed through its [`MIN_VALIDATOR_WITHDRAWABILITY_DELAY`](/part3/config/configuration/#min_validator_withdrawability_delay) period, and (c) it has a nonzero balance.

|||
|-|------|
| Uses | [`has_eth1_withdrawal_credential()`](#def_has_eth1_withdrawal_credential) |
| Used&nbsp;by | [`get_expected_withdrawals()`](/part3/transition/block/#def_get_expected_withdrawals) |

#### `is_partially_withdrawable_validator`

<a id="def_is_partially_withdrawable_validator"></a>

```python
def is_partially_withdrawable_validator(validator: Validator, balance: Gwei) -> bool:
    """
    Check if ``validator`` is partially withdrawable.
    """
    has_max_effective_balance = validator.effective_balance == MAX_EFFECTIVE_BALANCE
    has_excess_balance = balance > MAX_EFFECTIVE_BALANCE
    return has_eth1_withdrawal_credential(validator) and has_max_effective_balance and has_excess_balance
```

A partial withdrawal is the withdrawal of excess Ether from an active (non-exited) validator.

A validator has excess Ether only when (a) it's effective balance is at [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance), (b) its actual balance is greater than MAX_EFFECTIVE_BALANCE, and (c) it has an Eth1 withdrawal credential to make the withdrawal to.

The first of these conditions is related to the [hysteresis](/part2/incentives/balances/#hysteresis) in the effective balance. If a validator has previously suffered a drop in its balance, it's effective balance might be 31 Ether even while its actual balance is greater than 32 Ether. If we were to start skimming withdrawals in this situation, the validator's balance would never reach the 32.25 Ether necessary to bring its effective balance up to 32 Ether, and it would be forever stuck at 31 ETH. Therefore, only validators with the full effective balance are eligible for the excess to be withdrawn.

|||
|-|------|
| Used&nbsp;by | [`get_expected_withdrawals()`](/part3/transition/block/#def_get_expected_withdrawals) |
| Uses | [`has_eth1_withdrawal_credential()`](#def_has_eth1_withdrawal_credential) |
| See&nbsp;also | [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance), [hysteresis](/part2/incentives/balances/#hysteresis) |

### Misc <!-- /part3/helper/misc/ -->

#### `compute_shuffled_index`

<a id="def_compute_shuffled_index"></a>

```python
def compute_shuffled_index(index: uint64, index_count: uint64, seed: Bytes32) -> uint64:
    """
    Return the shuffled index corresponding to ``seed`` (and ``index_count``).
    """
    assert index < index_count

    # Swap or not (https://link.springer.com/content/pdf/10.1007%2F978-3-642-32009-5_1.pdf)
    # See the 'generalized domain' algorithm on page 3
    for current_round in range(SHUFFLE_ROUND_COUNT):
        pivot = bytes_to_uint64(hash(seed + uint_to_bytes(uint8(current_round)))[0:8]) % index_count
        flip = (pivot + index_count - index) % index_count
        position = max(index, flip)
        source = hash(
            seed
            + uint_to_bytes(uint8(current_round))
            + uint_to_bytes(uint32(position // 256))
        )
        byte = uint8(source[(position % 256) // 8])
        bit = (byte >> (position % 8)) % 2
        index = flip if bit else index

    return index
```

Selecting random, distinct committees of validators is a big part of Ethereum&nbsp;2.0; it is foundational for both its scalability and security. This selection is done by shuffling.

Shuffling a list of objects is a well understood problem in computer science.  Notice, however, that this routine manages to shuffle a _single index_ to a new location, knowing only the total length of the list. To use the technical term for this, it is _oblivious_. To shuffle the whole list, this routine needs to be called once per validator index in the list. By construction, each input index maps to a distinct output index. Thus, when applied to all indices in the list, it results in a permutation, also called a shuffling.

Why do this rather than a simpler, more efficient, conventional shuffle? It's all about light clients. Beacon nodes will generally need to know the whole shuffling, but light clients will often be interested only in a small number of committees. Using this technique allows the composition of a single committee to be calculated without having to shuffle the entire set: potentially a big saving on time and memory.

As stated in the code comments, this is an implementation of the "swap-or-not" shuffle, described in [the cited paper](https://link.springer.com/content/pdf/10.1007%2F978-3-642-32009-5_1.pdf). Vitalik [kicked off a search](https://github.com/ethereum/consensus-specs/issues/323) for a shuffle with these properties in late 2018. With the help of Professor Dan Boneh of Stanford University, the swap-or-not [was identified](https://github.com/ethereum/consensus-specs/issues/563) as a candidate a couple of months later, and [adopted](https://github.com/ethereum/consensus-specs/pull/576) into the spec.

The algorithm breaks down as follows. For each iteration (each round), we start with a current `index`.

1. Pseudo-randomly select a pivot. This is a 64-bit integer based on the seed and current round number. This domain is large enough that any non-uniformity caused by taking the modulus in the next step is [entirely negligible](https://github.com/ethereum/consensus-specs/pull/576#issuecomment-463293660).
2. Use `pivot` to find another index in the list of validators, `flip`, which is `pivot - index` accounting for wrap-around in the list.
3. Calculate a single pseudo-random bit based on the seed, the current round number, and some bytes from either `index` or `flip` depending on which is greater.
4. If our bit is zero, we keep `index` unchanged; if it is one, we set `index` to `flip`.

We are effectively swapping cards in a deck based on a deterministic algorithm.

The way that `position` is broken down is worth noting:

  - Bits 0-2 (3 bits) are used to select a single bit from the eight bits of `byte`.
  - Bits 3-7 (5 bits) are used to select a single byte from the thirty-two bytes of `source`.
  - Bits 8-39 (32 bits) are used in generating `source`. Note that the upper two bytes of this will always be zero in practice, due to limits on the number of active validators.

[`SHUFFLE_ROUND_COUNT`](/part3/config/preset/#shuffle_round_count) is, and always has been, 90 in the mainnet configuration, as explained there.

See the [section on Shuffling](/part2/building_blocks/shuffling/) for a more structured exposition and analysis of this algorithm (with diagrams!).

In practice, full beacon node implementations will run this once per epoch using an optimised version that shuffles the whole list, and cache the result of that for the epoch.

|||
|-|------|
| Used&nbsp;by | [`compute_committee()`](#def_compute_committee), [`compute_proposer_index()`](#def_compute_proposer_index), [`get_next_sync_committee_indices()`](/part3/helper/accessors/#def_get_next_sync_committee_indices) |
| Uses | [`bytes_to_uint64()`](/part3/helper/math/#def_bytes_to_uint64) |
| See&nbsp;also | [`SHUFFLE_ROUND_COUNT`](/part3/config/preset/#shuffle_round_count) |

#### `compute_proposer_index`

<a id="def_compute_proposer_index"></a>

```python
def compute_proposer_index(state: BeaconState, indices: Sequence[ValidatorIndex], seed: Bytes32) -> ValidatorIndex:
    """
    Return from ``indices`` a random index sampled by effective balance.
    """
    assert len(indices) > 0
    MAX_RANDOM_BYTE = 2**8 - 1
    i = uint64(0)
    total = uint64(len(indices))
    while True:
        candidate_index = indices[compute_shuffled_index(i % total, total, seed)]
        random_byte = hash(seed + uint_to_bytes(uint64(i // 32)))[i % 32]
        effective_balance = state.validators[candidate_index].effective_balance
        if effective_balance * MAX_RANDOM_BYTE >= MAX_EFFECTIVE_BALANCE * random_byte:
            return candidate_index
        i += 1
```

There is exactly one beacon block proposer per slot, selected randomly from among all the active validators. The seed parameter is set in [`get_beacon_proposer_index`](/part3/helper/accessors/#get_beacon_proposer_index) based on the epoch and slot. Note that there is a small but finite probability of the same validator being called on to propose a block more than once in an epoch.

A validator's chance of being the proposer is [weighted](https://github.com/ethereum/consensus-specs/pull/772) by its effective balance: a validator with a 32 Ether effective balance is twice as likely to be chosen as a validator with a 16 Ether effective balance.

To account for the need to weight by [effective balance](/part2/incentives/balances/), this function implements as a try-and-increment algorithm. A counter `i` starts at zero. This counter does double duty:

  - First `i` is used to uniformly select a candidate proposer with probability $1/N$ where, $N$ is the number of active validators. This is done by using the [`compute_shuffled_index`](#compute_shuffled_index) routine to shuffle index `i` to a new location, which is then the `candidate_index`.
  - Then `i` is used to generate a pseudo-random byte using the hash function as a seeded PRNG with at least 256 bits of output. The lower 5 bits of `i` select a byte in the hash function, and the upper bits salt the seed. (An obvious optimisation is that the output of the hash changes only once every 32 iterations.)

The `if` test is where the weighting by effective balance is done. If the candidate has `MAX_EFFECTIVE_BALANCE`, it will always pass this test and be returned as the proposer. If the candidate has a fraction of `MAX_EFFECTIVE_BALANCE` then that fraction is the probability of being returned as proposer.

If the candidate is not chosen, then `i` is incremented, and we try again. Since the minimum effective balance is half of the maximum, then this ought to terminate fairly swiftly. In the worst case, all validators have 16 Ether effective balance, so the chance of having to do another iteration is 50%, in which case there is a one in a million chance of having to do 20 iterations.

Note that this dependence on the validators' effective balances, which are updated at the end of each epoch, means that proposer assignments are valid [only in the current epoch](https://github.com/ethereum/consensus-specs/pull/772#issuecomment-475574357). This is different from attestation committee assignments, which are valid with a one epoch look-ahead.

|||
|-|------|
| Used&nbsp;by | [`get_beacon_proposer_index()`](/part3/helper/accessors/#def_get_beacon_proposer_index) |
| Uses | [`compute_shuffled_index()`](#def_compute_shuffled_index) |
| See&nbsp;also | [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) |

#### `compute_committee`

<a id="def_compute_committee"></a>

```python
def compute_committee(indices: Sequence[ValidatorIndex],
                      seed: Bytes32,
                      index: uint64,
                      count: uint64) -> Sequence[ValidatorIndex]:
    """
    Return the committee corresponding to ``indices``, ``seed``, ``index``, and committee ``count``.
    """
    start = (len(indices) * index) // count
    end = (len(indices) * uint64(index + 1)) // count
    return [indices[compute_shuffled_index(uint64(i), uint64(len(indices)), seed)] for i in range(start, end)]
```

`compute_committee` is used by [`get_beacon_committee()`](/part3/helper/accessors/#get_beacon_committee) to find the specific members of one of the committees at a slot.

Every epoch, a fresh set of committees is generated; during an epoch, the committees are stable.

Looking at the parameters in reverse order:

  - `count` is the total number of committees in an epoch. This is `SLOTS_PER_EPOCH` times the output of [`get_committee_count_per_slot()`](/part3/helper/accessors/#def_get_committee_count_per_slot).
  - `index` is the committee number within the epoch, running from `0` to `count - 1`. It is calculated in [`get_beacon_committee()`](/part3/helper/accessors/#def_get_beacon_committee) from the committee number in the slot `index` and the slot number as `(slot % SLOTS_PER_EPOCH) * committees_per_slot + index`.
  - `seed` is the seed value for computing the pseudo-random shuffling, based on the epoch number and a domain parameter. ([`get_beacon_committee()`](/part3/helper/accessors/#def_get_beacon_committee) uses [`DOMAIN_BEACON_ATTESTER`](/part3/config/constants/#domain_beacon_attester).)
  - `indices` is the list of validators eligible for inclusion in committees, namely the whole list of indices of active validators.

Random sampling among the validators is done by taking a contiguous slice of array indices from `start` to `end` and seeing where each one gets shuffled to by `compute_shuffled_index()`. Note that `ValidatorIndex(i)` is a type-cast in the above: it just turns `i` into a [ValidatorIndex](/part3/config/types/#validatorindex) type for input into the shuffling. The output value of the shuffling is then used as an index into the `indices` list. There is much here that client implementations will optimise with caching and batch operations.

It may not be immediately obvious, but not all committees returned will be the same size (they can vary by one), and every validator in `indices` will be a member of exactly one committee. As we increment `index` from zero, clearly `start` for `index == j + 1` is `end` for `index == j`, so there are no gaps. In addition, the highest `index` is `count - 1`, so every validator in `indices` finds its way into a committee.[^fn_formal_verif_committee_size]

[^fn_formal_verif_committee_size]: Also not immediately obvious is that there is a subtle issue with committee sizes that was [discovered by formal verification](https://github.com/ethereum/consensus-specs/issues/2500), although, given the max supply of ETH it will never be triggered.

This method of selecting committees is light client friendly. Light clients can compute only the committees that they are interested in without needing to deal with the entire validator set. See the [section on Shuffling](/part2/building_blocks/shuffling/) for explanation of how this works.

Sync committees are assigned by a [different process](/part3/helper/accessors/#get_next_sync_committee_indices) that is more akin to repeatedly performing [`compute_proposer_index()`](#def_compute_proposer_index).

|||
|-|------|
| Used&nbsp;by | [`get_beacon_committee`](/part3/helper/accessors/#get_beacon_committee) |
| Uses | [`compute_shuffled_index()`](#compute_shuffled_index) |

#### `compute_epoch_at_slot`

<a id="def_compute_epoch_at_slot"></a>

```python
def compute_epoch_at_slot(slot: Slot) -> Epoch:
    """
    Return the epoch number at ``slot``.
    """
    return Epoch(slot // SLOTS_PER_EPOCH)
```

This is trivial enough that I won't explain it. But note that it does rely on [`GENESIS_SLOT`](/part3/config/constants/#miscellaneous) and [`GENESIS_EPOCH`](/part3/config/constants/#miscellaneous) being zero. The more pernickety among us might prefer it to read,

```code
    return GENESIS_EPOCH + Epoch((slot - GENESIS_SLOT) // SLOTS_PER_EPOCH)
```

#### `compute_start_slot_at_epoch`

<a id="def_compute_start_slot_at_epoch"></a>

```python
def compute_start_slot_at_epoch(epoch: Epoch) -> Slot:
    """
    Return the start slot of ``epoch``.
    """
    return Slot(epoch * SLOTS_PER_EPOCH)
```

Maybe should read,

```code
    return GENESIS_SLOT + Slot((epoch - GENESIS_EPOCH) * SLOTS_PER_EPOCH))
```

|||
|-|------|
| Used&nbsp;by | [`get_block_root()`](/part3/helper/accessors/#def_get_block_root), [`compute_slots_since_epoch_start()`](/part3/forkchoice/phase0/#compute_slots_since_epoch_start) |
| See&nbsp;also | [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch), [`GENESIS_SLOT`](/part3/config/constants/#genesis_slot), [`GENESIS_EPOCH`](/part3/config/constants/#genesis_epoch) |

#### `compute_activation_exit_epoch`

<a id="def_compute_activation_exit_epoch"></a>

```python
def compute_activation_exit_epoch(epoch: Epoch) -> Epoch:
    """
    Return the epoch during which validator activations and exits initiated in ``epoch`` take effect.
    """
    return Epoch(epoch + 1 + MAX_SEED_LOOKAHEAD)
```

When queuing validators for activation or exit in [`process_registry_updates()`](/part3/transition/epoch/#def_process_registry_updates) and [`initiate_validator_exit()`](/part3/helper/mutators/#def_initiate_validator_exit) respectively, the activation or exit is delayed until the next epoch, plus [`MAX_SEED_LOOKAHEAD`](/part3/config/preset/#time-parameters) epochs, currently 4.

See [`MAX_SEED_LOOKAHEAD`](/part3/config/preset/#time-parameters) for the details, but in short it is designed to make it extremely hard for an attacker to manipulate the membership of committees via activations and exits.

|||
|-|------|
| Used&nbsp;by | [`initiate_validator_exit()`](/part3/helper/mutators/#def_initiate_validator_exit), [`process_registry_updates()`](/part3/transition/epoch/#def_process_registry_updates) |
| See&nbsp;also | [`MAX_SEED_LOOKAHEAD`](/part3/config/preset/#time-parameters) |

#### `compute_fork_data_root`

<a id="def_compute_fork_data_root"></a>

```python
def compute_fork_data_root(current_version: Version, genesis_validators_root: Root) -> Root:
    """
    Return the 32-byte fork data root for the ``current_version`` and ``genesis_validators_root``.
    This is used primarily in signature domains to avoid collisions across forks/chains.
    """
    return hash_tree_root(ForkData(
        current_version=current_version,
        genesis_validators_root=genesis_validators_root,
    ))
```

The fork data root serves as a unique identifier for the chain that we are on. `genesis_validators_root` identifies our unique genesis event, and `current_version` our own hard fork subsequent to that genesis event. This is useful, for example, to differentiate between a testnet and mainnet: both might have the same fork versions, but will definitely have different genesis validator roots.

It is used by [`compute_fork_digest()`](#def_compute_fork_digest) and [`compute_domain()`](#def_compute_domain).

|||
|-|------|
| Used&nbsp;by | [`compute_fork_digest()`](#def_compute_fork_digest), [`compute_domain()`](#def_compute_domain) |
| Uses | [`hash_tree_root()`](/part3/helper/crypto/#hash_tree_root) |
| See&nbsp;also | [`ForkData`](/part3/containers/dependencies/#forkdata) |

#### `compute_fork_digest`

<a id="def_compute_fork_digest"></a>

```python
def compute_fork_digest(current_version: Version, genesis_validators_root: Root) -> ForkDigest:
    """
    Return the 4-byte fork digest for the ``current_version`` and ``genesis_validators_root``.
    This is a digest primarily used for domain separation on the p2p layer.
    4-bytes suffices for practical separation of forks/chains.
    """
    return ForkDigest(compute_fork_data_root(current_version, genesis_validators_root)[:4])
```

Extracts the first four bytes of the [fork data root](#compute_fork_data_root) as a [`ForkDigest`](/part3/config/types/#forkdigest) type. It is primarily used for domain separation on the peer-to-peer networking layer.

`compute_fork_digest()` is used extensively in the [Ethereum 2.0 networking specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/p2p-interface.md#how-should-fork-version-be-used-in-practice) to distinguish between independent beacon chain networks or forks: it is important that activity on one chain does not interfere with other chains.

|||
|-|------|
| Uses | [`compute_fork_data_root()`](#def_compute_fork_data_root) |
| See&nbsp;also | [`ForkDigest`](/part3/config/types/#forkdigest) |

#### `compute_domain`

<a id="def_compute_domain"></a>

```python
def compute_domain(domain_type: DomainType, fork_version: Version=None, genesis_validators_root: Root=None) -> Domain:
    """
    Return the domain for the ``domain_type`` and ``fork_version``.
    """
    if fork_version is None:
        fork_version = GENESIS_FORK_VERSION
    if genesis_validators_root is None:
        genesis_validators_root = Root()  # all bytes zero by default
    fork_data_root = compute_fork_data_root(fork_version, genesis_validators_root)
    return Domain(domain_type + fork_data_root[:28])
```

When dealing with signed messages, the signature "domains" are separated according to three independent factors:

 1. All signatures include a [`DomainType`](/part3/config/constants/#domain-types) relevant to the message's purpose, which is just some cryptographic hygiene in case the same message is to be signed for different purposes at any point.
 2. All but signatures on deposit messages include the fork version. This ensures that messages across different forks of the chain become invalid, and that validators won't be slashed for signing attestations on two different chains (this is allowed).
 3. And, [now](https://github.com/ethereum/consensus-specs/pull/1614), the root hash of the validator Merkle tree at Genesis is included. Along with the fork version this gives a unique identifier for our chain.

This function is mainly used by [`get_domain()`](/part3/helper/accessors/#def_get_domain). It is also used in [deposit processing](/part3/transition/block/#deposits), in which case `fork_version` and `genesis_validators_root` take their default values since deposits are valid across forks.

Fun fact: this function looks pretty simple, but [I found a subtle bug](https://github.com/ethereum/consensus-specs/issues/1582) in the way tests were generated in a previous implementation.

|||
|-|------|
| Used&nbsp;by | [`get_domain()`](/part3/helper/accessors/#def_get_domain), [`process_deposit()`](/part3/transition/block/#def_process_deposit) |
| Uses | [`compute_fork_data_root()`](#def_compute_fork_data_root) |
| See&nbsp;also | [`Domain`](/part3/config/types/#domain), [`DomainType`](/part3/config/constants/#domain-types) [`GENESIS_FORK_VERSION`](/part3/config/configuration/#genesis_fork_version) |

#### `compute_signing_root`

<a id="def_compute_signing_root"></a>

```python
def compute_signing_root(ssz_object: SSZObject, domain: Domain) -> Root:
    """
    Return the signing root for the corresponding signing data.
    """
    return hash_tree_root(SigningData(
        object_root=hash_tree_root(ssz_object),
        domain=domain,
    ))
```

This is a pre-processor for signing objects with BLS signatures:

 1. calculate the [hash tree root](/part2/building_blocks/merkleization/#the-hash-tree-root) of the object;
 2. combine the hash tree root with the [`Domain`](/part3/config/types/#domain) inside a temporary [`SigningData`](/part3/containers/dependencies/#signingdata) object;
 3. return the hash tree root of that, which is the data to be signed.

The `domain` is usually the output of [`get_domain()`](/part3/helper/accessors/#def_get_domain), which mixes in the [cryptographic domain](/part3/config/constants/#domain-types), the fork version, and the genesis validators root to the message hash. For deposits, it is the output of [`compute_domain()`](#def_compute_domain), ignoring the fork version and genesis validators root.

This is exactly equivalent to adding the domain to an object and taking the hash tree root of the whole thing. Indeed, this function used to be called [`compute_domain_wrapper_root()`](https://github.com/ethereum/consensus-specs/blob/502ee295379c1f3c5c3649e12330fb5be5d7a83b/specs/core/0_beacon-chain.md#compute_domain_wrapper_root).

|||
|-|------|
| Used&nbsp;by | Many places |
| Uses | [`hash_tree_root()`](/part3/helper/crypto/#hash_tree_root) |
| See&nbsp;also | [`SigningData`](/part3/containers/dependencies/#signingdata), [`Domain`](/part3/config/types/#domain) |

#### `compute_timestamp_at_slot`

> _Note_: This function is unsafe with respect to overflows and underflows.

<a id="def_compute_timestamp_at_slot"></a>

```python
def compute_timestamp_at_slot(state: BeaconState, slot: Slot) -> uint64:
    slots_since_genesis = slot - GENESIS_SLOT
    return uint64(state.genesis_time + slots_since_genesis * SECONDS_PER_SLOT)
```

A simple utility for calculating the Unix timestamp at the start of the given slot. This is used when [validating execution payloads](/part3/transition/block/#process_execution_payload).

This function was added in the Bellatrix pre-Merge upgrade.

|||
|-|------|
| Used&nbsp;by | [`process_execution_payload()`](/part3/transition/block/#def_process_execution_payload) |

### Participation flags <!-- /part3/helper/participation/ -->

These two simple utilities were added in the Altair upgrade.

#### `add_flag`

<a id="def_add_flag"></a>

```python
def add_flag(flags: ParticipationFlags, flag_index: int) -> ParticipationFlags:
    """
    Return a new ``ParticipationFlags`` adding ``flag_index`` to ``flags``.
    """
    flag = ParticipationFlags(2**flag_index)
    return flags | flag
```

This is simple and self-explanatory. The `2**flag_index` is a bit Pythonic. In a C-like language it would use a bit-shift:

```code
    1 << flag_index
```

|||
|-|------|
| Used&nbsp;by | [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| See&nbsp;also | [`ParticipationFlags`](/part3/config/types/#participationflags) |

#### `has_flag`

<a id="def_has_flag"></a>

```python
def has_flag(flags: ParticipationFlags, flag_index: int) -> bool:
    """
    Return whether ``flags`` has ``flag_index`` set.
    """
    flag = ParticipationFlags(2**flag_index)
    return flags & flag == flag
```

Move along now, nothing to see here.

|||
|-|------|
| Used&nbsp;by | [`get_unslashed_participating_indices()`](/part3/helper/accessors/#def_get_unslashed_participating_indices), [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| See&nbsp;also | [`ParticipationFlags`](/part3/config/types/#participationflags) |

### Beacon State Accessors <!-- /part3/helper/accessors/ -->

As the name suggests, these functions access the beacon state to calculate various useful things, without modifying it.

#### `get_current_epoch`

<a id="def_get_current_epoch"></a>

```python
def get_current_epoch(state: BeaconState) -> Epoch:
    """
    Return the current epoch.
    """
    return compute_epoch_at_slot(state.slot)
```

A getter for the current epoch, as calculated by [`compute_epoch_at_slot()`](/part3/helper/misc/#def_compute_epoch_at_slot).

|||
|-|------|
| Used&nbsp;by | Everywhere |
| Uses | [`compute_epoch_at_slot()`](/part3/helper/misc/#def_compute_epoch_at_slot) |

#### `get_previous_epoch`

<a id="def_get_previous_epoch"></a>

```python
def get_previous_epoch(state: BeaconState) -> Epoch:
    """`
    Return the previous epoch (unless the current epoch is ``GENESIS_EPOCH``).
    """
    current_epoch = get_current_epoch(state)
    return GENESIS_EPOCH if current_epoch == GENESIS_EPOCH else Epoch(current_epoch - 1)
```

Return the previous epoch number as an [`Epoch`](/part3/config/types/#epoch) type. Returns [`GENESIS_EPOCH`](/part3/config/constants/#genesis_epoch) if we are in the `GENESIS_EPOCH`, since it has no prior, and we don't do negative numbers.

|||
|-|------|
| Used&nbsp;by | Everywhere |
| Uses | [`get_current_epoch()`](#def_get_current_epoch) |
| See&nbsp;also | [`GENESIS_EPOCH`](/part3/config/constants/#genesis_epoch) |

#### `get_block_root`

<a id="def_get_block_root"></a>

```python
def get_block_root(state: BeaconState, epoch: Epoch) -> Root:
    """
    Return the block root at the start of a recent ``epoch``.
    """
    return get_block_root_at_slot(state, compute_start_slot_at_epoch(epoch))
```

The Casper FFG part of consensus deals in [`Checkpoint`](/part3/containers/dependencies/#checkpoint)s that are the first slot of an epoch. `get_block_root` is a specialised version of [`get_block_root_at_slot()`](#get_block_root_at_slot) that returns the block root of the checkpoint, given only an epoch.

|||
|-|------|
| Used&nbsp;by | [`get_attestation_participation_flag_indices()`](#def_get_attestation_participation_flag_indices), [`weigh_justification_and_finalization()`](/part3/transition/epoch/#def_weigh_justification_and_finalization) |
| Uses | [`get_block_root_at_slot()`](#def_get_block_root_at_slot), [`compute_start_slot_at_epoch()`](/part3/helper/misc/#def_compute_start_slot_at_epoch) |
| See&nbsp;also | [`Root`](/part3/config/types/#root) |

#### `get_block_root_at_slot`

<a id="def_get_block_root_at_slot"></a>

```python
def get_block_root_at_slot(state: BeaconState, slot: Slot) -> Root:
    """
    Return the block root at a recent ``slot``.
    """
    assert slot < state.slot <= slot + SLOTS_PER_HISTORICAL_ROOT
    return state.block_roots[slot % SLOTS_PER_HISTORICAL_ROOT]
```

Recent block roots are stored in a circular list in state, with a length of [`SLOTS_PER_HISTORICAL_ROOT`](/part3/config/preset/#slots_per_historical_root) (currently ~27 hours).

`get_block_root_at_slot()` is used by [`get_attestation_participation_flag_indices()`](#def_get_attestation_participation_flag_indices) to check whether an attestation has voted for the correct chain head. It is also used in [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) to find the block that the sync committee is signing-off on.

|||
|-|------|
| Used&nbsp;by | [`get_block_root()`](#def_get_block_root), [`get_attestation_participation_flag_indices()`](#def_get_attestation_participation_flag_indices), [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) |
| See&nbsp;also | [`SLOTS_PER_HISTORICAL_ROOT`](/part3/config/preset/#slots_per_historical_root), [`Root`](/part3/config/types/#root) |

#### `get_randao_mix`

<a id="def_get_randao_mix"></a>

```python
def get_randao_mix(state: BeaconState, epoch: Epoch) -> Bytes32:
    """
    Return the randao mix at a recent ``epoch``.
    """
    return state.randao_mixes[epoch % EPOCHS_PER_HISTORICAL_VECTOR]
```

RANDAO mixes are stored in a circular list of length [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector). They are used when calculating the [seed](#get_seed) for assigning beacon proposers and committees.

The RANDAO mix for the current epoch is updated on a block-by-block basis as new RANDAO reveals come in. The mixes for previous epochs are the frozen RANDAO values at the end of the epoch.

|||
|-|------|
| Used&nbsp;by | [`get_seed`](#def_get_seed), [`process_randao_mixes_reset()`](/part3/transition/epoch/#def_process_randao_mixes_reset), [`process_randao()`](/part3/transition/block/#def_process_randao) |
| See&nbsp;also | [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector) |

#### `get_active_validator_indices`

<a id="def_get_active_validator_indices"></a>

```python
def get_active_validator_indices(state: BeaconState, epoch: Epoch) -> Sequence[ValidatorIndex]:
    """
    Return the sequence of active validator indices at ``epoch``.
    """
    return [ValidatorIndex(i) for i, v in enumerate(state.validators) if is_active_validator(v, epoch)]
```

Steps through the entire list of validators and returns the list of only the active ones. That is, the list of validators that have been activated but not exited as determined by [`is_active_validator()`](/part3/helper/predicates/#def_is_active_validator).

This function is heavily used, and I'd expect it to be [memoised](https://en.wikipedia.org/wiki/Memoization) in practice.

|||
|-|------|
| Used&nbsp;by | Many places |
| Uses | [`is_active_validator()`](/part3/helper/predicates/#def_is_active_validator) |

#### `get_validator_churn_limit`

<a id="def_get_validator_churn_limit"></a>

```python
def get_validator_churn_limit(state: BeaconState) -> uint64:
    """
    Return the validator churn limit for the current epoch.
    """
    active_validator_indices = get_active_validator_indices(state, get_current_epoch(state))
    return max(MIN_PER_EPOCH_CHURN_LIMIT, uint64(len(active_validator_indices)) // CHURN_LIMIT_QUOTIENT)
```

The "churn limit" applies when [activating](/part3/transition/epoch/#registry-updates) and [exiting](/part3/helper/mutators/#initiate_validator_exit) validators and acts as a [rate-limit](https://notes.ethereum.org/@vbuterin/rkhCgQteN#Exiting) on changes to the validator set. The value returned by this function provides the number of validators that may become active in an epoch, and the number of validators that may exit in an epoch.

Some small amount of churn is always allowed, set by [`MIN_PER_EPOCH_CHURN_LIMIT`](/part3/config/configuration/#min_per_epoch_churn_limit), and the amount of per-epoch churn allowed increases by one for every extra [`CHURN_LIMIT_QUOTIENT`](/part3/config/configuration/#churn_limit_quotient) validators that are currently active (once the minimum has been exceeded).

<!-- Number of validators -->

In concrete terms, with 500,000 validators, this means that up to seven validators can enter or exit the active validator set each epoch (1,575 per day).  At 524,288 active validators the limit will rise to eight per epoch (1,800 per day).

|||
|-|------|
| Used&nbsp;by | [`initiate_validator_exit()`](/part3/helper/mutators/#def_initiate_validator_exit), [`process_registry_updates()`](/part3/transition/epoch/#def_process_registry_updates) |
| Uses | [`get_active_validator_indices()`](#def_get_active_validator_indices) |
| See&nbsp;also | [`MIN_PER_EPOCH_CHURN_LIMIT`](/part3/config/configuration/#min_per_epoch_churn_limit), [`CHURN_LIMIT_QUOTIENT`](/part3/config/configuration/#churn_limit_quotient) |

#### `get_seed`

<a id="def_get_seed"></a>

```python
def get_seed(state: BeaconState, epoch: Epoch, domain_type: DomainType) -> Bytes32:
    """
    Return the seed at ``epoch``.
    """
    mix = get_randao_mix(state, Epoch(epoch + EPOCHS_PER_HISTORICAL_VECTOR - MIN_SEED_LOOKAHEAD - 1))  # Avoid underflow
    return hash(domain_type + uint_to_bytes(epoch) + mix)
```

Used in [`get_beacon_committee()`](#def_get_beacon_committee), [`get_beacon_proposer_index()`](#def_get_beacon_proposer_index), and [`get_next_sync_committee_indices()`](#def_get_next_sync_committee_indices) to provide the randomness for computing proposers and committees. `domain_type` is [`DOMAIN_BEACON_ATTESTER`](/part3/config/constants/#domain_beacon_attester), [`DOMAIN_BEACON_PROPOSER`](/part3/config/constants/#domain_beacon_proposer), and [`DOMAIN_SYNC_COMMITTEE`](/part3/config/constants/#domain_sync_committee) respectively.

RANDAO mixes are stored in a circular list of length [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector). The seed for an epoch is based on the randao mix from [`MIN_SEED_LOOKAHEAD`](/part3/config/preset/#min_seed_lookahead) epochs ago. This is to limit the forward visibility of randomness: see the explanation there.

The seed returned is not based only on the domain and the randao mix, but the epoch number is also mixed in. This is to handle the pathological case of no blocks being seen for more than two epochs, in which case we run out of randao updates. That could lock in forever a non-participating set of block proposers. Mixing in the epoch number means that fresh committees and proposers can continue to be selected.

|||
|-|------|
| Used&nbsp;by | [`get_beacon_committee()`](#def_get_beacon_committee), [`get_beacon_proposer_index()`](#def_get_beacon_proposer_index), [`get_next_sync_committee_indices()`](#def_get_next_sync_committee_indices) |
| Uses | [`get_randao_mix()`](#def_get_randao_mix) |
| See&nbsp;also | [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector), [`MIN_SEED_LOOKAHEAD`](/part3/config/preset/#min_seed_lookahead) |

#### `get_committee_count_per_slot`

<a id="def_get_committee_count_per_slot"></a>

```python
def get_committee_count_per_slot(state: BeaconState, epoch: Epoch) -> uint64:
    """
    Return the number of committees in each slot for the given ``epoch``.
    """
    return max(uint64(1), min(
        MAX_COMMITTEES_PER_SLOT,
        uint64(len(get_active_validator_indices(state, epoch))) // SLOTS_PER_EPOCH // TARGET_COMMITTEE_SIZE,
    ))
```

Every slot in a given epoch has the same number of beacon committees, as calculated by this function.

As far as the LMD GHOST consensus protocol is concerned, all the validators attesting in a slot effectively act as a single large committee. However, organising them into multiple committees gives two benefits.

 1. Having multiple smaller committees reduces the load on the aggregators that collect and aggregate the attestations from committee members. This is important, as validating the signatures and aggregating them takes time. The downside is that blocks need to be larger, as, in the best case, there are up to 64 aggregate attestations to store per block rather than a single large aggregate signature over all attestations.
 2. It maps well onto the future plans for data shards, when each committee will be responsible for committing to a block on one shard in addition to its current duties.

Since the original Phase&nbsp;1 sharding design that required these committees has now been abandoned, the second of these points no longer applies.

There is always at least one committee per slot, and never more than [`MAX_COMMITTEES_PER_SLOT`](/part3/config/preset/#max_committees_per_slot), currently 64.

Subject to these constraints, the actual number of committees per slot is $N / 4096$, where $N$ is the total number of active validators.

The intended behaviour looks like this:

  - The ideal case is that there are [`MAX_COMMITTEES_PER_SLOT`](/part3/config/preset/#max_committees_per_slot) = 64 committees per slot. This maps to one committee per slot per shard once data sharding has been implemented. These committees will be responsible for voting on shard crosslinks. There must be at least 262,144 active validators to achieve this.
  - If there are fewer active validators, then the number of committees per shard is reduced below 64 in order to maintain a minimum committee size of [`TARGET_COMMITTEE_SIZE`](/part3/config/preset/#target_committee_size) = 128. In this case, not every shard will get crosslinked at every slot (once sharding is in place).
  - Finally, only if the number of active validators falls below 4096 will the committee size be reduced to less than 128. With so few validators, the chain has no meaningful security in any case.

|||
|-|------|
| Used&nbsp;by | [`get_beacon_committee()`](#def_get_beacon_committee), [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| Uses | [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices) |
| See&nbsp;also | [`MAX_COMMITTEES_PER_SLOT`](/part3/config/preset/#max_committees_per_slot), [`TARGET_COMMITTEE_SIZE`](/part3/config/preset/#target_committee_size) |

#### `get_beacon_committee`

<a id="def_get_beacon_committee"></a>

```python
def get_beacon_committee(state: BeaconState, slot: Slot, index: CommitteeIndex) -> Sequence[ValidatorIndex]:
    """
    Return the beacon committee at ``slot`` for ``index``.
    """
    epoch = compute_epoch_at_slot(slot)
    committees_per_slot = get_committee_count_per_slot(state, epoch)
    return compute_committee(
        indices=get_active_validator_indices(state, epoch),
        seed=get_seed(state, epoch, DOMAIN_BEACON_ATTESTER),
        index=(slot % SLOTS_PER_EPOCH) * committees_per_slot + index,
        count=committees_per_slot * SLOTS_PER_EPOCH,
    )
```

Beacon committees vote on the beacon block at each slot via attestations. There are up to [`MAX_COMMITTEES_PER_SLOT`](/part3/config/preset/#max_committees_per_slot) beacon committees per slot, and each committee is active exactly once per epoch.

This function returns the list of committee members given a slot number and an index within that slot to select the desired committee, relying on [`compute_committee()`](/part3/helper/misc/#def_compute_committee) to do the heavy lifting.

Note that, since this uses [`get_seed()`](#def_get_seed), we can obtain committees only up to [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector) epochs into the past (minus [`MIN_SEED_LOOKAHEAD`](/part3/config/preset/#min_seed_lookahead)).

`get_beacon_committee` is used by [`get_attesting_indices()`](#def_get_attesting_indices) and [`process_attestation()`](/part3/transition/block/#def_process_attestation) when processing attestations coming from a committee, and by validators when checking their [committee assignments](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#validator-assignments) and [aggregation duties](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#aggregation-selection).

|||
|-|------|
| Used&nbsp;by | [`get_attesting_indices()`](/part3/helper/accessors/#def_get_attesting_indices), [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| Uses | [`get_committee_count_per_slot()`](#def_get_committee_count_per_slot), [`compute_committee()`](/part3/helper/misc/#def_compute_committee), [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices), [`get_seed()`](#def_get_seed) |
| See&nbsp;also | [`MAX_COMMITTEES_PER_SLOT`](/part3/config/preset/#max_committees_per_slot), [`DOMAIN_BEACON_ATTESTER`](/part3/config/constants/#domain_beacon_attester) |

#### `get_beacon_proposer_index`

<a id="def_get_beacon_proposer_index"></a>

```python
def get_beacon_proposer_index(state: BeaconState) -> ValidatorIndex:
    """
    Return the beacon proposer index at the current slot.
    """
    epoch = get_current_epoch(state)
    seed = hash(get_seed(state, epoch, DOMAIN_BEACON_PROPOSER) + uint_to_bytes(state.slot))
    indices = get_active_validator_indices(state, epoch)
    return compute_proposer_index(state, indices, seed)
```

Each slot, exactly one of the active validators is randomly chosen to be the proposer of the beacon block for that slot. The probability of being selected is weighted by the validator's effective balance in [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index).

The chosen block proposer does not need to be a member of one of the beacon committees for that slot: it is chosen from the entire set of active validators for that epoch.

The RANDAO seed returned by [`get_seed()`](#def_get_seed) is updated once per epoch. The slot number is mixed into the seed using a hash to allow us to choose a different proposer at each slot. This also protects us in the case that there is an entire epoch of empty blocks. If that were to happen the RANDAO would not be updated, but we would still be able to select a different set of proposers for the next epoch via this slot number mix-in process.

There is a chance of the same proposer being selected in two consecutive slots, or more than once per epoch. If every validator has the same effective balance, then the probability of being selected in a particular slot is simply $\frac{1}{N}$ independent of any other slot, where $N$ is the number of active validators in the epoch corresponding to the slot.

Currently, neither `get_beacon_proposer_index()` nor [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index) filter out slashed validators. This could result in a slashed validator, prior to its exit, being selected to propose a block. Its block would, however, be invalid due to the check in [`process_block_header()`](/part3/transition/block/#def_process_block_header). A [fix for this](https://github.com/ethereum/consensus-specs/pull/3175) has been proposed so as to avoid many missed slots (slots with invalid blocks) in the event of a mass slashing.

|||
|-|------|
| Used&nbsp;by | [`slash_validator()`](/part3/helper/mutators/#def_slash_validator), [`process_block_header()`](/part3/transition/block/#def_process_block_header), [`process_randao()`](/part3/transition/block/#def_process_randao), [`process_attestation()`](/part3/transition/block/#def_process_attestation), [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) |
| Uses | [`get_seed()`](#def_get_seed), [`uint_to_bytes()`](/part3/helper/math/#uint_to_bytes), [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices), [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index) |

#### `get_total_balance`

<a id="def_get_total_balance"></a>

```python
def get_total_balance(state: BeaconState, indices: Set[ValidatorIndex]) -> Gwei:
    """
    Return the combined effective balance of the ``indices``.
    ``EFFECTIVE_BALANCE_INCREMENT`` Gwei minimum to avoid divisions by zero.
    Math safe up to ~10B ETH, after which this overflows uint64.
    """
    return Gwei(max(EFFECTIVE_BALANCE_INCREMENT, sum([state.validators[index].effective_balance for index in indices])))
```

A simple utility that returns the total balance of all validators in the list, `indices`, passed in.

As an aside, there is an interesting example of some fragility in the spec lurking here. This function [used to](https://github.com/ethereum/consensus-specs/blame/8c532c0e9ad1e6016a1ef3f36012cfd9b3870c13/specs/phase0/beacon-chain.md#L1002) return a minimum of 1 Gwei to avoid a potential division by zero in the calculation of rewards and penalties. However, the rewards calculation was [modified](https://github.com/ethereum/consensus-specs/pull/1635) to avoid a possible integer overflow condition, without modifying this function, which re-introduced the possibility of a [division by zero](https://github.com/ethereum/consensus-specs/issues/1663). This was later [fixed](https://github.com/ethereum/consensus-specs/pull/1664) by returning a minimum of [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment). The [formal verification](https://github.com/ConsenSys/eth2.0-dafny) of the specification is helpful in avoiding issues like this.

|||
|-|------|
| Used&nbsp;by | [`get_total_active_balance()`](#def_get_total_active_balance), [`get_flag_index_deltas()`](#def_get_flag_index_deltas), [`process_justification_and_finalization()`](/part3/transition/epoch/#def_process_justification_and_finalization) |
| See&nbsp;also | [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment) |

#### `get_total_active_balance`

<a id="def_get_total_active_balance"></a>

```python
def get_total_active_balance(state: BeaconState) -> Gwei:
    """
    Return the combined effective balance of the active validators.
    Note: ``get_total_balance`` returns ``EFFECTIVE_BALANCE_INCREMENT`` Gwei minimum to avoid divisions by zero.
    """
    return get_total_balance(state, set(get_active_validator_indices(state, get_current_epoch(state))))
```

Uses [`get_total_balance()`](#def_get_total_balance) to calculate the sum of the effective balances of all active validators in the current epoch.

This quantity is frequently used in the spec. For example, Casper FFG uses the total active balance to judge whether the 2/3 majority threshold of attestations has been reached in [justification and finalisation](/part3/transition/epoch/#justification-and-finalization). And it is a fundamental part of the calculation of rewards and penalties. The [base reward](/part3/transition/epoch/#def_get_base_reward_per_increment) is proportional to the reciprocal of the square root of the total active balance. Thus, validator rewards are higher when little balance is at stake (few active validators) and lower when much balance is at stake (many active validators).

Since it is calculated from effective balances, total active balance does not change during an epoch, so is a great candidate for being cached.

|||
|-|------|
| Used&nbsp;by | [`get_flag_index_deltas()`](#def_get_flag_index_deltas), [`process_justification_and_finalization()`](/part3/transition/epoch/#def_process_justification_and_finalization), [`get_base_reward_per_increment()`](/part3/transition/epoch/#def_get_base_reward_per_increment), [`process_slashings()`](/part3/transition/epoch/#def_process_slashings), [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) |
| Uses | [`get_total_balance()`](#def_get_total_balance), [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices) |

#### `get_domain`

<a id="def_get_domain"></a>

```python
def get_domain(state: BeaconState, domain_type: DomainType, epoch: Epoch=None) -> Domain:
    """
    Return the signature domain (fork version concatenated with domain type) of a message.
    """
    epoch = get_current_epoch(state) if epoch is None else epoch
    fork_version = state.fork.previous_version if epoch < state.fork.epoch else state.fork.current_version
    return compute_domain(domain_type, fork_version, state.genesis_validators_root)
```

`get_domain()` pops up whenever signatures need to be verified, since a [`DomainType`](/part3/config/types/#domaintype) is always mixed in to the signed data. For the science behind domains, see [Domain types](/part3/config/constants/#domain-types) and [`compute_domain()`](/part3/helper/misc/#def_compute_domain).

Except for `DOMAIN_DEPOSIT`, domains are always combined with the fork [version](/part3/config/types/#version) before being used in signature generation. This is to distinguish messages from different chains, and ensure that validators don't get slashed if they choose to participate on two independent forks. (That is, deliberate forks, aka hard-forks. Participating on both branches of temporary consensus forks is punishable: that's basically the whole point of slashing.)

Note that a message signed under one fork version will be valid during the next fork version, but not thereafter. So, for example, voluntary exit messages signed during Altair will be valid after the Bellatrix beacon chain upgrade, but not after the Capella upgrade. Voluntary exit messages signed during Phase&nbsp;0 are valid under Altair but were made invalid by the Bellatrix upgrade[^fn-vem-fork-agnostic].

[^fn-vem-fork-agnostic]: There is [some discussion](https://github.com/ethereum/consensus-specs/pull/3288) around changing this to make voluntary exit messages fork-agnostic in future, but that has not yet been implemented.

|||
|-|------|
| Used&nbsp;by | [`is_valid_indexed_attestation()`](/part3/helper/predicates/#def_is_valid_indexed_attestation), [`verify_block_signature()`](/part3/transition/#def_verify_block_signature), [`process_randao()`](/part3/transition/block/#def_process_randao), [`process_proposer_slashing()`](/part3/transition/block/#def_process_proposer_slashing), [`process_voluntary_exit()`](/part3/transition/block/#def_process_voluntary_exit), [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) |
| Uses | [`compute_domain()`](/part3/helper/misc/#def_compute_domain) |
| See&nbsp;also | [`DomainType`](/part3/config/types/#domaintype), [Domain types](/part3/config/constants/#domain-types) |

#### `get_indexed_attestation`

<a id="def_get_indexed_attestation"></a>

```python
def get_indexed_attestation(state: BeaconState, attestation: Attestation) -> IndexedAttestation:
    """
    Return the indexed attestation corresponding to ``attestation``.
    """
    attesting_indices = get_attesting_indices(state, attestation.data, attestation.aggregation_bits)

    return IndexedAttestation(
        attesting_indices=sorted(attesting_indices),
        data=attestation.data,
        signature=attestation.signature,
    )
```

Lists of validators within committees occur in two forms in the specification.

  - They can be compressed into a bitlist, in which each bit represents the presence or absence of a validator from a particular committee. The committee is referenced by slot, and committee index within that slot. This is how sets of validators are represented in [`Attestation`](/part3/containers/operations/#attestation)s.
  - Or they can be listed explicitly by their validator indices, as in [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation)s. Note that the list of indices is sorted: an attestation is [invalid](/part3/helper/predicates/#is_valid_indexed_attestation) if not.

`get_indexed_attestation()` converts from the former representation to the latter. The slot number and the committee index are provided by the [`AttestationData`](/part3/containers/dependencies/#attestationdata) and are used to reconstruct the committee members via [`get_beacon_committee()`](/part3/helper/accessors/#def_get_beacon_committee). The supplied bitlist will have come from an `Attestation`.

Attestations are aggregatable, which means that attestations from multiple validators making the same vote can be rolled up into a single attestation through the magic of BLS signature aggregation. However, in order to be able to verify the signature later, a record needs to be kept of which validators actually contributed to the attestation. This is so that those validators' public keys can be aggregated to match the construction of the signature.

The conversion from the bit-list format to the list format is performed by [`get_attesting_indices()`](#get_attesting_indices), below.

|||
|-|------|
| Used&nbsp;by | [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| Uses | [`get_attesting_indices()`](#def_get_attesting_indices) |
| See&nbsp;also | [`Attestation`](/part3/containers/operations/#attestation), [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation) |

#### `get_attesting_indices`

<a id="def_get_attesting_indices"></a>

```python
def get_attesting_indices(state: BeaconState,
                          data: AttestationData,
                          bits: Bitlist[MAX_VALIDATORS_PER_COMMITTEE]) -> Set[ValidatorIndex]:
    """
    Return the set of attesting indices corresponding to ``data`` and ``bits``.
    """
    committee = get_beacon_committee(state, data.slot, data.index)
    return set(index for i, index in enumerate(committee) if bits[i])
```

As described under [`get_indexed_attestation()`](#def_get_indexed_attestation), lists of validators come in two forms. This routine converts from the compressed form, in which validators are represented as a subset of a committee with their presence or absence indicated by a 1 bit or a 0 bit respectively, to an explicit list of [`ValidatorIndex`](/part3/config/types/#validatorindex) types.

|||
|-|------|
| Used&nbsp;by | [`get_indexed_attestation()`](#def_get_indexed_attestation), [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| Uses | [`get_beacon_committee()`](#def_get_beacon_committee) |
| See&nbsp;also | [`AttestationData`](/part3/containers/dependencies/#attestationdata), [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation) |

#### `get_next_sync_committee_indices`

<a id="def_get_next_sync_committee_indices"></a>

```python
def get_next_sync_committee_indices(state: BeaconState) -> Sequence[ValidatorIndex]:
    """
    Return the sync committee indices, with possible duplicates, for the next sync committee.
    """
    epoch = Epoch(get_current_epoch(state) + 1)

    MAX_RANDOM_BYTE = 2**8 - 1
    active_validator_indices = get_active_validator_indices(state, epoch)
    active_validator_count = uint64(len(active_validator_indices))
    seed = get_seed(state, epoch, DOMAIN_SYNC_COMMITTEE)
    i = 0
    sync_committee_indices: List[ValidatorIndex] = []
    while len(sync_committee_indices) < SYNC_COMMITTEE_SIZE:
        shuffled_index = compute_shuffled_index(uint64(i % active_validator_count), active_validator_count, seed)
        candidate_index = active_validator_indices[shuffled_index]
        random_byte = hash(seed + uint_to_bytes(uint64(i // 32)))[i % 32]
        effective_balance = state.validators[candidate_index].effective_balance
        if effective_balance * MAX_RANDOM_BYTE >= MAX_EFFECTIVE_BALANCE * random_byte:
            sync_committee_indices.append(candidate_index)
        i += 1
    return sync_committee_indices
```

`get_next_sync_committee_indices()` is used to select the subset of validators that will make up a sync committee. The committee size is [`SYNC_COMMITTEE_SIZE`](/part3/config/preset/#sync_committee_size), and the committee is allowed to contain duplicates, that is, the same validator more than once. This is to [handle gracefully](https://github.com/ethereum/consensus-specs/pull/2130#discussion_r532499943) the situation of there being fewer active validators than `SYNC_COMMITTEE_SIZE`.

Similarly to being chosen to propose a block, the probability of any validator being selected for a sync committee is proportional to its effective balance. Thus, the algorithm is almost the same as that of [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index), except that this one exits only after finding `SYNC_COMMITTEE_SIZE` members, rather than exiting as soon as a candidate is found. Both routines use the try-and-increment method to weight the probability of selection with the validators' effective balances.

It's fairly clear why block proposers are selected with a probability proportional to their effective balances: block production is subject to slashing, and proposers with less at stake have less to slash, so we reduce their influence accordingly. It is not so clear why the probability of being in a sync committee is also proportional to a validator's effective balance; sync committees are not subject to slashing. It has to do with keeping calculations for [light clients simple](https://github.com/ethereum/consensus-specs/pull/2130#discussion_r524848644). We don't want to burden light clients with summing up validators' balances to judge whether a 2/3 supermajority of stake in the committee has voted for a block. Ideally, they can just count the participation flags. To make this somewhat reliable, we weight the probability that a validator participates in proportion to its effective balance.

|||
|-|------|
| Used&nbsp;by | [`get_next_sync_committee()`](#def_get_next_sync_committee) |
| Uses | [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices), [`get_seed()`](/part3/helper/accessors/#def_get_seed), [`compute_shuffled_index()`](/part3/helper/misc/#def_compute_shuffled_index), [`uint_to_bytes()`](/part3/helper/math/#uint_to_bytes) |
| See&nbsp;also | [`SYNC_COMMITTEE_SIZE`](/part3/config/preset/#sync_committee_size), [`compute_proposer_index()`](/part3/helper/misc/#def_compute_proposer_index) |

#### `get_next_sync_committee`

> _Note_: The function `get_next_sync_committee` should only be called at sync committee period boundaries and when [upgrading state to Altair](/part4/history/altair/).

The random seed that generates the sync committee is based on the number of the next epoch. [`get_next_sync_committee_indices()`](#def_get_next_sync_committee_indices) doesn't contain any check that the epoch corresponds to a sync-committee change boundary, which allowed the timing of the Altair upgrade to be more flexible. But a consequence is that you will get an incorrect committee if you call `get_next_sync_committee()` at the wrong time.

<a id="def_get_next_sync_committee"></a>

```python
def get_next_sync_committee(state: BeaconState) -> SyncCommittee:
    """
    Return the next sync committee, with possible pubkey duplicates.
    """
    indices = get_next_sync_committee_indices(state)
    pubkeys = [state.validators[index].pubkey for index in indices]
    aggregate_pubkey = eth_aggregate_pubkeys(pubkeys)
    return SyncCommittee(pubkeys=pubkeys, aggregate_pubkey=aggregate_pubkey)
```

`get_next_sync_committee()` is a simple wrapper around [`get_next_sync_committee_indices()`](#def_get_next_sync_committee_indices) that packages everything up into a nice [`SyncCommittee`](/part3/containers/dependencies/#synccommittee) object.

See the [`SyncCommittee`](/part3/containers/dependencies/#synccommittee) type for an explanation of how the `aggregate_pubkey` is intended to be used.

|||
|-|------|
| Used&nbsp;by | [`process_sync_committee_updates()`](/part3/transition/epoch/#def_process_sync_committee_updates), [`initialize_beacon_state_from_eth1()`](/part3/initialise/#def_initialize_beacon_state_from_eth1) |
| Uses | [`get_next_sync_committee_indices()`](#def_get_next_sync_committee_indices), [`eth_aggregate_pubkeys()`](/part3/helper/crypto/#def_eth_aggregate_pubkeys) |
| See&nbsp;also | [`SyncCommittee`](/part3/containers/dependencies/#synccommittee) |

#### `get_unslashed_participating_indices`

<a id="def_get_unslashed_participating_indices"></a>

```python
def get_unslashed_participating_indices(state: BeaconState, flag_index: int, epoch: Epoch) -> Set[ValidatorIndex]:
    """
    Return the set of validator indices that are both active and unslashed for the given ``flag_index`` and ``epoch``.
    """
    assert epoch in (get_previous_epoch(state), get_current_epoch(state))
    if epoch == get_current_epoch(state):
        epoch_participation = state.current_epoch_participation
    else:
        epoch_participation = state.previous_epoch_participation
    active_validator_indices = get_active_validator_indices(state, epoch)
    participating_indices = [i for i in active_validator_indices if has_flag(epoch_participation[i], flag_index)]
    return set(filter(lambda index: not state.validators[index].slashed, participating_indices))
```

`get_unslashed_participating_indices()` returns the list of validators that made a timely attestation with the type [`flag_index`](/part3/config/constants/#participation-flag-indices) during the `epoch` in question.

It is used with the `TIMELY_TARGET_FLAG_INDEX` flag in [`process_justification_and_finalization()`](/part3/transition/epoch/#def_process_justification_and_finalization) to calculate the proportion of stake that voted for the candidate checkpoint in the current and previous epochs.

It is also used with the `TIMELY_TARGET_FLAG_INDEX` for applying inactivity penalties in [`process_inactivity_updates()`](/part3/transition/epoch/#def_process_inactivity_updates) and [`get_inactivity_penalty_deltas()`](/part3/transition/epoch/#def_get_inactivity_penalty_deltas). If a validator misses a correct target vote during an inactivity leak then it is considered not to have participated at all (it is not contributing anything useful).

And it is used in [`get_flag_index_deltas()`](#def_get_flag_index_deltas) for calculating rewards due for each type of correct vote.

Slashed validators are ignored. Once slashed, validators no longer receive rewards or participate in consensus, although they are subject to penalties until they have finally been exited.

|||
|-|------|
| Used&nbsp;by | [`get_flag_index_deltas()`](#def_get_flag_index_deltas), [`process_justification_and_finalization()`](/part3/transition/epoch/#def_process_justification_and_finalization), [`process_inactivity_updates()`](/part3/transition/epoch/#def_process_inactivity_updates), [`get_inactivity_penalty_deltas()`](/part3/transition/epoch/#def_get_inactivity_penalty_deltas) |
| Uses | [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices), [`has_flag()`](/part3/helper/participation/#def_has_flag) |
| See&nbsp;also | [Participation flag indices](/part3/config/constants/#participation-flag-indices) |

#### `get_attestation_participation_flag_indices`

<a id="def_get_attestation_participation_flag_indices"></a>

```python
def get_attestation_participation_flag_indices(state: BeaconState,
                                               data: AttestationData,
                                               inclusion_delay: uint64) -> Sequence[int]:
    """
    Return the flag indices that are satisfied by an attestation.
    """
    if data.target.epoch == get_current_epoch(state):
        justified_checkpoint = state.current_justified_checkpoint
    else:
        justified_checkpoint = state.previous_justified_checkpoint

    # Matching roots
    is_matching_source = data.source == justified_checkpoint
    is_matching_target = is_matching_source and data.target.root == get_block_root(state, data.target.epoch)
    is_matching_head = is_matching_target and data.beacon_block_root == get_block_root_at_slot(state, data.slot)
    assert is_matching_source

    participation_flag_indices = []
    if is_matching_source and inclusion_delay <= integer_squareroot(SLOTS_PER_EPOCH):
        participation_flag_indices.append(TIMELY_SOURCE_FLAG_INDEX)
    if is_matching_target and inclusion_delay <= SLOTS_PER_EPOCH:
        participation_flag_indices.append(TIMELY_TARGET_FLAG_INDEX)
    if is_matching_head and inclusion_delay == MIN_ATTESTATION_INCLUSION_DELAY:
        participation_flag_indices.append(TIMELY_HEAD_FLAG_INDEX)

    return participation_flag_indices
```

This is called by [`process_attestation()`](/part3/transition/block/#def_process_attestation) during block processing, and is the heart of the mechanism for recording validators' votes as contained in their attestations. It filters the given attestation against the beacon state's current view of the chain, and returns [participation flag indices](/part3/config/constants/#participation-flag-indices) only for the votes that are both correct and timely.

`data` is an [`AttestationData`](/part3/containers/dependencies/#attestationdata) object that contains the source, target, and head votes of the validators that contributed to the attestation. The attestation may represent the votes of one or more validators.

`inclusion_delay` is the difference between the current slot on the beacon chain and the slot for which the attestation was created. For the block containing the attestation to be valid, `inclusion_delay` must be between [`MIN_ATTESTATION_INCLUSION_DELAY`](/part3/config/preset/#min_attestation_inclusion_delay) and [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch) inclusive. In other words, attestations must be included in the next block, or in any block up to 32 slots later, after which they are ignored.

Since the attestation may be up to 32 slots old, it might have been generated in the current epoch or the previous epoch, so the first thing we do is to check the attestation's target vote epoch to see which epoch we should be looking at in the beacon state.

Next, we check whether each of the votes in the attestation are correct:

  - Does the attestation's source vote match what we believe to be the justified checkpoint in the epoch in question?
  - If so, does the attestation's target vote match the head block at the epoch's checkpoint, that is, the first slot of the epoch?
  - If so, does the attestation's head vote match what we believe to be the head block at the attestation's slot? Note that the slot may not contain a block &ndash; it may be a skip slot &ndash; in which case the last known block is considered to be the head.

These three build on each other, so that it is not possible to have a correct target vote without a correct source vote, and it is not possible to have a correct head vote without a correct target vote.

The `assert` statement is interesting. If an attestation does not have the correct source vote, the block containing it is invalid and is discarded. Having an incorrect source vote means that the block proposer disagrees with me about the last justified checkpoint, which is an irreconcilable difference.

[TODO: check the irreconcilable bit. Maybe explain it.]::

After checking the validity of the votes, the timeliness of each vote is checked. Let's take them in reverse order.

  - Correct head votes must be included immediately, that is, in the very next slot.
    - Head votes, used for LMD GHOST consensus, are not useful after one slot.
  - Correct target votes must be included within 32 slots, one epoch.
    - Target votes are useful at any time, but it is simpler if they don't span more than a couple of epochs, so 32 slots is a reasonable limit. This check is actually redundant since attestations in blocks cannot be older than 32 slots.
  - Correct source votes must be included within 5 slots (`integer_squareroot(32)`).
    - This is the geometric mean of 1 (the timely head threshold) and 32 (the timely target threshold). This is an arbitrary choice. Vitalik's view[^fn_vitalik_geometric_mean] is that, with this setting, the cumulative timeliness rewards most closely match an exponentially decreasing curve, which "feels more logical".

[^fn_vitalik_geometric_mean]: From a [conversation](https://discord.com/channels/595666850260713488/595701173944713277/871340571107655700) on the Ethereum Research Discord server.

The timely inclusion requirements are new in Altair. In Phase&nbsp;0, all correct votes received a reward, and there was an additional reward for inclusion the was proportional to the reciprocal of the inclusion distance. This led to an oddity where it was always more profitable to vote for a correct head, even if that meant waiting longer and risking not being included in the next slot.

|||
|-|------|
| Used&nbsp;by | [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| Uses | [`get_block_root()`](#def_get_block_root), [`get_block_root_at_slot()`](#def_get_block_root_at_slot), [`integer_squareroot()`](/part3/helper/math/#def_integer_squareroot) |
| See&nbsp;also | [Participation flag indices](/part3/config/constants/#participation-flag-indices), [`AttestationData`](/part3/containers/dependencies/#attestationdata), [`MIN_ATTESTATION_INCLUSION_DELAY`](/part3/config/preset/#min_attestation_inclusion_delay) |

#### `get_flag_index_deltas`

<a id="def_get_flag_index_deltas"></a>

```python
def get_flag_index_deltas(state: BeaconState, flag_index: int) -> Tuple[Sequence[Gwei], Sequence[Gwei]]:
    """
    Return the deltas for a given ``flag_index`` by scanning through the participation flags.
    """
    rewards = [Gwei(0)] * len(state.validators)
    penalties = [Gwei(0)] * len(state.validators)
    previous_epoch = get_previous_epoch(state)
    unslashed_participating_indices = get_unslashed_participating_indices(state, flag_index, previous_epoch)
    weight = PARTICIPATION_FLAG_WEIGHTS[flag_index]
    unslashed_participating_balance = get_total_balance(state, unslashed_participating_indices)
    unslashed_participating_increments = unslashed_participating_balance // EFFECTIVE_BALANCE_INCREMENT
    active_increments = get_total_active_balance(state) // EFFECTIVE_BALANCE_INCREMENT
    for index in get_eligible_validator_indices(state):
        base_reward = get_base_reward(state, index)
        if index in unslashed_participating_indices:
            if not is_in_inactivity_leak(state):
                reward_numerator = base_reward * weight * unslashed_participating_increments
                rewards[index] += Gwei(reward_numerator // (active_increments * WEIGHT_DENOMINATOR))
        elif flag_index != TIMELY_HEAD_FLAG_INDEX:
            penalties[index] += Gwei(base_reward * weight // WEIGHT_DENOMINATOR)
    return rewards, penalties
```

This function is used during epoch processing to assign rewards and penalties to individual validators based on their voting record in the previous epoch. Rewards for block proposers for including attestations are calculated [during block processing](/part3/transition/block/#def_process_attestation). The "deltas" in the function name are the separate lists of rewards and penalties returned. Rewards and penalties are always treated separately to avoid negative numbers.

The function is called once for each of the [flag types](/part3/config/constants/#participation-flag-indices) corresponding to correct attestation votes: timely source, timely target, timely head.

The list of validators returned by [`get_unslashed_participating_indices()`](/part3/helper/accessors/#def_get_unslashed_participating_indices) contains the ones that will be rewarded for making this vote type in a timely and correct manner. That routine uses the flags set in state for each validator by [`process_attestation()`](/part3/transition/block/#def_process_attestation) during block processing and returns the validators for which the corresponding flag is set.

Every active validator is expected to make an attestation exactly once per epoch, so we then cycle through the entire set of active validators, rewarding them if they appear in `unslashed_participating_indices`, as long as we are not in an inactivity leak. If we are in a leak, no validator is rewarded for any of its votes, but penalties still apply to non-participating validators.

Notice that the reward is weighted with `unslashed_participating_increments`, which is proportional to the total stake of the validators that made a correct vote with this flag. This means that, if participation by other validators is lower, then my rewards are lower, even if I perform my duties perfectly. The reason for this is to do with [discouragement attacks](https://raw.githubusercontent.com/ethereum/research/master/papers/discouragement/discouragement.pdf) (see also this [nice explainer](https://web.archive.org/web/20221225163839/https://hackingresear.ch/discouragement-attacks/)[^fn-discouragement-link]). In short, with this mechanism, validators are incentivised to help each other out (e.g. by forwarding gossip messages, or aggregating attestations well) rather than to attack or censor one-another.

[^fn-discouragement-link]: Unfortunately, the original page, `https://hackingresear.ch/discouragement-attacks/` seems to be unavailable now. The link in the text is to archive.org, but their version is a bit broken.

Validators that did not make a correct and timely vote are penalised with a full weighted base reward for each flag that they missed, except for missing the head vote. Head votes have only a single slot to get included, so a missing block in the next slot is sufficient to cause a miss, but is completely outside the attester's control. Thus, head votes are only rewarded, not penalised. This also allows perfectly performing validators to break even during an inactivity leak, when we expect at least a third of blocks to be missing: they receive no rewards, but ideally no penalties either.

Untangling the arithmetic, the maximum total issuance due to rewards for attesters in an epoch, $I_A$, comes out as follows, in the [notation](/part3/transition/epoch/#reward-and-penalty-calculations) described later.

$$
I_A = \frac{W_s + W_t + W_h}{W_{\Sigma}}NB
$$

|||
|-|------|
| Used&nbsp;by | [`process_rewards_and_penalties()`](/part3/transition/epoch/#def_process_rewards_and_penalties) |
| Uses | [`get_unslashed_participating_indices()`](/part3/helper/accessors/#def_get_unslashed_participating_indices), [`get_total_balance()`](/part3/helper/accessors/#def_get_total_balance), [`get_total_active_balance()`](/part3/helper/accessors/#get_total_active_balance), [`get_eligible_validator_indices()`](/part3/transition/epoch/#def_get_eligible_validator_indices), [`get_base_reward()`](/part3/transition/epoch/#def_get_base_reward), [`is_in_inactivity_leak()`](/part3/transition/epoch/#def_is_in_inactivity_leak) |
| See&nbsp;also | [`process_attestation()`](/part3/transition/block/#def_process_attestation), [participation flag indices](/part3/config/constants/#participation-flag-indices), [rewards and penalties](/part3/transition/epoch/#reward-and-penalty-calculations) |

### Beacon State Mutators <!-- /part3/helper/mutators/ -->

#### `increase_balance`

<a id="def_increase_balance"></a>

```python
def increase_balance(state: BeaconState, index: ValidatorIndex, delta: Gwei) -> None:
    """
    Increase the validator balance at index ``index`` by ``delta``.
    """
    state.balances[index] += delta
```

After creating a validator with its deposit balance, this and [`decrease_balance()`](#decrease_balance) are the only places in the spec where validator balances are ever modified.

We need two separate functions to change validator balances, one to increase them and one to decrease them, since we are using only unsigned integers.

Fun fact: A typo around this led to Teku's one and only [consensus failure](https://github.com/ConsenSys/teku/pull/885/files) at the initial [client interop event](https://web.archive.org/web/20221129214218/https://media.consensys.net/how-30-eth-2-0-devs-locked-themselves-in-to-achieve-interoperability-175e4a807d92?gi=cbf4f6b2df72). Unsigned integers [induce bugs](https://critical.eschertech.com/2010/04/07/danger-unsigned-types-used-here/)!

|||
|-|------|
| Used&nbsp;by | [`slash_validator()`](#def_slash_validator), [`process_rewards_and_penalties()`](/part3/transition/epoch/#def_process_rewards_and_penalties), [`process_attestation()`](/part3/transition/block/#def_process_attestation), [`process_deposit()`](/part3/transition/block/#def_process_deposit), [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) |
| See&nbsp;also | [`decrease_balance()`](#def_decrease_balance) |

#### `decrease_balance`

<a id="def_decrease_balance"></a>

```python
def decrease_balance(state: BeaconState, index: ValidatorIndex, delta: Gwei) -> None:
    """
    Decrease the validator balance at index ``index`` by ``delta``, with underflow protection.
    """
    state.balances[index] = 0 if delta > state.balances[index] else state.balances[index] - delta
```

The counterpart to [`increase_balance()`](#increase_balance). This has a little extra work to do to check for unsigned int underflow since balances may not go negative.

|||
|-|------|
| Used&nbsp;by | [`slash_validator()`](#def_slash_validator), [`process_rewards_and_penalties()`](/part3/transition/epoch/#def_process_rewards_and_penalties), [`process_slashings()`](/part3/transition/epoch/#def_process_slashings), [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) |
| See&nbsp;also | [`increase_balance()`](#increase_balance) |

#### `initiate_validator_exit`

<a id="def_initiate_validator_exit"></a>

```python
def initiate_validator_exit(state: BeaconState, index: ValidatorIndex) -> None:
    """
    Initiate the exit of the validator with index ``index``.
    """
    # Return if validator already initiated exit
    validator = state.validators[index]
    if validator.exit_epoch != FAR_FUTURE_EPOCH:
        return

    # Compute exit queue epoch
    exit_epochs = [v.exit_epoch for v in state.validators if v.exit_epoch != FAR_FUTURE_EPOCH]
    exit_queue_epoch = max(exit_epochs + [compute_activation_exit_epoch(get_current_epoch(state))])
    exit_queue_churn = len([v for v in state.validators if v.exit_epoch == exit_queue_epoch])
    if exit_queue_churn >= get_validator_churn_limit(state):
        exit_queue_epoch += Epoch(1)

    # Set validator exit epoch and withdrawable epoch
    validator.exit_epoch = exit_queue_epoch
    validator.withdrawable_epoch = Epoch(validator.exit_epoch + MIN_VALIDATOR_WITHDRAWABILITY_DELAY)
```

Exits may be initiated [voluntarily](/part3/transition/block/#voluntary-exits), as a result of [being slashed](/part3/helper/mutators/#slash_validator), or by [dropping to](/part3/transition/epoch/#registry-updates) the [`EJECTION_BALANCE`](/part3/config/configuration/#ejection_balance) threshold.

In all cases, a dynamic "churn limit" caps the number of validators that may exit per epoch. This is calculated by [`get_validator_churn_limit()`](/part3/helper/accessors/#get_validator_churn_limit). The mechanism for enforcing this is the exit queue: the validator's `exit_epoch` is set such that it is at the end of the queue.

The exit queue is not maintained as a separate data structure, but is continually re-calculated from the exit epochs of all validators and allowing for a fixed number to exit per epoch. I expect there are some optimisations to be had around this in actual implementations.

An exiting validator is expected to continue with its proposing and attesting duties until its `exit_epoch` has passed, and will continue to receive rewards and penalties accordingly.

In addition, an exited validator remains eligible to be slashed until its `withdrawable_epoch`, which is set to [`MIN_VALIDATOR_WITHDRAWABILITY_DELAY`](/part3/config/configuration/#min_validator_withdrawability_delay) epochs after its `exit_epoch`. This is to allow some extra time for any slashable offences by the validator to be detected and reported.

|||
|-|------|
| Used by | [`slash_validator()`](/part3/helper/mutators/#def_slash_validator), [`process_registry_updates()`](/part3/transition/epoch/#def_process_registry_updates), [`process_voluntary_exit()`](/part3/transition/block/#def_process_voluntary_exit) |
| Uses | [`compute_activation_exit_epoch()`](/part3/helper/misc/#compute_activation_exit_epoch), [`get_validator_churn_limit()`](/part3/helper/accessors/#get_validator_churn_limit)|
| See also | [Voluntary Exits](/part3/transition/block/#voluntary-exits), [`MIN_VALIDATOR_WITHDRAWABILITY_DELAY`](/part3/config/configuration/#min_validator_withdrawability_delay) |

#### `slash_validator`

<a id="def_slash_validator"></a>

```python
def slash_validator(state: BeaconState,
                    slashed_index: ValidatorIndex,
                    whistleblower_index: ValidatorIndex=None) -> None:
    """
    Slash the validator with index ``slashed_index``.
    """
    epoch = get_current_epoch(state)
    initiate_validator_exit(state, slashed_index)
    validator = state.validators[slashed_index]
    validator.slashed = True
    validator.withdrawable_epoch = max(validator.withdrawable_epoch, Epoch(epoch + EPOCHS_PER_SLASHINGS_VECTOR))
    state.slashings[epoch % EPOCHS_PER_SLASHINGS_VECTOR] += validator.effective_balance
    slashing_penalty = validator.effective_balance // MIN_SLASHING_PENALTY_QUOTIENT_BELLATRIX
    decrease_balance(state, slashed_index, slashing_penalty)

    # Apply proposer and whistleblower rewards
    proposer_index = get_beacon_proposer_index(state)
    if whistleblower_index is None:
        whistleblower_index = proposer_index
    whistleblower_reward = Gwei(validator.effective_balance // WHISTLEBLOWER_REWARD_QUOTIENT)
    proposer_reward = Gwei(whistleblower_reward * PROPOSER_WEIGHT // WEIGHT_DENOMINATOR)
    increase_balance(state, proposer_index, proposer_reward)
    increase_balance(state, whistleblower_index, Gwei(whistleblower_reward - proposer_reward))
```

Both [proposer slashings](/part3/transition/block/#proposer-slashings) and [attester slashings](/part3/transition/block/#attester-slashings) end up here when a report of a slashable offence has been verified during block processing.

When a validator is slashed, several things happen immediately:

  - The validator is processed for exit via [`initiate_validator_exit()`](#initiate_validator_exit), so it joins the exit queue.
  - The validator is marked as slashed. This information is used when calculating rewards and penalties: while being exited, whatever it does, a slashed validator receives penalties as if it had failed to propose or attest, including the inactivity leak if applicable.
  - Normally, as part of the exit process, the `withdrawable_epoch` for a validator (the point at which a validator's stake is in principle unlocked) is set to [`MIN_VALIDATOR_WITHDRAWABILITY_DELAY`](/part3/config/configuration/#min_validator_withdrawability_delay) epochs after it exits. When a validator is slashed, a much longer period of lock-up applies, namely [`EPOCHS_PER_SLASHINGS_VECTOR`](/part3/config/preset/#epochs_per_slashings_vector). This is to allow a further, potentially much greater, slashing penalty [to be applied later](/part3/transition/epoch/#slashings) once the chain knows how many validators have been slashed together around the same time. The postponement of the withdrawable epoch is twice as long as required to apply the extra penalty, which is applied [half-way through](/part3/transition/epoch/#slashings) the period. This simply means that slashed validators continue to accrue attestation penalties for some 18 days longer than necessary. Treating slashed validators fairly is not a big priority for the protocol.
  - The effective balance of the validator is added to the accumulated effective balances of validators slashed this epoch, and stored in the circular list, `state.slashings`. This will later be used by the slashing penalty calculation mentioned in the previous point.
  - An initial "slap on the wrist" slashing penalty of the validator's effective balance (in Gwei) divided by the [`MIN_SLASHING_PENALTY_QUOTIENT_BELLATRIX`](/part3/config/preset/#min_slashing_penalty_quotient) is applied. For a validator with a full Effective Balance of 32&nbsp;ETH, this initial penalty is 1&nbsp;ETH.
  - The block proposer that included the slashing proof receives a reward.

In short, a slashed validator receives an initial minor penalty, can expect to receive a further penalty later, and is marked for exit.

Note that the `whistleblower_index` defaults to `None` in the parameter list. This is never used in Phase&nbsp;0, with the result that the proposer that included the slashing gets the entire whistleblower reward; there is no separate whistleblower reward for the finder of proposer or attester slashings. One reason is simply that reports are too easy to steal: if I report a slashable event to a block proposer, there is nothing to prevent that proposer claiming the report as its own. We could introduce some fancy ZK protocol to make this trustless, but this is what we're going with for now. Later developments, such as the [proof-of-custody game](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/_features/custody_game/beacon-chain.md#early-derived-secret-reveals), may reward whistleblowers directly.

|||
|-|------|
| Used&nbsp;by | [`process_proposer_slashing()`](/part3/transition/block/#def_process_proposer_slashing), [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing) |
| Uses | [`initiate_validator_exit()`](#def_initiate_validator_exit), [`get_beacon_proposer_index()`](/part3/helper/accessors/#def_get_beacon_proposer_index), [`decrease_balance()`](#def_decrease_balance), [`increase_balance()`](#def_increase_balance) |
| See&nbsp;also | [`EPOCHS_PER_SLASHINGS_VECTOR`](/part3/config/preset/#epochs_per_slashings_vector), [`MIN_SLASHING_PENALTY_QUOTIENT_BELLATRIX`](/part3/config/preset/#min_slashing_penalty_quotient), [`process_slashings()`](/part3/transition/epoch/#def_process_slashings) |

## Beacon Chain State Transition Function <!-- /part3/transition/ -->

### Preamble

#### State transitions

The state transition function is at the heart of what blockchains do. Each node on the network maintains a [state](/part3/containers/state/#beaconstate) that corresponds to its view of the state of the world.

Classically, the node's state is updated by applying blocks, in order, with a "state transition function". The state transition function is "pure" in that its output depends only on the input, and it has no side effects. This makes it deterministic: if every node starts with the same state (the [Genesis](/part3/initialise/#initialise-state) state), and applies the same sequence of blocks, then all nodes must end up with the same resulting state. If for some reason they don't, then we have a consensus failure.

If $S$ is a beacon state, and $B$ a beacon block, then the state transition function $f$ can be written

$$
S' \equiv f(S, B)
$$

In this equation we call $S$ the pre-state (the state before applying the block $B$), and $S'$ the post-state. The function $f$ is then iterated as we receive new blocks to constantly update the state.

That's the essence of blockchain progress in its purest form, as it existed under proof of work; under proof of work, the state transition function is driven exclusively by processing blocks.

The beacon chain, however, is not block-driven. Rather, it is slot-driven. Updates to the state depends on the progress of slots, whether or not that slot has a block associated with it.

Thus, the beacon chain's state transition function comprises three elements.

1. A per-slot transition function, $S' \equiv f_s(S)$. (The state contains the slot number, so we do not need to supply it.)
2. A per-block transition function $S' \equiv f_b(S, B)$.
3. A per-epoch transition function $S' \equiv f_e(S)$.

Each of these state transition functions needs to be run at the appropriate point when updating the chain, and it is the role of this part of the beacon chain specification to define all of this precisely.

#### Validity conditions

<a id="assert"></a>

> The post-state corresponding to a pre-state `state` and a signed block `signed_block` is defined as `state_transition(state, signed_block)`. State transitions that trigger an unhandled exception (e.g. a failed `assert` or an out-of-range list access) are considered invalid. State transitions that cause a `uint64` overflow or underflow are also considered invalid.

This is a very important statement of how the spec deals with invalid conditions and errors. Basically, if any block is processed that would trigger any kind of exception in the Python code of the specification, then that block is invalid and must be rejected. That means having to undo any state modifications already made in the course of processing the block.

People who do [formal verification](https://github.com/ConsenSys/eth2.0-dafny) of the specification [don't much like this](https://github.com/ethereum/consensus-specs/issues/1797), as having assert statements in running code is an anti-pattern: it is better to ensure that your code can simply never fail.

#### Specification

Anyway, as discussed above, the beacon chain state transition has three elements:

  1. [slot processing](#def_process_slots), which is performed for every slot regardless of what else is happening;
  2. [epoch processing](/part3/transition/epoch/#epoch-processing), which happens every [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch) (32) slots, again regardless of whatever else is going on; and,
  3. [block processing](/part3/transition/block/#block-processing), which happens only in slots for which a beacon block has been received.

<a id="def_state_transition"></a>

```python
def state_transition(state: BeaconState, signed_block: SignedBeaconBlock, validate_result: bool=True) -> None:
    block = signed_block.message
    # Process slots (including those with no blocks) since block
    process_slots(state, block.slot)
    # Verify signature
    if validate_result:
        assert verify_block_signature(state, signed_block)
    # Process block
    process_block(state, block)
    # Verify state root
    if validate_result:
        assert block.state_root == hash_tree_root(state)
```

Although the beacon chain's state transition is conceptually slot-driven, as the spec is written a state transition is triggered by receiving a block to process. That means that we first need to fast-forward from our current slot number in the state (which is the slot at which we last processed a block) to the slot of the block we are processing. We treat intervening slots, if any, as empty. This "fast-forward" is done by [`process_slots()`](#def_process_slots), which also triggers epoch processing as required.

In actual client implementations, state updates will usually be time-based, triggered by moving to the next slot if a block has not been received. However, the fast-forward functionality will be used when exploring different forks in the block tree.

The `validate_result` parameter defaults to `True`, meaning that the block's signature will be checked, and that the result of applying the block to the state results in the same state root that the block claims it does (the "post-states" must match). When creating blocks, however, proposers can set `validate_result` to `False` in order to allow the state root to be calculated, else we'd have a circular dependency. The signature over the initial candidate block is omitted to avoid bad interactions with slashing protection when signing twice in a slot.

|||
|-|------|
| Uses | [`process_slots()`](#def_process_slots), [`verify_block_signature`](#def_verify_block_signature), [`process_block`](/part3/transition/block/#def_process_block) |

<a id="def_verify_block_signature"></a>

```python
def verify_block_signature(state: BeaconState, signed_block: SignedBeaconBlock) -> bool:
    proposer = state.validators[signed_block.message.proposer_index]
    signing_root = compute_signing_root(signed_block.message, get_domain(state, DOMAIN_BEACON_PROPOSER))
    return bls.Verify(proposer.pubkey, signing_root, signed_block.signature)
```

Check that the signature on the block matches the block's contents and the public key of the claimed proposer of the block. This ensures that blocks cannot be forged, or tampered with in transit. All the public keys for validators are stored in the [`Validator`](/part3/containers/dependencies/#validator)s list in state.

|||
|-|------|
| Used&nbsp;by | [`state_transition()`](#def_state_transition) |
| Uses | [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root), [`get_domain()`](/part3/helper/accessors/#def_get_domain), [`bls.Verify()`](/part3/helper/crypto/#bls-signatures) |
| See&nbsp;also | [`DOMAIN_BEACON_PROPOSER`](/part3/config/constants/#domain-types) |

<a id="def_process_slots"></a>

```python
def process_slots(state: BeaconState, slot: Slot) -> None:
    assert state.slot < slot
    while state.slot < slot:
        process_slot(state)
        # Process epoch on the start slot of the next epoch
        if (state.slot + 1) % SLOTS_PER_EPOCH == 0:
            process_epoch(state)
        state.slot = Slot(state.slot + 1)
```

Updates the state from its current slot up to the given slot number assuming that all the intermediate slots are empty (that they do not contain blocks). Iteratively calls [`process_slot()`](#def_process_slot) to apply the empty slot state-transition.

This is where epoch processing is triggered when required. Empty slot processing is lightweight, but any epoch transitions that need to be processed require the full rewards and penalties, and justification&ndash;finalisation apparatus.

|||
|-|------|
| Used&nbsp;by | [`state_transition()`](#def_state_transition) |
| Uses | [`process_slot()`](#def_process_slot), [`process_epoch()`](/part3/transition/epoch/#def_process_epoch) |
| See&nbsp;also | [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch) |

<a id="def_process_slot"></a>

```python
def process_slot(state: BeaconState) -> None:
    # Cache state root
    previous_state_root = hash_tree_root(state)
    state.state_roots[state.slot % SLOTS_PER_HISTORICAL_ROOT] = previous_state_root
    # Cache latest block header state root
    if state.latest_block_header.state_root == Bytes32():
        state.latest_block_header.state_root = previous_state_root
    # Cache block root
    previous_block_root = hash_tree_root(state.latest_block_header)
    state.block_roots[state.slot % SLOTS_PER_HISTORICAL_ROOT] = previous_block_root
```

Apply a single slot state-transition (but updating the slot number, and any required epoch processing is handled by [`process_slots()`](#def_process_slots)). This is done at each slot whether or not there is a block present; if there is no block present then it is the only thing that is done.

Slot processing is almost trivial and consists only of calculating the updated state and block hash tree roots (as necessary), and storing them in the historical lists in the state. In a circular way, the state roots only change over an empty slot state transition due to updating the lists of state and block roots.

[`SLOTS_PER_HISTORICAL_ROOT`](/part3/config/preset/#slots_per_historical_root) is a multiple of [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch), so there is no danger of overwriting the circular lists of `state_roots` and `block_roots`. These will be dealt with correctly during epoch processing.

The only curiosity here is the lines,

```none
    if state.latest_block_header.state_root == Bytes32():
        state.latest_block_header.state_root = previous_state_root
```

This logic [was introduced](https://github.com/ethereum/consensus-specs/pull/711) to avoid a circular dependency while also keeping the state transition clean. Each block that we receive contains a post-state root, but as part of state processing we store the block in the state (in `state.latest_block_header`), thus changing the post-state root.

Therefore, to be able to verify the state transition, we use the convention that the state root of the incoming block, and the state root that we calculate after inserting the block into the state, are both based on a _temporary_ block header that has a stubbed state root, namely `Bytes32()`. This allows the block's claimed post-state root to validated without the circularity. The next time that `process_slots()` is called, the block's stubbed state root is updated to the actual post-state root, as above.

|||
|-|------|
| Used&nbsp;by | [`process_slots()`](#def_process_slots) |
| Uses | [`hash_tree_root`](/part3/helper/crypto/#hash_tree_root) |
| See&nbsp;also | [`SLOTS_PER_HISTORICAL_ROOT`](/part3/config/preset/#slots_per_historical_root) |

### Execution engine <!-- /part3/transition/execution/ -->

Ethereum's "Merge" to proof of stake occurred on the 15th of September 2022. As far as the beacon chain was concerned, the most significant change was that an extra block validity condition now applies. Post-Merge Beacon blocks contain a new [`ExecutionPayload`](/part3/containers/execution/#executionpayload) object which is basically an Eth1 block. For the beacon block to be valid, the contents of its execution payload must be valid according to Ethereum's longstanding block and transaction execution rules (minus any proof of work conditions).

The beacon chain does not know how to validate Ethereum transactions. The entire point of the Merge was to enable beacon chain clients to hand off the validation of the execution payload to a locally connected execution client (formerly an Eth1 client). The beacon chain consensus client does this hand-off via the `notify_new_payload()` function described below.

Architecturally, the `notify_new_payload()` function is accessed via a new interface called the Engine API which the [Bellatrix specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/beacon-chain.md) characterises as follows.

> The implementation-dependent `ExecutionEngine` protocol encapsulates the execution sub-system logic via:
>
>   - a state object `self.execution_state` of type `ExecutionState`
>   - a notification function `self.notify_new_payload` which may apply changes to the `self.execution_state`
>
> _Note_: `notify_new_payload` is a function accessed through the `EXECUTION_ENGINE` module which instantiates the `ExecutionEngine` protocol.
>
> The body of this function is implementation dependent.
> The Engine API may be used to implement this and similarly defined functions via an external execution engine.

[TODO - link to execution API chapter when written]::

#### `notify_new_payload`

<a id="def_notify_new_payload"></a>

```python
def notify_new_payload(self: ExecutionEngine, execution_payload: ExecutionPayload) -> bool:
    """
    Return ``True`` if and only if ``execution_payload`` is valid with respect to ``self.execution_state``.
    """
    ...
```

This function is called during [block processing](/part3/transition/block/) to verify the validity of a beacon block's execution payload. The contents of the execution payload are largely opaque to the consensus layer (hence the `...` in the function definition) and validation of the execution payload relies almost entirely on the execution client. You can think of it as just an external black-box library call if that helps.

|||
|-|------|
| Used&nbsp;by | [`process_execution_payload()`](/part3/transition/block/#def_process_execution_payload) |

### Epoch processing <!-- /part3/transition/epoch/ -->

<a id="def_process_epoch"></a>

```python
def process_epoch(state: BeaconState) -> None:
    process_justification_and_finalization(state)  # [Modified in Altair]
    process_inactivity_updates(state)  # [New in Altair]
    process_rewards_and_penalties(state)  # [Modified in Altair]
    process_registry_updates(state)
    process_slashings(state)  # [Modified in Altair]
    process_eth1_data_reset(state)
    process_effective_balance_updates(state)
    process_slashings_reset(state)
    process_randao_mixes_reset(state)
    process_historical_summaries_update(state)  # [Modified in Capella]
    process_participation_flag_updates(state)  # [New in Altair]
    process_sync_committee_updates(state)  # [New in Altair]
```

The long laundry list of things that need to be done at the end of an epoch. You can see from the comments that a bunch of extra work was added in the Altair upgrade.

|||
|-|------|
| Used&nbsp;by | [`process_slots()`](/part3/transition/#def_process_slots) |
| Uses | All the things below |

#### Justification and finalization

<a id="def_process_justification_and_finalization"></a>

```python
def process_justification_and_finalization(state: BeaconState) -> None:
    # Initial FFG checkpoint values have a `0x00` stub for `root`.
    # Skip FFG updates in the first two epochs to avoid corner cases that might result in modifying this stub.
    if get_current_epoch(state) <= GENESIS_EPOCH + 1:
        return
    previous_indices = get_unslashed_participating_indices(state, TIMELY_TARGET_FLAG_INDEX, get_previous_epoch(state))
    current_indices = get_unslashed_participating_indices(state, TIMELY_TARGET_FLAG_INDEX, get_current_epoch(state))
    total_active_balance = get_total_active_balance(state)
    previous_target_balance = get_total_balance(state, previous_indices)
    current_target_balance = get_total_balance(state, current_indices)
    weigh_justification_and_finalization(state, total_active_balance, previous_target_balance, current_target_balance)
```

I believe the corner cases mentioned in the comments are related to [Issue 849](https://github.com/ethereum/consensus-specs/issues/849)[^fn-ugly-integers]. In any case, skipping justification and finalisation calculations during the first two epochs definitely simplifies things.

[^fn-ugly-integers]: Worth a visit if only to have a chuckle at Jacek's description of `uint`s as "ugly integers".

For the purposes of the Casper FFG finality calculations, we want attestations that have both source and target votes we agree with. If the source vote is incorrect, then the attestation is never processed into the state, so we just need the validators that voted for the correct target, according to their [participation flag indices](/part3/config/constants/#participation-flag-indices).

Since correct target votes can be included up to 32 slots after they are made, we collect votes from both the previous epoch and the current epoch to ensure that we have them all.

Once we know which validators voted for the correct source and head in the current and previous epochs, we add up their effective balances (not actual balances). `total_active_balance` is the sum of the effective balances for all validators that ought to have voted during the current epoch. Slashed, but not exited validators are not included in these calculations.

These aggregate balances are passed to [`weigh_justification_and_finalization()`](#def_weigh_justification_and_finalization) to do the actual work of updating justification and finalisation.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch), [`compute_pulled_up_tip`](/part3/forkchoice/phase0/#compute_pulled_up_tip) |
| Uses | [`get_unslashed_participating_indices()`](/part3/helper/accessors/#def_get_unslashed_participating_indices), [`get_total_active_balance()`](/part3/helper/accessors/#def_get_total_active_balance), [`get_total_balance()`](/part3/helper/accessors/#def_get_total_balance), [`weigh_justification_and_finalization()`](#def_weigh_justification_and_finalization) |
| See&nbsp;also | [participation flag indices](/part3/config/constants/#participation-flag-indices) |

<a id="def_weigh_justification_and_finalization"></a>

```python
def weigh_justification_and_finalization(state: BeaconState,
                                         total_active_balance: Gwei,
                                         previous_epoch_target_balance: Gwei,
                                         current_epoch_target_balance: Gwei) -> None:
    previous_epoch = get_previous_epoch(state)
    current_epoch = get_current_epoch(state)
    old_previous_justified_checkpoint = state.previous_justified_checkpoint
    old_current_justified_checkpoint = state.current_justified_checkpoint

    # Process justifications
    state.previous_justified_checkpoint = state.current_justified_checkpoint
    state.justification_bits[1:] = state.justification_bits[:JUSTIFICATION_BITS_LENGTH - 1]
    state.justification_bits[0] = 0b0
    if previous_epoch_target_balance * 3 >= total_active_balance * 2:
        state.current_justified_checkpoint = Checkpoint(epoch=previous_epoch,
                                                        root=get_block_root(state, previous_epoch))
        state.justification_bits[1] = 0b1
    if current_epoch_target_balance * 3 >= total_active_balance * 2:
        state.current_justified_checkpoint = Checkpoint(epoch=current_epoch,
                                                        root=get_block_root(state, current_epoch))
        state.justification_bits[0] = 0b1

    # Process finalizations
    bits = state.justification_bits
    # The 2nd/3rd/4th most recent epochs are justified, the 2nd using the 4th as source
    if all(bits[1:4]) and old_previous_justified_checkpoint.epoch + 3 == current_epoch:
        state.finalized_checkpoint = old_previous_justified_checkpoint
    # The 2nd/3rd most recent epochs are justified, the 2nd using the 3rd as source
    if all(bits[1:3]) and old_previous_justified_checkpoint.epoch + 2 == current_epoch:
        state.finalized_checkpoint = old_previous_justified_checkpoint
    # The 1st/2nd/3rd most recent epochs are justified, the 1st using the 3rd as source
    if all(bits[0:3]) and old_current_justified_checkpoint.epoch + 2 == current_epoch:
        state.finalized_checkpoint = old_current_justified_checkpoint
    # The 1st/2nd most recent epochs are justified, the 1st using the 2nd as source
    if all(bits[0:2]) and old_current_justified_checkpoint.epoch + 1 == current_epoch:
        state.finalized_checkpoint = old_current_justified_checkpoint
```

This routine handles justification first, and then finalisation.

##### Justification

A supermajority link is a vote with a justified source checkpoint $C_m$ and a target checkpoint $C_n$ that was made by validators controlling more than two-thirds of the stake. If a checkpoint has a supermajority link pointing to it then we consider it justified. So, if more than two-thirds of the validators agree that checkpoint 3 was justified (their source vote) and have checkpoint 4 as their target vote, then we justify checkpoint 4.

We know that all the attestations have source votes that we agree with. The first `if` statement tries to justify the previous epoch's checkpoint seeing if the (source, target) pair is a supermajority. The second `if` statement tries to justify the current epoch's checkpoint. Note that the previous epoch's checkpoint might already have been justified; this is not checked but does not affect the logic.

The justification status of the last four epochs is stored in an array of bits in the state. After shifting the bits along by one at the outset of the routine, the justification status of the current epoch is stored in element 0, the previous in element 1, and so on.

Note that the `total_active_balance` is the current epoch's total balance, so it may not be strictly correct for calculating the supermajority for the previous epoch. However, the rate at which the validator set can change between epochs is [tightly constrained](/part3/config/configuration/#min_per_epoch_churn_limit), so this is not a significant issue.

##### Finalisation

The version of Casper FFG described in the [Gasper paper](https://arxiv.org/abs/2003.03052) uses $k$-finality, which extends the handling of finality in the [original Casper FFG paper](https://arxiv.org/abs/1710.09437). See the [k-finality section](/part2/consensus/casper_ffg/#k-finality) in the chapter on Consensus for more on how it interacts with the safety guarantees of Casper FFG.

In $k$-finality, if we have a consecutive set of $k$ justified checkpoints ${C_j, \ldots, C_{j+k-1}}$, and a supermajority link from $C_j$ to $C_{j+k}$, then $C_j$ is finalised. Also note that this justifies $C_{j+k}$, by the rules above.

The Casper FFG version of this is $1$-finality. So, a supermajority link from a justified checkpoint $C_n$ to the very next checkpoint $C_{n+1}$ both justifies $C_{n+1}$ and finalises $C_n$.

On the beacon chain we are using $2$-finality, since target votes may be included up to an epoch late. In $2$-finality, we keep records of checkpoint justification status for four epochs and have the following conditions for finalisation, where the checkpoint for the current epoch is $C_n$. Note that we have already updated the justification status of $C_n$ and $C_{n-1}$ in this routine, which implies the existence of supermajority links pointing to them if the corresponding bits are set, respectively.

 1. Checkpoints $C_{n-3}$ and $C_{n-2}$ are justified, and there is a supermajority link from $C_{n-3}$ to $C_{n-1}$: finalise $C_{n-3}$.
 2. Checkpoint $C_{n-2}$ is justified, and there is a supermajority link from $C_{n-2}$ to $C_{n-1}$: finalise $C_{n-2}$. This is equivalent to $1$-finality applied to the previous epoch.
 3. Checkpoints $C_{n-2}$ and $C_{n-1}$ are justified, and there is a supermajority link from $C_{n-2}$ to $C_n$: finalise $C_{n-2}$.
 4. Checkpoint $C_{n-1}$ is justified, and there is a supermajority link from $C_{n-1}$ to $C_n$: finalise $C_{n-1}$. This is equivalent to $1$-finality applied to the current epoch.

<a id="img_consensus_2_finality"></a>
<figure class="diagram" style="width: 65%">

![A diagram of the four 2-finality scenarios.](images/diagrams/consensus-2-finality.svg)

<figcaption>

The four cases of 2-finality. In each case the supermajority link causes the checkpoint at its start (the source) to become finalised and the checkpoint at its end (the target) to become justified. Checkpoint numbers are along the bottom.

</figcaption>
</figure>

Almost always we would expect to see only the $1$-finality cases, in particular, case 4. The $2$-finality cases would occur only in situations where many attestations are delayed, or when we are very close to the 2/3rds participation threshold. Note that these evaluations stack, so it is possible for rule 2 to finalise $C_{n-2}$ and then for rule 4 to immediately finalise $C_{n-1}$, for example.

For the uninitiated, in Python's array slice syntax, `bits[1:4]` means bits 1, 2, and 3 (but not 4). This always trips me up.

|||
|-|------|
| Used&nbsp;by | [`process_justification_and_finalization()`](#def_process_justification_and_finalization) |
| Uses | [`get_block_root()`](/part3/helper/accessors/#def_get_block_root) |
| See&nbsp;also | [`JUSTIFICATION_BITS_LENGTH`](/part3/config/constants/#justification_bits_length), [`Checkpoint`](/part3/containers/dependencies/#checkpoint) |

#### Inactivity scores

<a id="def_process_inactivity_updates"></a>

```python
def process_inactivity_updates(state: BeaconState) -> None:
    # Skip the genesis epoch as score updates are based on the previous epoch participation
    if get_current_epoch(state) == GENESIS_EPOCH:
        return

    for index in get_eligible_validator_indices(state):
        # Increase the inactivity score of inactive validators
        if index in get_unslashed_participating_indices(state, TIMELY_TARGET_FLAG_INDEX, get_previous_epoch(state)):
            state.inactivity_scores[index] -= min(1, state.inactivity_scores[index])
        else:
            state.inactivity_scores[index] += INACTIVITY_SCORE_BIAS
        # Decrease the inactivity score of all eligible validators during a leak-free epoch
        if not is_in_inactivity_leak(state):
            state.inactivity_scores[index] -= min(INACTIVITY_SCORE_RECOVERY_RATE, state.inactivity_scores[index])
```

Since the Altair upgrade, each validator has an individual inactivity score in the beacon state which is updated as follows.

  - At the end of epoch $N$, irrespective of the inactivity leak,
    - decrease the score by one when the validator made a correct and [timely target vote](/part3/config/constants/#participation-flag-indices) during epoch $N-1$, and
    - increase the score by [`INACTIVITY_SCORE_BIAS`](/part3/config/configuration/#inactivity_score_bias) otherwise. Note that [`get_eligible_validator_indices()`](#def_get_eligible_validator_indices) includes slashed but not yet withdrawable validators: slashed validators are treated as not participating, whatever they actually do.
  - When _not_ in an inactivity leak
    - decrease all validators' scores by [`INACTIVITY_SCORE_RECOVERY_RATE`](/part3/config/configuration/#inactivity_score_recovery_rate).

<a id="img_incentives_inactivity_scores_flow"></a>
<figure class="diagram">

![Flowchart showing how inactivity score updates are calculated.](images/diagrams/incentives-inactivity_scores_flow.svg)

<figcaption>

How each validator's inactivity score is updated. The happy flow is right through the middle. "Active", when updating the scores at the end of epoch $N$, means having made a correct and timely target vote in epoch $N-1$.

</figcaption>
</figure>

There is a floor of zero on the score. So, outside a leak, validators' scores will rapidly return to zero and stay there, since `INACTIVITY_SCORE_RECOVERY_RATE` is greater than `INACTIVITY_SCORE_BIAS`.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| Uses | [`get_eligible_validator_indices()`](#def_get_eligible_validator_indices), [`get_unslashed_participating_indices()`](/part3/helper/accessors/#get_unslashed_participating_indices), [`is_in_inactivity_leak()`](/part3/transition/epoch/#def_is_in_inactivity_leak) |
| See&nbsp;also | [`INACTIVITY_SCORE_BIAS`](/part3/config/configuration/#inactivity_score_bias), [`INACTIVITY_SCORE_RECOVERY_RATE`](/part3/config/configuration/#inactivity_score_recovery_rate), [`INACTIVITY_SCORE_RECOVERY_RATE`](/part3/config/configuration/#inactivity_score_recovery_rate) |

#### Reward and penalty calculations

Without wanting to go full [Yellow Paper](https://ethereum.github.io/yellowpaper/paper.pdf) on you, I am going to adopt a little notation to help analyse the rewards.

We will define a base reward $B$ that we will see turns out to be the expected long-run average income of an optimally performing validator per epoch (ignoring validator set size changes). The total number of active validators is $N$.

The base reward is calculated from a [base reward per increment](#def_get_base_reward_per_increment), $b$. An "increment" is a unit of effective balance in terms of [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment). $B = 32b$ because [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) = `32` ` * ` `EFFECTIVE_BALANCE_INCREMENT`

Other quantities we will use in rewards calculation are the [incentivization weights](/part3/config/constants/#incentivization-weights): $W_s$, $W_t$, $W_h$, and $W_y$ being the weights for correct source, target, head, and sync committee votes respectively; $W_p$ being the proposer weight; and the weight denominator $W_{\Sigma}$ which is the sum of the weights.

Issuance for regular rewards happens in four ways:

  - $I_A$ is the maximum total reward for all validators attesting in an epoch;
  - $I_{A_P}$ is the maximum reward issued to proposers in an epoch for including attestations;
  - $I_S$ is the maximum total reward for all sync committee participants in an epoch; and
  - $I_{S_P}$ is the maximum reward issued to proposers in an epoch for including sync aggregates;

Under [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas), [`process_attestation()`](/part3/transition/block/#def_process_attestation), and [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) we find that these work out as follows in terms of $B$ and $N$:

$$
\begin{aligned}
&I_A = \frac{W_s + W_t + W_h}{W_{\Sigma}}NB \\
&I_{A_P} = \frac{W_p}{W_{\Sigma} - W_p}I_A \\
&I_S = \frac{W_y}{W_{\Sigma}}NB \\
&I_{S_P} = \frac{W_p}{W_{\Sigma} - W_p}I_S
\end{aligned}
$$

To find the total optimal issuance per epoch, we can first sum $I_A$ and $I_S$,

$$
I_A + I_S = \frac{W_s + W_t + W_h + W_y}{W_{\Sigma}}NB = \frac{W_{\Sigma} - W_p}{W_{\Sigma}}NB
$$

Now adding in the proposer rewards,

$$
I_A + I_S + I_{A_P} + I_{S_P} = \frac{W_{\Sigma} - W_p}{W_{\Sigma}}(1 + \frac{W_p}{W_{\Sigma} - W_p})NB = (\frac{W_{\Sigma} - W_p}{W_{\Sigma}} + \frac{W_p}{W_{\Sigma}})NB = NB
$$

So, we see that every epoch, $NB$ Gwei is awarded to $N$ validators. Every validator participates in attesting, and proposing and sync committee duties are uniformly random, so the long-term expected income per optimally performing validator per epoch is $B$ Gwei.

##### Helpers

<a id="def_get_base_reward_per_increment"></a>

```python
def get_base_reward_per_increment(state: BeaconState) -> Gwei:
    return Gwei(EFFECTIVE_BALANCE_INCREMENT * BASE_REWARD_FACTOR // integer_squareroot(get_total_active_balance(state)))
```

The base reward per increment is the fundamental unit of reward in terms of which all other regular rewards and penalties are calculated. We will denote the base reward per increment, $b$.

As I noted under [`BASE_REWARD_FACTOR`](/part3/config/preset/#base_reward_factor), this is the big knob to turn if we wish to increase or decrease the total reward for participating in Eth2, otherwise known as the issuance rate of new Ether.

An increment is a single unit of a validator's effective balance, denominated in terms of [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment), which happens to be one Ether. So, an increment is 1 Ether of effective balance, and maximally effective validator has 32 increments.

The base reward per increment is inversely proportional to the square root of the total balance of all active validators. This means that, as the number $N$ of validators increases, the reward per validator decreases as $\frac{1}{\sqrt{N}}$, and the overall issuance per epoch increases as $\sqrt{N}$.

The decrease with increasing $N$ in per-validator rewards provides a price discovery mechanism: the idea is that an equilibrium will be found where the total number of validators results in a reward similar to returns available elsewhere for similar risk. A different curve could have been chosen for the rewards profile. For example, the inverse of total balance rather than its square root would keep total issuance constant. The [section on Issuance](/part2/incentives/issuance/) has a deeper exploration of these topics.

|||
|-|------|
| Used&nbsp;by | [`get_base_reward()`](#def_get_base_reward), [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate) |
| Uses | [`integer_squareroot()`](/part3/helper/math/#def_integer_squareroot), [`get_total_active_balance()`](/part3/helper/accessors/#def_get_total_active_balance) |

<a id="def_get_base_reward"></a>

```python
def get_base_reward(state: BeaconState, index: ValidatorIndex) -> Gwei:
    """
    Return the base reward for the validator defined by ``index`` with respect to the current ``state``.
    """
    increments = state.validators[index].effective_balance // EFFECTIVE_BALANCE_INCREMENT
    return Gwei(increments * get_base_reward_per_increment(state))
```

The base reward is the reward that an optimally performing validator can expect to earn on average per epoch, over the long term. It is proportional to the validator's effective balance; a validator with [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) can expect to receive the full base reward $B = 32b$ per epoch on a long-term average.

|||
|-|------|
| Used&nbsp;by | [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas), [`process_attestation()`](/part3/transition/block/#def_process_attestation) |
| Uses | [`get_base_reward_per_increment()`](#def_get_base_reward_per_increment) |
| See&nbsp;also | [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment) |

<a id="def_get_finality_delay"></a>

```python
def get_finality_delay(state: BeaconState) -> uint64:
    return get_previous_epoch(state) - state.finalized_checkpoint.epoch
```

Returns the number of epochs since the last finalised checkpoint (minus one). In ideal running this ought to be zero: during epoch processing we aim to have justified the checkpoint in the current epoch and finalised the checkpoint in the previous epoch. A delay in finalisation suggests a chain split or a large fraction of validators going offline.

|||
|-|------|
| Used&nbsp;by | [`is_in_inactivity_leak()`](#def_is_in_inactivity_leak) |

<a id="def_is_in_inactivity_leak"></a>

```python
def is_in_inactivity_leak(state: BeaconState) -> bool:
    return get_finality_delay(state) > MIN_EPOCHS_TO_INACTIVITY_PENALTY
```

If the beacon chain has not managed to finalise a checkpoint for [`MIN_EPOCHS_TO_INACTIVITY_PENALTY`](/part3/config/preset/#min_epochs_to_inactivity_penalty) epochs (that is, four epochs), then the chain enters the [inactivity leak](/part3/config/preset/#inactivity_penalty_quotient). In this mode, penalties for non-participation are heavily increased, with the goal of reducing the proportion of stake controlled by non-participants, and eventually regaining finality.

|||
|-|------|
| Used&nbsp;by | [`get_flag_index_deltas()`](/part3/helper/accessors/#get_flag_index_deltas), [`process_inactivity_updates()`](#def_process_inactivity_updates) |
| Uses | [`get_finality_delay()`](#def_get_finality_delay) |
| See&nbsp;also | [inactivity leak](/part3/config/preset/#inactivity_penalty_quotient), [`MIN_EPOCHS_TO_INACTIVITY_PENALTY`](/part3/config/preset/#min_epochs_to_inactivity_penalty) |

<a id="def_get_eligible_validator_indices"></a>

```python
def get_eligible_validator_indices(state: BeaconState) -> Sequence[ValidatorIndex]:
    previous_epoch = get_previous_epoch(state)
    return [
        ValidatorIndex(index) for index, v in enumerate(state.validators)
        if is_active_validator(v, previous_epoch) or (v.slashed and previous_epoch + 1 < v.withdrawable_epoch)
    ]
```

These are the validators that were subject to rewards and penalties in the previous epoch.

The list differs from the active validator set returned by [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices) by including slashed but not fully exited validators in addition to the ones marked active. Slashed validators are subject to penalties right up to when they become withdrawable and are thus fully exited.

|||
|-|------|
| Used&nbsp;by | [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas), [`process_inactivity_updates()`](#def_process_inactivity_updates), [`get_inactivity_penalty_deltas()`](#def_get_inactivity_penalty_deltas) |
| Uses | [`is_active_validator()`](/part3/helper/predicates/#def_is_active_validator) |

##### Inactivity penalty deltas

<a id="def_get_inactivity_penalty_deltas"></a>

```python
def get_inactivity_penalty_deltas(state: BeaconState) -> Tuple[Sequence[Gwei], Sequence[Gwei]]:
    """
    Return the inactivity penalty deltas by considering timely target participation flags and inactivity scores.
    """
    rewards = [Gwei(0) for _ in range(len(state.validators))]
    penalties = [Gwei(0) for _ in range(len(state.validators))]
    previous_epoch = get_previous_epoch(state)
    matching_target_indices = get_unslashed_participating_indices(state, TIMELY_TARGET_FLAG_INDEX, previous_epoch)
    for index in get_eligible_validator_indices(state):
        if index not in matching_target_indices:
            penalty_numerator = state.validators[index].effective_balance * state.inactivity_scores[index]
            penalty_denominator = INACTIVITY_SCORE_BIAS * INACTIVITY_PENALTY_QUOTIENT_BELLATRIX
            penalties[index] += Gwei(penalty_numerator // penalty_denominator)
    return rewards, penalties
```

Validators receive penalties proportional to their individual inactivity scores, even when the beacon chain is not in an [inactivity leak](/part3/transition/epoch/#def_is_in_inactivity_leak). However, these scores reduce to zero fairly rapidly outside a leak. This is a change from Phase&nbsp;0 in which inactivity penalties were applied only during leaks.

All unslashed validators that made a correct and timely [target vote](/part3/config/constants/#participation-flag-indices) in the previous epoch are identified by [`get_unslashed_participating_indices()`](/part3/helper/accessors/#get_unslashed_participating_indices), and all other active validators receive a penalty, including slashed validators.

The penalty is proportional to the validator's effective balance and its inactivity score. See [`INACTIVITY_PENALTY_QUOTIENT_BELLATRIX`](/part3/config/preset/#inactivity_penalty_quotient) for more details of the calculation, and [`INACTIVITY_SCORE_RECOVERY_RATE`](/part3/config/configuration/#inactivity_score_recovery_rate) for some charts of how the penalties accrue.

The returned `rewards` array always contains only zeros. It's here just to make the Python syntax simpler in the calling routine.

|||
|-|------|
| Used&nbsp;by | [`def_process_rewards_and_penalties()`](#def_process_rewards_and_penalties) |
| Uses | [`get_unslashed_participating_indices()`](/part3/helper/accessors/#get_unslashed_participating_indices), [`get_eligible_validator_indices()`](/part3/transition/epoch/#def_get_eligible_validator_indices) |
| See&nbsp;also | [Inactivity Scores](#inactivity-scores), [`INACTIVITY_PENALTY_QUOTIENT_BELLATRIX`](/part3/config/preset/#inactivity_penalty_quotient), [`INACTIVITY_SCORE_RECOVERY_RATE`](/part3/config/configuration/#inactivity_score_recovery_rate) |

##### Process rewards and penalties

<a id="def_process_rewards_and_penalties"></a>

```python
def process_rewards_and_penalties(state: BeaconState) -> None:
    # No rewards are applied at the end of `GENESIS_EPOCH` because rewards are for work done in the previous epoch
    if get_current_epoch(state) == GENESIS_EPOCH:
        return

    flag_deltas = [get_flag_index_deltas(state, flag_index) for flag_index in range(len(PARTICIPATION_FLAG_WEIGHTS))]
    deltas = flag_deltas + [get_inactivity_penalty_deltas(state)]
    for (rewards, penalties) in deltas:
        for index in range(len(state.validators)):
            increase_balance(state, ValidatorIndex(index), rewards[index])
            decrease_balance(state, ValidatorIndex(index), penalties[index])
```

This is where validators are rewarded and penalised according to their attestation records.

Attestations included in beacon blocks were processed by [`process_attestation`](/part3/transition/block/#def_process_attestation) as blocks were received, and [flags](/part3/config/types/#participationflags) were set in the beacon state according to their timeliness and correctness. These flags are now processed into rewards and penalties for each validator by calling [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas) for each of the [flag types](/part3/config/constants/#participation-flag-indices).

Once the normal attestation rewards and penalties have been calculated, [additional penalties](#def_get_inactivity_penalty_deltas) based on validators' inactivity scores are accumulated.

As noted elsewhere, rewards and penalties are handled separately from each other since we don't do negative numbers.

For reference, the only other places where rewards and penalties are applied are as follows:

  - during block processing: for [sync committee participation](/part3/transition/block/#def_process_sync_aggregate), when applying the [proposer reward](/part3/transition/block/#def_process_attestation), and when applying initial [slashing rewards and penalties](/part3/helper/mutators/#def_slash_validator).
  - during epoch processing: when applying [extended slashing penalties](/part3/transition/epoch/#def_process_slashings).

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| Uses | [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas), [`get_inactivity_penalty_deltas()`](#def_get_inactivity_penalty_deltas), [`increase_balance()`](/part3/helper/mutators/#def_increase_balance), [`decrease_balance()`](/part3/helper/mutators/#def_decrease_balance) |
| See&nbsp;also | [`ParticipationFlags`](/part3/config/types/#participationflags), [`PARTICIPATION_FLAG_WEIGHTS`](/part3/config/constants/#participation_flag_weights) |

#### Registry updates

<a id="def_process_registry_updates"></a>

```python
def process_registry_updates(state: BeaconState) -> None:
    # Process activation eligibility and ejections
    for index, validator in enumerate(state.validators):
        if is_eligible_for_activation_queue(validator):
            validator.activation_eligibility_epoch = get_current_epoch(state) + 1

        if (
            is_active_validator(validator, get_current_epoch(state))
            and validator.effective_balance <= EJECTION_BALANCE
        ):
            initiate_validator_exit(state, ValidatorIndex(index))

    # Queue validators eligible for activation and not yet dequeued for activation
    activation_queue = sorted([
        index for index, validator in enumerate(state.validators)
        if is_eligible_for_activation(state, validator)
        # Order by the sequence of activation_eligibility_epoch setting and then index
    ], key=lambda index: (state.validators[index].activation_eligibility_epoch, index))
    # Dequeued validators for activation up to churn limit
    for index in activation_queue[:get_validator_churn_limit(state)]:
        validator = state.validators[index]
        validator.activation_epoch = compute_activation_exit_epoch(get_current_epoch(state))
```

The [`Registry`](/part3/containers/state/#registry) is the part of the beacon state that stores [`Validator`](/part3/containers/dependencies/#validator) records. These particular updates are, for the most part, concerned with moving validators through the activation queue.

[`is_eligible_for_activation_queue()`](/part3/helper/predicates/#def_is_eligible_for_activation_queue) finds validators that have a sufficient deposit amount yet their `activation_eligibility_epoch` is still set to [`FAR_FUTURE_EPOCH`](/part3/config/constants/#far_future_epoch). These will be at most the validators for which deposits were processed during the last epoch, potentially up to `MAX_DEPOSITS * SLOTS_PER_EPOCH`, which is 512 (minus any partial deposits that don't yet add up to a whole deposit). These have their `activation_eligibility_epoch` set to the next epoch. They will become eligible for activation once that epoch is finalised &ndash; "eligible for activation" means only that they can be added to the activation queue; they will not become active until they reach the end of the queue.

Next, any validators whose effective balance has fallen to [`EJECTION_BALANCE`](/part3/config/configuration/#ejection_balance) have their exit initiated.

[`is_eligible_for_activation()`](/part3/helper/predicates/#is_eligible_for_activation) selects validators whose `activation_eligibility_epoch` has just been finalised. The list of these is ordered by eligibility epoch, and then by index. There might be multiple eligibility epochs in the list if finalisation got delayed for some reason.

Finally, the first [`get_validator_churn_limit()`](/part3/helper/accessors/#def_get_validator_churn_limit) validators in the list get their activation epochs set to [`compute_activation_exit_epoch()`](/part3/helper/misc/#def_compute_activation_exit_epoch).

On first sight, you'd think that the activation epochs of the whole queue could be set here, rather than just a single epoch's worth. But at some point, `get_validator_churn_limit()` will change unpredictably (we don't know when validators will exit), which makes that infeasible. Though, curiously, that is exactly what [`initiate_validator_exit()`](/part3/helper/mutators/#def_initiate_validator_exit) does. Anyway, clients could optimise this by persisting the sorted activation queue rather than recalculating it.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| Uses | [`is_eligible_for_activation_queue()`](/part3/helper/predicates/#def_is_eligible_for_activation_queue), [`is_active_validator()`](/part3/helper/predicates/#def_is_active_validator), [`initiate_validator_exit()`](/part3/helper/mutators/#initiate_validator_exit), [`is_eligible_for_activation()`](/part3/helper/predicates/#def_is_eligible_for_activation), [`get_validator_churn_limit()`](/part3/helper/accessors/#def_get_validator_churn_limit), [`compute_activation_exit_epoch()`](/part3/helper/misc/#def_compute_activation_exit_epoch) |
| See&nbsp;also | [`Validator`](/part3/containers/dependencies/#validator), [`EJECTION_BALANCE`](/part3/config/configuration/#ejection_balance) |

#### Slashings

<a id="def_process_slashings"></a>

```python
def process_slashings(state: BeaconState) -> None:
    epoch = get_current_epoch(state)
    total_balance = get_total_active_balance(state)
    adjusted_total_slashing_balance = min(
        sum(state.slashings) * PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX,
        total_balance
    )
    for index, validator in enumerate(state.validators):
        if validator.slashed and epoch + EPOCHS_PER_SLASHINGS_VECTOR // 2 == validator.withdrawable_epoch:
            increment = EFFECTIVE_BALANCE_INCREMENT  # Factored out from penalty numerator to avoid uint64 overflow
            penalty_numerator = validator.effective_balance // increment * adjusted_total_slashing_balance
            penalty = penalty_numerator // total_balance * increment
            decrease_balance(state, ValidatorIndex(index), penalty)
```

Slashing penalties are applied in two stages: the first stage is in [`slash_validator()`](/part3/helper/mutators/#def_slash_validator), immediately on detection; the second stage is here.

In `slash_validator()` the withdrawable epoch is set [`EPOCHS_PER_SLASHINGS_VECTOR`](/part3/config/preset/#epochs_per_slashings_vector) in the future, so in this function we are considering all slashed validators that are halfway to being withdrawable, that is, completely exited from the protocol. Equivalently, they were slashed `EPOCHS_PER_SLASHINGS_VECTOR` ` // ` `2` epochs ago (about 18 days).

To calculate the additional slashing penalty, we do the following:

 1. Find the sum of the effective balances (at the time of the slashing) of all validators that were slashed in the previous `EPOCHS_PER_SLASHINGS_VECTOR` epochs (36 days). These are stored as a vector in the state.
 2. Multiply this sum by [`PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX`](/part3/config/preset/#proportional_slashing_multiplier), but cap the result at `total_balance`, the total active balance of all validators.
 3. For each slashed validator being considered, multiply its effective balance by the result of #2 and then divide by the `total_balance`. This results in an amount between zero and the full effective balance of the validator. That amount is subtracted from its actual balance as the penalty. Note that the effective balance could exceed the actual balance in odd corner cases, but [`decrease_balance()`](/part3/helper/mutators/#def_decrease_balance) ensures the balance does not go negative.

If only a single validator were slashed within the 36 days, then this secondary penalty is tiny (actually zero, see below). If one-third of validators are slashed (the minimum required to finalise conflicting blocks), then, with `PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX` set to three, a successful chain attack will result in the attackers losing their entire effective balances.

Interestingly, due to the way the integer arithmetic is constructed in this routine, in particular the factoring out of `increment`, the result of this calculation will be zero if `validator.effective_balance * adjusted_total_slashing_balance` is less than `total_balance`. Effectively, the penalty is rounded down to the nearest whole amount of Ether. Issues [1322](https://github.com/ethereum/consensus-specs/issues/1322) and [2161](https://github.com/ethereum/consensus-specs/issues/2161) discuss this. In the end, the consequence is that when there are few slashings there is no extra correlated slashing penalty at all, which is probably a good thing.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| Uses | [`get_total_active_balance()`](/part3/helper/accessors/#def_get_total_active_balance), [`decrease_balance()`](/part3/helper/mutators/#def_decrease_balance) |
| See&nbsp;also | [`slash_validator()`](/part3/helper/mutators/#def_slash_validator), [`EPOCHS_PER_SLASHINGS_VECTOR`](/part3/config/preset/#epochs_per_slashings_vector), [`PROPORTIONAL_SLASHING_MULTIPLIER_BELLATRIX`](/part3/config/preset/#proportional_slashing_multiplier) |

#### Eth1 data votes updates

<a id="def_process_eth1_data_reset"></a>

```python
def process_eth1_data_reset(state: BeaconState) -> None:
    next_epoch = Epoch(get_current_epoch(state) + 1)
    # Reset eth1 data votes
    if next_epoch % EPOCHS_PER_ETH1_VOTING_PERIOD == 0:
        state.eth1_data_votes = []
```

There is a fixed period during which beacon block proposers vote on their view of the Eth1 deposit contract and try to come to a simple majority agreement. At the end of the period, the record of votes is cleared and voting begins again, whether or not agreement was reached during the period.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| See&nbsp;also | [`EPOCHS_PER_ETH1_VOTING_PERIOD`](/part3/config/preset/#epochs_per_eth1_voting_period), [`Eth1Data`](/part3/containers/dependencies/#eth1data) |

#### Effective balances updates

<a id="def_process_effective_balance_updates"></a>

```python
def process_effective_balance_updates(state: BeaconState) -> None:
    # Update effective balances with hysteresis
    for index, validator in enumerate(state.validators):
        balance = state.balances[index]
        HYSTERESIS_INCREMENT = uint64(EFFECTIVE_BALANCE_INCREMENT // HYSTERESIS_QUOTIENT)
        DOWNWARD_THRESHOLD = HYSTERESIS_INCREMENT * HYSTERESIS_DOWNWARD_MULTIPLIER
        UPWARD_THRESHOLD = HYSTERESIS_INCREMENT * HYSTERESIS_UPWARD_MULTIPLIER
        if (
            balance + DOWNWARD_THRESHOLD < validator.effective_balance
            or validator.effective_balance + UPWARD_THRESHOLD < balance
        ):
            validator.effective_balance = min(balance - balance % EFFECTIVE_BALANCE_INCREMENT, MAX_EFFECTIVE_BALANCE)
```

Each validator's balance is represented twice in the state: once accurately in a list separate from validator records, and once in a [coarse-grained format](/part3/config/preset/#effective_balance_increment) within the validator's record. Only effective balances are used in calculations within the spec, but rewards and penalties are applied to actual balances. This routine is where effective balances are updated once per epoch to follow the actual balances.

A hysteresis mechanism is used when calculating the effective balance of a validator when its actual balance changes. See [Hysteresis Parameters](/part3/config/preset/#hysteresis-parameters) for more discussion of this, and the values of the related constants. With the current values, a validator's effective balance drops to `X`&nbsp;ETH when its actual balance drops below `X.75`&nbsp;ETH, and increases to `Y`&nbsp;ETH when its actual balance rises above `Y.25`&nbsp;ETH. The hysteresis mechanism ensures that effective balances change infrequently, which means that the list of validator records needs to be re-hashed only infrequently when calculating the state root, saving considerably on work.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| See&nbsp;also | [Hysteresis Parameters](/part3/config/preset/#hysteresis-parameters) |

#### Slashings balances updates

<a id="def_process_slashings_reset"></a>

```python
def process_slashings_reset(state: BeaconState) -> None:
    next_epoch = Epoch(get_current_epoch(state) + 1)
    # Reset slashings
    state.slashings[next_epoch % EPOCHS_PER_SLASHINGS_VECTOR] = Gwei(0)
```

`state.slashings` is a circular list of length [`EPOCHS_PER_SLASHINGS_VECTOR`](/part3/config/preset/#epochs_per_slashings_vector) that contains the total of the effective balances of all validators that have been slashed at each epoch. These are used to apply a correlated slashing penalty to slashed validators before they are exited. Each epoch we overwrite the oldest entry with zero, and it becomes the current entry.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| See&nbsp;also | [`process_slashings()`](/part3/transition/epoch/#def_process_slashings), [`EPOCHS_PER_SLASHINGS_VECTOR`](/part3/config/preset/#epochs_per_slashings_vector) |

#### Randao mixes updates

<a id="def_process_randao_mixes_reset"></a>

```python
def process_randao_mixes_reset(state: BeaconState) -> None:
    current_epoch = get_current_epoch(state)
    next_epoch = Epoch(current_epoch + 1)
    # Set randao mix
    state.randao_mixes[next_epoch % EPOCHS_PER_HISTORICAL_VECTOR] = get_randao_mix(state, current_epoch)
```

`state.randao_mixes` is a circular list of length [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector). The current value of the RANDAO, which is updated with every block that arrives, is stored at position `state.randao_mixes[current_epoch % EPOCHS_PER_HISTORICAL_VECTOR]`, as per [`get_randao_mix()`](/part3/helper/accessors/#def_get_randao_mix).

At the end of every epoch, final value of the RANDAO for this epoch is copied over to become the starting value of the randao for the next, preserving the remaining entries as historical values.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| Uses | [`get_randao_mix()`](/part3/helper/accessors/#def_get_randao_mix) |
| See&nbsp;also | [`process_randao()`](/part3/transition/block/#def_process_randao), [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector) |

#### Historical summaries updates

<a id="def_process_historical_summaries_update"></a>

```python
def process_historical_summaries_update(state: BeaconState) -> None:
    # Set historical block root accumulator.
    next_epoch = Epoch(get_current_epoch(state) + 1)
    if next_epoch % (SLOTS_PER_HISTORICAL_ROOT // SLOTS_PER_EPOCH) == 0:
        historical_summary = HistoricalSummary(
            block_summary_root=hash_tree_root(state.block_roots),
            state_summary_root=hash_tree_root(state.state_roots),
        )
        state.historical_summaries.append(historical_summary)
```

This routine replaced [`process_historical_roots_update()`](/../bellatrix/part3/transition/epoch/#def_process_historical_roots_update) at the [Capella upgrade](/part4/history/capella/).

Previously, both the `state.block_roots` and `state.state_roots` lists were Merkleized together into a single root before being added to the `state.historical_roots` [double batched accumulator](https://ethresear.ch/t/double-batched-merkle-log-accumulator/571?u=benjaminion). Now they are separately Merkleized and appended to `state.historical_summaries` via the [`HistoricalSummary`](/part3/containers/dependencies/#historicalsummary) container. The Capella upgrade [changed this](https://github.com/ethereum/consensus-specs/pull/2649) to make it possible to validate past block history without having to know the state history.

The summary is appended to the list every [`SLOTS_PER_HISTORICAL_ROOT`](/part3/config/preset/#slots_per_historical_root) slots. At 64 bytes per summary, the list will grow at the rate of 20&nbsp;KB per year. The corresponding block and state root lists in the beacon state are circular and just get overwritten in the next period.

The `process_historical_roots_update()` function that this replaces remains [documented in the Bellatrix edition](/../bellatrix/part3/transition/epoch/#def_process_historical_roots_updates).

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| See&nbsp;also | [`HistoricalSummary`](/part3/containers/dependencies/#historicalsummary), [`SLOTS_PER_HISTORICAL_ROOT`](/part3/config/preset/#slots_per_historical_root) |

#### Participation flags updates

<a id="def_process_participation_flag_updates"></a>

```python
def process_participation_flag_updates(state: BeaconState) -> None:
    state.previous_epoch_participation = state.current_epoch_participation
    state.current_epoch_participation = [ParticipationFlags(0b0000_0000) for _ in range(len(state.validators))]
```

Two epochs' worth of validator participation flags (that record validators' attestation activity) are stored. At the end of every epoch the current becomes the previous, and a new empty list becomes current.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| See&nbsp;also | [`ParticipationFlags`](/part3/config/types/#participationflags) |

#### Sync committee updates

<a id="def_process_sync_committee_updates"></a>

```python
def process_sync_committee_updates(state: BeaconState) -> None:
    next_epoch = get_current_epoch(state) + Epoch(1)
    if next_epoch % EPOCHS_PER_SYNC_COMMITTEE_PERIOD == 0:
        state.current_sync_committee = state.next_sync_committee
        state.next_sync_committee = get_next_sync_committee(state)
```

Sync committees are rotated every [`EPOCHS_PER_SYNC_COMMITTEE_PERIOD`](/part3/config/preset/#epochs_per_sync_committee_period). The next sync committee is ready and waiting so that validators can prepare in advance by subscribing to the necessary subnets. That becomes the current sync committee, and the next is calculated.

|||
|-|------|
| Used&nbsp;by | [`process_epoch()`](#def_process_epoch) |
| Uses | [`get_next_sync_committee()`](/part3/helper/accessors/#def_get_next_sync_committee) |
| See&nbsp;also | [`EPOCHS_PER_SYNC_COMMITTEE_PERIOD`](/part3/config/preset/#epochs_per_sync_committee_period) |

### Block processing <!-- /part3/transition/block/ -->

<a id="def_process_block"></a>

```python
def process_block(state: BeaconState, block: BeaconBlock) -> None:
    process_block_header(state, block)
    if is_execution_enabled(state, block.body):
        process_withdrawals(state, block.body.execution_payload)  # [New in Capella]
        process_execution_payload(state, block.body.execution_payload, EXECUTION_ENGINE)  # [Modified in Capella]
    process_randao(state, block.body)
    process_eth1_data(state, block.body)
    process_operations(state, block.body)  # [Modified in Capella]
    process_sync_aggregate(state, block.body.sync_aggregate)
```

These are the tasks that the beacon node performs in order to process a block and update the state. If any of the called functions triggers the failure of an `assert` statement, or any other kind of exception, then the [entire block is invalid](/part3/transition/#assert), and any state changes must be rolled back.

> _Note_: The call to the `process_execution_payload` must happen before the call to the `process_randao` as the former depends on the `randao_mix` computed with the reveal of the previous block.

The call to [`process_execution_payload()`](#def_process_execution_payload) was added in the Bellatrix pre-Merge upgrade. The [`EXECUTION_ENGINE` object](/part3/transition/execution/) is not really defined in the beacon chain spec, but corresponds to an API that calls out to an attached execution client (formerly Eth1 client) that will do most of the payload validation.

[`process_operations()`](#def_process_operations) covers the processing of any slashing reports (proposer and attester) in the block, any attestations, any deposits, and any voluntary exits.

|||
|-|------|
| Used&nbsp;by | [`state_transition()`](/part3/transition/#def_state_transition) |
| Uses | [`process_block_header()`](#def_process_block_header), [`is_execution_enabled()`](/part3/helper/predicates/#def_is_execution_enabled), [`process_execution_payload()`](#def_process_execution_payload), [`process_randao()`](#def_process_randao), [`process_eth1_data()`](#def_process_eth1_data), [`process_operations()`](#def_process_operations), [`process_sync_aggregate()`](#def_process_sync_aggregate) |

#### Block header

<a id="def_process_block_header"></a>

```python
def process_block_header(state: BeaconState, block: BeaconBlock) -> None:
    # Verify that the slots match
    assert block.slot == state.slot
    # Verify that the block is newer than latest block header
    assert block.slot > state.latest_block_header.slot
    # Verify that proposer index is the correct index
    assert block.proposer_index == get_beacon_proposer_index(state)
    # Verify that the parent matches
    assert block.parent_root == hash_tree_root(state.latest_block_header)
    # Cache current block as the new latest block
    state.latest_block_header = BeaconBlockHeader(
        slot=block.slot,
        proposer_index=block.proposer_index,
        parent_root=block.parent_root,
        state_root=Bytes32(),  # Overwritten in the next process_slot call
        body_root=hash_tree_root(block.body),
    )

    # Verify proposer is not slashed
    proposer = state.validators[block.proposer_index]
    assert not proposer.slashed
```

A straightforward set of validity conditions for the [block header](/part3/containers/dependencies/#beaconblockheader) data.

The version of the block header object that this routine stores in the state is a duplicate of the incoming block's header, but with its `state_root` set to its default empty `Bytes32()` value. See [`process_slot()`](/part3/transition/#def_process_slot) for the explanation of this.

|||
|-|------|
| Used&nbsp;by | [`process_block()`](#def_process_block) |
| Uses | [`get_beacon_proposer_index()`](/part3/helper/accessors/#def_get_beacon_proposer_index), [`hash_tree_root()`](/part3/helper/crypto/#hash_tree_root) |
| See&nbsp;also | [BeaconBlockHeader](/part3/containers/dependencies/#beaconblockheader), [`process_slot()`](/part3/transition/#def_process_slot) |

#### Withdrawals

##### `get_expected_withdrawals`

<a id="def_get_expected_withdrawals"></a>

```python
def get_expected_withdrawals(state: BeaconState) -> Sequence[Withdrawal]:
    epoch = get_current_epoch(state)
    withdrawal_index = state.next_withdrawal_index
    validator_index = state.next_withdrawal_validator_index
    withdrawals: List[Withdrawal] = []
    bound = min(len(state.validators), MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP)
    for _ in range(bound):
        validator = state.validators[validator_index]
        balance = state.balances[validator_index]
        if is_fully_withdrawable_validator(validator, balance, epoch):
            withdrawals.append(Withdrawal(
                index=withdrawal_index,
                validator_index=validator_index,
                address=ExecutionAddress(validator.withdrawal_credentials[12:]),
                amount=balance,
            ))
            withdrawal_index += WithdrawalIndex(1)
        elif is_partially_withdrawable_validator(validator, balance):
            withdrawals.append(Withdrawal(
                index=withdrawal_index,
                validator_index=validator_index,
                address=ExecutionAddress(validator.withdrawal_credentials[12:]),
                amount=balance - MAX_EFFECTIVE_BALANCE,
            ))
            withdrawal_index += WithdrawalIndex(1)
        if len(withdrawals) == MAX_WITHDRAWALS_PER_PAYLOAD:
            break
        validator_index = ValidatorIndex((validator_index + 1) % len(state.validators))
    return withdrawals
```

This is used in both block processing and block building to construct the list of automatic validator withdrawals that we expect to see in the block.

At most [`MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP`](/part3/config/preset/#max_validators_per_withdrawals_sweep) validators will be considered for a withdrawal. As described under that heading, this serves to bound the load on nodes when eligible validators are few and far between.

Picking up where the previous sweep left off (`state.next_withdrawal_validator_index`), we consider validators in turn, in increasing order of their validator indices. If a validator is [eligible for a full withdrawal](/part3/helper/predicates/#def_is_fully_withdrawable_validator) then a withdrawal transaction for its entire balance is added to the list. If a validator is [eligible for a partial_withdrawal](/part3/helper/predicates/#def_is_partially_withdrawable_validator) then a withdrawal transaction for its excess balance above [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) is added to the list.

Each [withdrawal transaction](/part3/containers/dependencies/#withdrawal) is associated with a unique, consecutive [withdrawal index](/part3/config/types/#withdrawalindex), which is the total number of previous withdrawals.

Once either [`MAX_WITHDRAWALS_PER_PAYLOAD`](/part3/config/preset/#max_withdrawals_per_payload) transactions have been assembled, or [`MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP`](/part3/config/preset/#max_validators_per_withdrawals_sweep) have been considered, the sweep terminates and returns the list of transactions.

The `next_withdrawal_index` and `next_withdrawal_validator_index` counters in the beacon state are not updated here, but in the calling function.

|||
|-|------|
| Used&nbsp;by | [`process_withdrawals()`](#def_process_withdrawals) |
| Uses | [`is_fully_withdrawable_validator()`](/part3/helper/predicates/#def_is_fully_withdrawable_validator), [`is_partially_withdrawable_validator()`](/part3/helper/predicates/#def_is_partially_withdrawable_validator) |
| See&nbsp;also | [`MAX_WITHDRAWALS_PER_PAYLOAD`](/part3/config/preset/#max_withdrawals_per_payload), [`MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP`](/part3/config/preset/#max_validators_per_withdrawals_sweep), [Withdrawal](/part3/containers/dependencies/#withdrawal) |

##### `process_withdrawals`

<a id="def_process_withdrawals"></a>

```python
def process_withdrawals(state: BeaconState, payload: ExecutionPayload) -> None:
    expected_withdrawals = get_expected_withdrawals(state)
    assert len(payload.withdrawals) == len(expected_withdrawals)

    for expected_withdrawal, withdrawal in zip(expected_withdrawals, payload.withdrawals):
        assert withdrawal == expected_withdrawal
        decrease_balance(state, withdrawal.validator_index, withdrawal.amount)

    # Update the next withdrawal index if this block contained withdrawals
    if len(expected_withdrawals) != 0:
        latest_withdrawal = expected_withdrawals[-1]
        state.next_withdrawal_index = WithdrawalIndex(latest_withdrawal.index + 1)

    # Update the next validator index to start the next withdrawal sweep
    if len(expected_withdrawals) == MAX_WITHDRAWALS_PER_PAYLOAD:
        # Next sweep starts after the latest withdrawal's validator index
        next_validator_index = ValidatorIndex((expected_withdrawals[-1].validator_index + 1) % len(state.validators))
        state.next_withdrawal_validator_index = next_validator_index
    else:
        # Advance sweep by the max length of the sweep if there was not a full set of withdrawals
        next_index = state.next_withdrawal_validator_index + MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP
        next_validator_index = ValidatorIndex(next_index % len(state.validators))
        state.next_withdrawal_validator_index = next_validator_index
```

The withdrawal transactions in a block appear in its [`ExecutionPayload`](/part3/containers/execution/#executionpayload) since they span both the consensus and execution layers. When processing the withdrawals, we first check that they match what we expect to see. This is taken care of by the call to [`get_expected_withdrawals()`](#def_get_expected_withdrawals) and the pairwise comparison within the `for` loop[^fn-withdrawals-zip]. If any of the `assert` tests fails then the entire block is invalid and all changes, including balance updates already made, must be rolled back. For each withdrawal, the corresponding validator's balance is decreased; the execution client will add the same amount to the validator's Eth1 withdrawal address on the execution layer.

[^fn-withdrawals-zip]: The use of [`zip()`](https://docs.python.org/3/library/functions.html#zip) here is quite Pythonic, but just means that with two lists of equal length we take their elements pairwise in turn.

After that we have some trickery for updating the values of `next_withdrawal_index` and `next_withdrawal_validator_index` in the beacon state.

For `next_withdrawal_index`, which just counts the number of withdrawals every made, we take the index of the last withdrawal in our list and add one. Adding the length of the list to our current value would be equivalent.

For `next_withdrawal_validator_index`, we have two cases. If we have a full list of [`MAX_WITHDRAWALS_PER_PAYLOAD`](/part3/config/preset/#max_withdrawals_per_payload) withdrawal transactions then we know that this is the condition that terminated the sweep. Therefore the first validator we need to consider next time is the one after the validator in the last withdrawal transaction. Otherwise, the sweep was terminated by reaching [`MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP`](/part3/config/preset/#max_validators_per_withdrawals_sweep), and the first validator we need to consider next time is the one after that.

I can't help thinking that it would have been easier to return these both from [`get_expected_withdrawals()`](#def_get_expected_withdrawals), where they have just been calculated independently.

|||
|-|------|
| Used&nbsp;by | [`process_block()`](#def_process_block) |
| Uses | [`get_expected_withdrawals()`](#def_get_expected_withdrawals), [`decrease_balance()`](/part3/helper/mutators/#def_decrease_balance) |
| See&nbsp;also | [WithdrawalIndex](/part3/config/types/#withdrawalindex), [ValidatorIndex](/part3/config/types/#validatorindex), [`MAX_WITHDRAWALS_PER_PAYLOAD`](/part3/config/preset/#max_withdrawals_per_payload), [`MAX_VALIDATORS_PER_WITHDRAWALS_SWEEP`](/part3/config/preset/#max_validators_per_withdrawals_sweep) |

#### Execution payload

##### `process_execution_payload`

<a id="def_process_execution_payload"></a>

```python
def process_execution_payload(state: BeaconState, payload: ExecutionPayload, execution_engine: ExecutionEngine) -> None:
    # Verify consistency of the parent hash with respect to the previous execution payload header
    if is_merge_transition_complete(state):
        assert payload.parent_hash == state.latest_execution_payload_header.block_hash
    # Verify prev_randao
    assert payload.prev_randao == get_randao_mix(state, get_current_epoch(state))
    # Verify timestamp
    assert payload.timestamp == compute_timestamp_at_slot(state, state.slot)
    # Verify the execution payload is valid
    assert execution_engine.notify_new_payload(payload)
    # Cache execution payload header
    state.latest_execution_payload_header = ExecutionPayloadHeader(
        parent_hash=payload.parent_hash,
        fee_recipient=payload.fee_recipient,
        state_root=payload.state_root,
        receipts_root=payload.receipts_root,
        logs_bloom=payload.logs_bloom,
        prev_randao=payload.prev_randao,
        block_number=payload.block_number,
        gas_limit=payload.gas_limit,
        gas_used=payload.gas_used,
        timestamp=payload.timestamp,
        extra_data=payload.extra_data,
        base_fee_per_gas=payload.base_fee_per_gas,
        block_hash=payload.block_hash,
        transactions_root=hash_tree_root(payload.transactions),
        withdrawals_root=hash_tree_root(payload.withdrawals),  # [New in Capella]
    )
```

Since the Merge, the execution payload (formerly an Eth1 block) now forms part of the beacon block.

There isn't much beacon chain processing to be done for execution payloads as they are for the most part opaque blobs of data that are meaningful only to the execution client. However, the beacon chain does need to know whether the execution payload is valid in the view of the execution client. An execution payload that is invalid by the rules of the execution (Eth1) chain makes the beacon block containing it invalid.

Some initial sanity checks are performed:

  - Unless this is the very first execution payload that we have seen then its `parent_hash` must match the `block_hash` that we have in the beacon state, that of the last execution payload we processed. This ensures that the chain of execution payloads is continuous, since it is essentially a blockchain within a blockchain.
  - We check that the `prev_randao` value is correctly set, otherwise a block proposer could trivially control the randomness on the execution layer.
  - The timestamp on the execution payload must match the slot timestamp. Again, this prevents proposers manipulating the execution layer time for any smart contracts that depend on it.

Next we send the payload over to the execution engine via the Engine&nbsp;API, using the [`notify_new_payload()`](/part3/transition/execution/#def_notify_new_payload) function it provides. This serves two purposes: first it requests that the execution client check the validity of the payload, and second, if the payload is valid, it allows the execution layer to update its own state by running the transactions contained in the payload.

Finally, the header of the execution payload is stored in the [beacon state](/part3/containers/state/#beaconstate), primarily so that the `block_hash`&ndash;`parent_hash` check can be made next time this function is called. The remainder of the execution header data is not currently used in the beacon chain specification, despite being stored.

This function was added in the Bellatrix pre-Merge upgrade.

|||
|-|------|
| Used&nbsp;by | [`process_block()`](#def_process_block) |
| Uses | [`is_merge_transition_complete()`](/part3/helper/predicates/#def_is_merge_transition_complete), [`get_randao_mix()`](/part3/helper/accessors/#def_get_randao_mix), [`compute_timestamp_at_slot()`](/part3/helper/misc/#def_compute_timestamp_at_slot), [`notify_new_payload()`](/part3/transition/execution/#def_notify_new_payload), [`hash_tree_root()`](/part3/helper/crypto/#hash_tree_root) |
| See&nbsp;also | [`ExecutionPayloadHeader`](/part3/containers/execution/#executionpayloadheader) |

#### RANDAO

<a id="def_process_randao"></a>

```python
def process_randao(state: BeaconState, body: BeaconBlockBody) -> None:
    epoch = get_current_epoch(state)
    # Verify RANDAO reveal
    proposer = state.validators[get_beacon_proposer_index(state)]
    signing_root = compute_signing_root(epoch, get_domain(state, DOMAIN_RANDAO))
    assert bls.Verify(proposer.pubkey, signing_root, body.randao_reveal)
    # Mix in RANDAO reveal
    mix = xor(get_randao_mix(state, epoch), hash(body.randao_reveal))
    state.randao_mixes[epoch % EPOCHS_PER_HISTORICAL_VECTOR] = mix
```

A good source of randomness is foundational to the operation of the beacon chain. Security of the protocol depends significantly on being able to unpredictably and uniformly select block proposers and committee members. In fact, the very name "beacon chain" was inspired by Dfinity's concept of a [randomness beacon](https://arxiv.org/abs/1805.04548).

The current mechanism for providing randomness is a RANDAO, in which each block proposer provides some randomness and all the contributions are mixed together over the course of an epoch. This is not unbiasable (a malicious proposer may choose to skip a block if it is to its advantage to do so), but is [good enough](https://ethresear.ch/t/rng-exploitability-analysis-assuming-pure-randao-based-main-chain/1825?u=benjaminion). In future, Ethereum might use a verifiable delay function ([VDF](https://www.vdfalliance.org/)) to provide unbiasable randomness.

[Early designs](https://github.com/ethereum/consensus-specs/pull/33/files) had the validators pre-committing to "hash onions", peeling off one layer of hashing at each block proposal. This [was changed](https://github.com/ethereum/consensus-specs/pull/483) to using a BLS signature over the [epoch number](https://github.com/ethereum/consensus-specs/pull/498) as the entropy source. Using signatures is both a simplification, and an enabler for multi-party (distributed) validators. The (reasonable) assumption is that sufficient numbers of validators generated their secret keys with good entropy to ensure that the RANDAO's entropy is adequate.

[TODO: link to DVT ^^^]::

The `process_randao()` function simply uses the proposer's public key to verify that the RANDAO reveal in the block is indeed the epoch number signed with the proposer's private key. It then mixes the hash of the reveal into the current epoch's RANDAO accumulator. The hash is used in order to reduce the signature down from 96 to 32 bytes, and to make it uniform. [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector) past values of the RANDAO accumulator at the ends of epochs are stored in the state.

From Justin Drake's [notes](https://notes.ethereum.org/@JustinDrake/rkPjB1_xr):
> Using `xor` in `process_randao` is (slightly) more secure than using `hash`. To illustrate why, imagine an attacker can grind randomness in the current epoch such that two of his validators are the last proposers, in a different order, in two resulting samplings of the next epochs. The commutativity of `xor` makes those two samplings equivalent, hence reducing the attacker's grinding opportunity for the next epoch versus `hash` (which is not commutative). The strict security improvement may simplify the derivation of RANDAO security formal lower bounds.

Note that the `assert` statement means that the whole block is invalid if the RANDAO reveal is incorrectly formed.

|||
|-|------|
| Used&nbsp;by | [`process_block()`](#def_process_block) |
| Uses | [`get_beacon_proposer_index()`](/part3/helper/accessors/#def_get_beacon_proposer_index), [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root), [`get_domain()`](/part3/helper/accessors/#def_get_domain), [`bls.Verify()`](/part3/helper/crypto/#bls-signatures), [`hash()`](/part3/helper/crypto/#hash), [`xor()`](/part3/helper/math/#def_xor), [`get_randao_mix()`](/part3/helper/accessors/#def_get_randao_mix) |
| See&nbsp;also | [`EPOCHS_PER_HISTORICAL_VECTOR`](/part3/config/preset/#epochs_per_historical_vector) |

#### Eth1 data

<a id="def_process_eth1_data"></a>

```python
def process_eth1_data(state: BeaconState, body: BeaconBlockBody) -> None:
    state.eth1_data_votes.append(body.eth1_data)
    if state.eth1_data_votes.count(body.eth1_data) * 2 > EPOCHS_PER_ETH1_VOTING_PERIOD * SLOTS_PER_EPOCH:
        state.eth1_data = body.eth1_data
```

Blocks may contain [`Eth1Data`](/part3/containers/dependencies/#eth1data) which is supposed to be the proposer's best view of the Eth1 chain and the deposit contract at the time. There is no incentive to get this data correct, or penalty for it being incorrect.

If there is a simple majority of the same vote being cast by proposers during each voting period of [`EPOCHS_PER_ETH1_VOTING_PERIOD`](/part3/config/preset/#epochs_per_eth1_voting_period) epochs (6.8 hours) then the Eth1 data is committed to the beacon state. This updates the chain's view of the deposit contract, and new deposits since the last update will start being processed.

This mechanism has proved to [be fragile](https://github.com/ethereum/consensus-specs/issues/2018) in the past, but appears to be workable if not perfect.

|||
|-|------|
| Used&nbsp;by | [`process_block()`](#def_process_block) |
| See&nbsp;also | [`Eth1Data`](/part3/containers/dependencies/#eth1data), [`EPOCHS_PER_ETH1_VOTING_PERIOD`](/part3/config/preset/#epochs_per_eth1_voting_period) |

#### Operations

<a id="def_process_operations"></a>

```python
def process_operations(state: BeaconState, body: BeaconBlockBody) -> None:
    # Verify that outstanding deposits are processed up to the maximum number of deposits
    assert len(body.deposits) == min(MAX_DEPOSITS, state.eth1_data.deposit_count - state.eth1_deposit_index)

    def for_ops(operations: Sequence[Any], fn: Callable[[BeaconState, Any], None]) -> None:
        for operation in operations:
            fn(state, operation)

    for_ops(body.proposer_slashings, process_proposer_slashing)
    for_ops(body.attester_slashings, process_attester_slashing)
    for_ops(body.attestations, process_attestation)
    for_ops(body.deposits, process_deposit)
    for_ops(body.voluntary_exits, process_voluntary_exit)
    for_ops(body.bls_to_execution_changes, process_bls_to_execution_change)  # [New in Capella]
```

Just a dispatcher for handling the various optional contents in a block.

Deposits are optional only in the sense that some blocks have them and some don't. However, as per the `assert` statement, if, according to the beacon chain's view of the Eth1 chain, there are deposits pending, then the block _must_ include them, otherwise the block is invalid.

Regarding incentives for block proposers to include each of these elements:

  - Proposers are explicitly rewarded for including any available attestations and slashing reports.
  - There is a validity condition, and thus an implicit reward, related to including deposit messages.
  - The incentive for including voluntary exits is that a smaller validator set means higher rewards for the remaining validators.
  - There is no incentive, implicit or explicit, for including BLS withdrawal credential change messages. These are handled on a purely altruistic basis.

|||
|-|------|
| Used&nbsp;by | [`process_block()`](#def_process_block) |
| Uses | [`process_proposer_slashing()`](#def_process_proposer_slashing), [`process_attester_slashing()`](#def_process_attester_slashing), [`process_attestation()`](#def_process_attestation), [`process_deposit()`](#def_process_deposit), [`process_voluntary_exit()`](#def_process_voluntary_exit), [`process_bls_to_execution_change()`](#def_process_bls_to_execution_change) |
| See&nbsp;also | [`BeaconBlockBody`](/part3/containers/blocks/#beaconblockbody) |

##### Proposer slashings

<a id="def_process_proposer_slashing"></a>

```python
def process_proposer_slashing(state: BeaconState, proposer_slashing: ProposerSlashing) -> None:
    header_1 = proposer_slashing.signed_header_1.message
    header_2 = proposer_slashing.signed_header_2.message

    # Verify header slots match
    assert header_1.slot == header_2.slot
    # Verify header proposer indices match
    assert header_1.proposer_index == header_2.proposer_index
    # Verify the headers are different
    assert header_1 != header_2
    # Verify the proposer is slashable
    proposer = state.validators[header_1.proposer_index]
    assert is_slashable_validator(proposer, get_current_epoch(state))
    # Verify signatures
    for signed_header in (proposer_slashing.signed_header_1, proposer_slashing.signed_header_2):
        domain = get_domain(state, DOMAIN_BEACON_PROPOSER, compute_epoch_at_slot(signed_header.message.slot))
        signing_root = compute_signing_root(signed_header.message, domain)
        assert bls.Verify(proposer.pubkey, signing_root, signed_header.signature)

    slash_validator(state, header_1.proposer_index)
```

A [`ProposerSlashing`](/part3/containers/operations/#proposerslashing) is a proof that a proposer has signed two blocks at the same height. Up to [`MAX_PROPOSER_SLASHINGS`](/part3/config/preset/#max_proposer_slashings) of them may be included in a block. It contains the evidence in the form of a pair of [`SignedBeaconBlockHeader`](/part3/containers/envelopes/#signedbeaconblockheader)s.

The proof is simple: the two proposals come from the same slot, have the same proposer, but differ in one or more of `parent_root`, `state_root`, or `body_root`. In addition, they were both signed by the proposer. The conflicting blocks do not need to be valid: any pair of headers that meet the criteria, irrespective of the blocks' contents, are liable to be slashed.

As ever, the `assert` statements ensure that the containing block is invalid if it contains any invalid slashing claims.

Fun fact: the [first slashing](https://beaconcha.in/slot/138731#proposer-slashings) to occur on the beacon chain was a proposer slashing. Two clients running side-by-side with the same keys will often produce the same attestations since the protocol is designed to encourage that. Independently producing the same block is very unlikely as blocks contain much more data.

|||
|-|------|
| Used&nbsp;by | [`process_block()`](#def_process_block) |
| Uses | [`is_slashable_validator()`](/part3/helper/predicates/#def_is_slashable_validator), [`get_domain()`](/part3/helper/accessors/#def_get_domain), [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root), [`bls.Verify()`](/part3/helper/crypto/#bls-signatures), [`slash_validator()`](/part3/helper/mutators/#def_slash_validator) |
| See&nbsp;also | [`ProposerSlashing`](/part3/containers/operations/#proposerslashing) |

##### Attester slashings

<a id="def_process_attester_slashing"></a>

```python
def process_attester_slashing(state: BeaconState, attester_slashing: AttesterSlashing) -> None:
    attestation_1 = attester_slashing.attestation_1
    attestation_2 = attester_slashing.attestation_2
    assert is_slashable_attestation_data(attestation_1.data, attestation_2.data)
    assert is_valid_indexed_attestation(state, attestation_1)
    assert is_valid_indexed_attestation(state, attestation_2)

    slashed_any = False
    indices = set(attestation_1.attesting_indices).intersection(attestation_2.attesting_indices)
    for index in sorted(indices):
        if is_slashable_validator(state.validators[index], get_current_epoch(state)):
            slash_validator(state, index)
            slashed_any = True
    assert slashed_any
```

[`AttesterSlashing`](/part3/containers/operations/#attesterslashing)s are similar to proposer slashings in that they just provide the evidence of the two aggregate [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation)s that conflict with each other. Up to [`MAX_ATTESTER_SLASHINGS`](/part3/config/preset/#max_attester_slashings) of them may be included in a block.

The validity checking is done by [`is_slashable_attestation_data()`](/part3/helper/predicates/#is_slashable_attestation_data), which checks the double vote and surround vote conditions, and by [`is_valid_indexed_attestation()`](/part3/helper/predicates/#def_is_valid_indexed_attestation) which verifies the signatures on the attestations.

Any validators that appear in both attestations are slashed. If no validator is slashed, then the attester slashing claim was not valid after all, and therefore its containing block is invalid.

Examples: a [double vote](https://beaconcha.in/slot/43920#attester-slashings) attester slashing; [surround vote](https://beaconcha.in/slot/17184#attester-slashings) attester slashings.

|||
|-|------|
| Used&nbsp;by | [`process_block()`](#def_process_block) |
| Uses | [`is_slashable_attestation_data()`](/part3/helper/predicates/#is_slashable_attestation_data), [`is_valid_indexed_attestation()`](/part3/helper/predicates/#def_is_valid_indexed_attestation), [`is_slashable_validator()`](/part3/helper/predicates/#def_is_slashable_validator), [`slash_validator()`](/part3/helper/mutators/#def_slash_validator) |
| See&nbsp;also | [`AttesterSlashing`](/part3/containers/operations/#attesterslashing) |

##### Attestations

<a id="def_process_attestation"></a>

```python
def process_attestation(state: BeaconState, attestation: Attestation) -> None:
    data = attestation.data
    assert data.target.epoch in (get_previous_epoch(state), get_current_epoch(state))
    assert data.target.epoch == compute_epoch_at_slot(data.slot)
    assert data.slot + MIN_ATTESTATION_INCLUSION_DELAY <= state.slot <= data.slot + SLOTS_PER_EPOCH
    assert data.index < get_committee_count_per_slot(state, data.target.epoch)

    committee = get_beacon_committee(state, data.slot, data.index)
    assert len(attestation.aggregation_bits) == len(committee)

    # Participation flag indices
    participation_flag_indices = get_attestation_participation_flag_indices(state, data, state.slot - data.slot)

    # Verify signature
    assert is_valid_indexed_attestation(state, get_indexed_attestation(state, attestation))

    # Update epoch participation flags
    if data.target.epoch == get_current_epoch(state):
        epoch_participation = state.current_epoch_participation
    else:
        epoch_participation = state.previous_epoch_participation

    proposer_reward_numerator = 0
    for index in get_attesting_indices(state, data, attestation.aggregation_bits):
        for flag_index, weight in enumerate(PARTICIPATION_FLAG_WEIGHTS):
            if flag_index in participation_flag_indices and not has_flag(epoch_participation[index], flag_index):
                epoch_participation[index] = add_flag(epoch_participation[index], flag_index)
                proposer_reward_numerator += get_base_reward(state, index) * weight

    # Reward proposer
    proposer_reward_denominator = (WEIGHT_DENOMINATOR - PROPOSER_WEIGHT) * WEIGHT_DENOMINATOR // PROPOSER_WEIGHT
    proposer_reward = Gwei(proposer_reward_numerator // proposer_reward_denominator)
    increase_balance(state, get_beacon_proposer_index(state), proposer_reward)
```

Block proposers are rewarded here for including attestations during block processing, while attesting validators receive their rewards and penalties during [epoch processing](/part3/transition/epoch/#process-rewards-and-penalties).

This routine processes each attestation included in the block. First a bunch of validity checks are performed. If any of these fails, then the whole block is invalid (it is most likely from a proposer on a different fork, and so useless to us):

  - The target vote of the attestation must be either the previous epoch's checkpoint or the current epoch's checkpoint.
  - The target checkpoint and the attestation's slot must belong to the same epoch.
  - The attestation must be no newer than [`MIN_ATTESTATION_INCLUSION_DELAY`](/part3/config/preset/#min_attestation_inclusion_delay) slots, which is one. So this condition rules out attestations from the current or future slots.
  - The attestation must be no older than [`SLOTS_PER_EPOCH`](/part3/config/preset/#slots_per_epoch) slots, which is 32.[^fn-eip7045]
  - The attestation must come from a committee that existed when the attestation was created.
  - The size of the committee and the size of the aggregate must match (`aggregation_bits`).
  - The (aggregate) signature on the attestation must be valid and must correspond to the aggregated public keys of the validators that it claims to be signed by. This (and other criteria) is checked by [`is_valid_indexed_attestation()`](/part3/helper/predicates/#def_is_valid_indexed_attestation).

[^fn-eip7045]: This is due to change in [EIP-7045](https://eips.ethereum.org/EIPS/eip-7045), scheduled for inclusion in the [Deneb upgrade](/part4/history/deneb/). The change will allow attestations to be included from the whole of current and previous epochs.

Once the attestation has passed the checks it is processed by converting the votes from validators that it contains into flags in the state.

It's easy to skip over amidst all the checking, but the actual attestation processing is done by [`get_attestation_participation_flag_indices()`](/part3/helper/accessors/#def_get_attestation_participation_flag_indices). This takes the source, target, and head votes of the attestation, along with its inclusion delay (how many slots late was it included in a block) and returns a list of up to [three flags](/part3/config/constants/#participation-flag-indices) corresponding to the votes that were both correct and timely, in `participation_flag_indices`.

For each validator that signed the attestation, if each flag in `participation_flag_indices` is not already set for it in its `epoch_participation` record, then the flag is set, and the proposer is rewarded. Recall that the validator making the attestation is not rewarded until the end of the epoch. If the flag is already set in the corresponding epoch for a validator, no proposer reward is accumulated: the attestation for this validator was included in an earlier block.

The proposer reward is accumulated, and weighted according to the [weight](/part3/config/constants/#participation_flag_weights) assigned to each of the flags (timely source, timely target, timely head).

If a proposer includes all the attestations only for one slot, and all the relevant validators vote, then its reward will be, in the [notation](/part3/transition/epoch/#reward-and-penalty-calculations) established earlier,

$$
I_{A_P} = \frac{W_p}{32(W_{\Sigma} - W_p)}I_A
$$

Where $I_A$ is the total maximum reward per epoch for attesters, calculated in [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas). The total available reward in an epoch for proposers including attestations is 32 times this.

|||
|-|------|
| Used&nbsp;by | [`process_operations()`](#def_process_operations) |
| Uses | [`get_committee_count_per_slot()`](/part3/helper/accessors/#def_get_committee_count_per_slot), [`get_beacon_committee()`](/part3/helper/accessors/#def_get_beacon_committee), [`get_attestation_participation_flag_indices()`](/part3/helper/accessors/#def_get_attestation_participation_flag_indices), [`is_valid_indexed_attestation()`](/part3/helper/predicates/#def_is_valid_indexed_attestation), [`get_indexed_attestation()`](/part3/helper/accessors/#def_get_indexed_attestation), [`get_attesting_indices()`](/part3/helper/accessors/#def_get_attesting_indices), [`has_flag()`](/part3/helper/participation/#def_has_flag), [`add_flag()`](/part3/helper/participation/#def_add_flag), [`get_base_reward()`](/part3/transition/epoch/#def_get_base_reward), [`increase_balance()`](/part3/helper/mutators/#def_increase_balance) |
| See&nbsp;also | [Participation flag indices](/part3/config/constants/#participation-flag-indices), [`PARTICIPATION_FLAG_WEIGHTS`](/part3/config/constants/#participation_flag_weights), [`get_flag_index_deltas()`](/part3/helper/accessors/#def_get_flag_index_deltas) |

##### Deposits

The code in this section handles deposit transactions that were included in a block. A deposit is created when a user transfers one or more ETH to the [deposit contract](/part2/deposits-withdrawals/contract/). We need to check that the data sent with the deposit is valid. If it is, we either create a new validator record (for the first deposit for a validator) or update an existing record.

<a id="def_get_validator_from_deposit"></a>

```python
def get_validator_from_deposit(pubkey: BLSPubkey, withdrawal_credentials: Bytes32, amount: uint64) -> Validator:
    effective_balance = min(amount - amount % EFFECTIVE_BALANCE_INCREMENT, MAX_EFFECTIVE_BALANCE)

    return Validator(
        pubkey=pubkey,
        withdrawal_credentials=withdrawal_credentials,
        activation_eligibility_epoch=FAR_FUTURE_EPOCH,
        activation_epoch=FAR_FUTURE_EPOCH,
        exit_epoch=FAR_FUTURE_EPOCH,
        withdrawable_epoch=FAR_FUTURE_EPOCH,
        effective_balance=effective_balance,
    )
```

Create a newly initialised validator object based on deposit data. This was [factored out](https://github.com/ethereum/consensus-specs/commit/1623086088e6f0496566ab7d50d16a8c78cdebf0) of `process_deposit()` for better code reuse between the Phase&nbsp;0 spec and the (now deprecated) sharding spec.

The `pubkey` is supplied in the initial deposit transaction. The depositor generates the validator's public key from the its private key.

|||
|-|------|
| Used&nbsp;by | [`apply_deposit()`](#def_apply_deposit) |
| See&nbsp;also | [`Validator`](/part3/containers/dependencies/#validator), [`FAR_FUTURE_EPOCH`](/part3/config/constants/#far_future_epoch), [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment), [`MAX_EFFECTIVE_BALANCE`](/part3/config/preset/#max_effective_balance) |

<a id="def_apply_deposit"></a>

```python
def apply_deposit(state: BeaconState,
                  pubkey: BLSPubkey,
                  withdrawal_credentials: Bytes32,
                  amount: uint64,
                  signature: BLSSignature) -> None:
    validator_pubkeys = [validator.pubkey for validator in state.validators]
    if pubkey not in validator_pubkeys:
        # Verify the deposit signature (proof of possession) which is not checked by the deposit contract
        deposit_message = DepositMessage(
            pubkey=pubkey,
            withdrawal_credentials=withdrawal_credentials,
            amount=amount,
        )
        domain = compute_domain(DOMAIN_DEPOSIT)  # Fork-agnostic domain since deposits are valid across forks
        signing_root = compute_signing_root(deposit_message, domain)
        # Initialize validator if the deposit signature is valid
        if bls.Verify(pubkey, signing_root, signature):
            state.validators.append(get_validator_from_deposit(pubkey, withdrawal_credentials, amount))
            state.balances.append(amount)
            # [New in Altair]
            state.previous_epoch_participation.append(ParticipationFlags(0b0000_0000))
            state.current_epoch_participation.append(ParticipationFlags(0b0000_0000))
            state.inactivity_scores.append(uint64(0))
    else:
        # Increase balance by deposit amount
        index = ValidatorIndex(validator_pubkeys.index(pubkey))
        increase_balance(state, index, amount)
```

The `apply_deposit()` function was [factored out](https://github.com/ethereum/consensus-specs/pull/3177) of `process_deposit()` in the Capella release for better code reuse between the Phase&nbsp;0 spec and the [EIP-6110 spec](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs/_features/eip6110)[^fn-eip-6110].

[^fn-eip-6110]: [EIP-6110](https://eips.ethereum.org/EIPS/eip-6110) is a potential future upgrade that would allow deposits to be processed more or less instantly, rather than having to go through the [Eth1 follow distance](/part3/config/configuration/#eth1_follow_distance) and [Eth1 voting period](/part3/config/preset/#epochs_per_eth1_voting_period) as they do now.

Deposits are signed with the private key of the depositing validator, and the corresponding public key is included in the deposit data. This constitutes a "proof of possession" of the private key, and prevents nastiness like the [rogue key attack](https://hackmd.io/@benjaminion/bls12-381#Rogue-key-attacks). Note that [`compute_domain()`](/part3/helper/misc/#def_compute_domain) is used directly here when validating the deposit's signature, rather than the more usual [`get_domain()`](/part3/helper/accessors/#def_get_domain) wrapper. This is because deposit messages are valid across beacon chain upgrades (such as Phase&nbsp;0, Altair, and Bellatrix), so we don't want to mix the fork version into the domain. In addition, deposits can be made before `genesis_validators_root` is known.

The `if pubkey not in validator_pubkeys` test distinguishes new deposits from top-up deposits. When the public key associated with the deposit is not present in the existing validator set, a new validator record is created. When the public key is already present, the balance of the existing validator record is topped up. A validator's public key is its unique identity. (Its validator index is also unique for now, but that [might change](https://eips.ethereum.org/EIPS/eip-6914) in future.)

An interesting quirk of this routine is that only the first deposit for a validator needs to be signed. Subsequent deposits for the same public key do not have their signatures checked. This could allow one staker (the key holder) to make an initial deposit (1 ETH, say), and for that to be topped up by others who do not have the private key. I don't know of any practical uses for this feature, but would be glad to hear of any. It slightly reduces the risk for stakers making multiple deposits for the same validator as they don't need to worry about incorrectly signing any but the first deposit.

Similarly, once a validator's withdrawal credentials have been set by the initial deposit transaction, the withdrawal credentials of subsequent deposits for the same validator are ignored. Only the credentials appearing on the initial deposit are stored on the beacon chain. This is an important security measure. If an attacker steals a validator's signing key (which signs deposit transactions), we don't want them to be able to change the withdrawal credentials in order to steal the stake for themselves. However, it works both ways, and [a vulnerability](https://medium.com/immunefi/rocketpool-lido-frontrunning-bug-fix-postmortem-e701f26d7971) was identified for staking pools in which a malicious operator could potentially front-run a deposit transaction with a 1&nbsp;ETH deposit to set the withdrawal credentials to their own.

Note that the `withdrawal_credential` in the deposit data is not checked in any way. It's up to the depositor to ensure that they are using the [correct prefix](/part3/config/constants/#withdrawal-prefixes) and contents to be able to receive their rewards and retrieve their stake back after exiting the consensus layer.

|||
|-|------|
| Used&nbsp;by | [`process_deposit()`](#def_process_deposit) |
| Uses | [`compute_domain()`](/part3/helper/misc/#def_compute_domain), [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root), [`bls.Verify()`](/part3/helper/crypto/#bls-signatures), [`get_validator_from_deposit()`](#def_get_validator_from_deposit) |
| See&nbsp;also | [`DepositMessage`](/part3/containers/dependencies/#depositmessage), [`DOMAIN_DEPOSIT`](/part3/config/constants/#domain_deposit) |

<a id="def_process_deposit"></a>

```python
def process_deposit(state: BeaconState, deposit: Deposit) -> None:
    # Verify the Merkle branch
    assert is_valid_merkle_branch(
        leaf=hash_tree_root(deposit.data),
        branch=deposit.proof,
        depth=DEPOSIT_CONTRACT_TREE_DEPTH + 1,  # Add 1 for the List length mix-in
        index=state.eth1_deposit_index,
        root=state.eth1_data.deposit_root,
    )

    # Deposits must be processed in order
    state.eth1_deposit_index += 1

    apply_deposit(
        state=state,
        pubkey=deposit.data.pubkey,
        withdrawal_credentials=deposit.data.withdrawal_credentials,
        amount=deposit.data.amount,
        signature=deposit.data.signature,
    )
```

Here, we process a deposit from a block. If the deposit is valid, either a new validator is created or the deposit amount is added to an existing validator.

The call to [`is_valid_merkle_branch()`](/part3/helper/predicates/#def_is_valid_merkle_branch) ensures that it is not possible to fake a deposit. The `eth1data.deposit_root` from the deposit contract has been [agreed](/part3/transition/block/#eth1-data) by the beacon chain and includes all pending deposits visible to the beacon chain. The deposit itself contains a Merkle proof that it is included in that root. The `state.eth1_deposit_index` counter ensures that deposits are processed in order. In short, the proposer provides `leaf` and `branch`, but neither `index` nor `root`.

If the Merkle branch check fails, then the whole block is invalid. However, individual deposits can fail the signature check without invalidating the block.

Deposits must be processed in order, and all available deposits must be included in the block (up to [`MAX_DEPOSITS`](/part3/config/preset/#max_deposits) - checked in [`process_operations()`](#def_process_operations)). This ensures that the beacon chain cannot censor deposit transactions, except at the expense of stopping block production entirely.

|||
|-|------|
| Used&nbsp;by | [`process_operations()`](#def_process_operations) |
| Uses | [`is_valid_merkle_branch()`](/part3/helper/predicates/#def_is_valid_merkle_branch), [`hash_tree_root()`](/part3/helper/crypto/#hash_tree_root), [`apply_deposit()`](#def_apply_deposit) |
| See&nbsp;also | [`Deposit`](/part3/containers/operations/#deposit), [`DEPOSIT_CONTRACT_TREE_DEPTH`](/part3/config/constants/#deposit_contract_tree_depth) |

##### Voluntary exits

<a id="def_process_voluntary_exit"></a>

```python
def process_voluntary_exit(state: BeaconState, signed_voluntary_exit: SignedVoluntaryExit) -> None:
    voluntary_exit = signed_voluntary_exit.message
    validator = state.validators[voluntary_exit.validator_index]
    # Verify the validator is active
    assert is_active_validator(validator, get_current_epoch(state))
    # Verify exit has not been initiated
    assert validator.exit_epoch == FAR_FUTURE_EPOCH
    # Exits must specify an epoch when they become valid; they are not valid before then
    assert get_current_epoch(state) >= voluntary_exit.epoch
    # Verify the validator has been active long enough
    assert get_current_epoch(state) >= validator.activation_epoch + SHARD_COMMITTEE_PERIOD
    # Verify signature
    domain = get_domain(state, DOMAIN_VOLUNTARY_EXIT, voluntary_exit.epoch)
    signing_root = compute_signing_root(voluntary_exit, domain)
    assert bls.Verify(validator.pubkey, signing_root, signed_voluntary_exit.signature)
    # Initiate exit
    initiate_validator_exit(state, voluntary_exit.validator_index)
```

A voluntary exit message is submitted by a validator to indicate that it wishes to cease being an active validator. A proposer receives [voluntary exit messages](/part3/containers/operations/#voluntaryexit) via gossip or via its own API and then includes the message in a block so that it can be processed by the network.

Most of the checks are straightforward, as per the comments in the code. Note the following.

  - Voluntary exits are invalid if they are included in blocks before the given `epoch`, so nodes should buffer any future-dated exits they see before putting them in a block.
  - A validator must have been active for at least [`SHARD_COMMITTEE_PERIOD`](/part3/config/configuration/#shard_committee_period) epochs (27 hours). See [there](/part3/config/configuration/#shard_committee_period) for the rationale.
  - Voluntary exits are signed with the validator's usual signing key. There is some discussion about [changing this](https://github.com/ethereum/consensus-specs/issues/1578) to also allow signing of a voluntary exit with the validator's withdrawal key.

If the voluntary exit message is valid then the validator is added to the exit queue by calling [`initiate_validator_exit()`](/part3/helper/mutators/#initiate_validator_exit).

At present, it is [not possible](https://notes.ethereum.org/elDvTNrbRqmgP6np_YWc2g#Concerns-that-motivated-removing-re-activation-functionality-in-2017) for a validator to exit and re-enter, but this functionality [may be introduced](https://hackmd.io/@HWeNw8hNRimMm2m2GH56Cw/HkTzLKOov#Exit-and-re-entry) in future.

|||
|-|------|
| Used&nbsp;by | [`process_operations()`](/part3/transition/block/#def_process_operations) |
| Uses | [`is_active_validator()`](/part3/helper/predicates/#def_is_active_validator), [`get_domain()`](/part3/helper/accessors/#def_get_domain), [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root), [`bls.Verify()`](/part3/helper/crypto/#bls-signatures), [`initiate_validator_exit()`](/part3/helper/mutators/#def_initiate_validator_exit) |
| See&nbsp;also | [`VoluntaryExit`](/part3/containers/operations/#voluntaryexit), [`SHARD_COMMITTEE_PERIOD`](/part3/config/configuration/#shard_committee_period) |

##### `process_bls_to_execution_change`

<a id="def_process_bls_to_execution_change"></a>

```python
def process_bls_to_execution_change(state: BeaconState,
                                    signed_address_change: SignedBLSToExecutionChange) -> None:
    address_change = signed_address_change.message

    assert address_change.validator_index < len(state.validators)

    validator = state.validators[address_change.validator_index]

    assert validator.withdrawal_credentials[:1] == BLS_WITHDRAWAL_PREFIX
    assert validator.withdrawal_credentials[1:] == hash(address_change.from_bls_pubkey)[1:]

    # Fork-agnostic domain since address changes are valid across forks
    domain = compute_domain(DOMAIN_BLS_TO_EXECUTION_CHANGE, genesis_validators_root=state.genesis_validators_root)
    signing_root = compute_signing_root(address_change, domain)
    assert bls.Verify(address_change.from_bls_pubkey, signing_root, signed_address_change.signature)

    validator.withdrawal_credentials = (
        ETH1_ADDRESS_WITHDRAWAL_PREFIX
        + b'\x00' * 11
        + address_change.to_execution_address
    )
```

The [Capella upgrade](/part4/history/capella/) provides a one-time operation to allow stakers to change their withdrawal credentials from BLS type ([`BLS_WITHDRAWAL_PREFIX`](/part3/config/constants/#bls_withdrawal_prefix)), which do not allow withdrawals, to Eth1 style ([`ETH1_ADDRESS_WITHDRAWAL_PREFIX`](/part3/config/constants/#eth1_address_withdrawal_prefix)), which enable automatic withdrawals.

Stakers can make the change by signing a [`BLSToExecutionChange`](/part3/containers/operations/#blstoexecutionchange) message and broadcasting it to the network. At some point a proposer will include the change message in a block and it will arrive at this function in the state transition.

For [BLS credentials](/part3/config/constants/#bls_withdrawal_prefix) the withdrawal credential contains the last 31 bytes of the SHA256 hash of a public key. That public key is the validator's withdrawal key, distinct from its signing key, although often [derived from the same mnemonic](https://eips.ethereum.org/EIPS/eip-2334#validator-keys). By checking its hash, we are confirming that the public key provided in the change message is the same one that created the withdrawal credential in the initial deposit.

Once we are satisfied that the public key is the same on previously committed to, then we can use it to verify the signature on the withdrawal transaction. Again, this transaction must be signed with the validator's withdrawal private key, not its usual signing key.

Having verified the signature, we can finally, and irrevocably, update the validator's withdrawal credentials from BLS style to Eth1 style.

|||
|-|------|
| Used&nbsp;by | [`process_operations()`](/part3/transition/block/#def_process_operations) |
| Uses | [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root), [`compute_domain()`](/part3/helper/misc/#def_compute_domain), [`bls.Verify()`](/part3/helper/crypto/#bls-signatures) |
| See&nbsp;also | [`BLS_WITHDRAWAL_PREFIX`](/part3/config/constants/#bls_withdrawal_prefix), [`BLSToExecutionChange`](/part3/containers/operations/#blstoexecutionchange) |

#### Sync aggregate processing

<a id="def_process_sync_aggregate"></a>

```python
def process_sync_aggregate(state: BeaconState, sync_aggregate: SyncAggregate) -> None:
    # Verify sync committee aggregate signature signing over the previous slot block root
    committee_pubkeys = state.current_sync_committee.pubkeys
    participant_pubkeys = [pubkey for pubkey, bit in zip(committee_pubkeys, sync_aggregate.sync_committee_bits) if bit]
    previous_slot = max(state.slot, Slot(1)) - Slot(1)
    domain = get_domain(state, DOMAIN_SYNC_COMMITTEE, compute_epoch_at_slot(previous_slot))
    signing_root = compute_signing_root(get_block_root_at_slot(state, previous_slot), domain)
    assert eth_fast_aggregate_verify(participant_pubkeys, signing_root, sync_aggregate.sync_committee_signature)

    # Compute participant and proposer rewards
    total_active_increments = get_total_active_balance(state) // EFFECTIVE_BALANCE_INCREMENT
    total_base_rewards = Gwei(get_base_reward_per_increment(state) * total_active_increments)
    max_participant_rewards = Gwei(total_base_rewards * SYNC_REWARD_WEIGHT // WEIGHT_DENOMINATOR // SLOTS_PER_EPOCH)
    participant_reward = Gwei(max_participant_rewards // SYNC_COMMITTEE_SIZE)
    proposer_reward = Gwei(participant_reward * PROPOSER_WEIGHT // (WEIGHT_DENOMINATOR - PROPOSER_WEIGHT))

    # Apply participant and proposer rewards
    all_pubkeys = [v.pubkey for v in state.validators]
    committee_indices = [ValidatorIndex(all_pubkeys.index(pubkey)) for pubkey in state.current_sync_committee.pubkeys]
    for participant_index, participation_bit in zip(committee_indices, sync_aggregate.sync_committee_bits):
        if participation_bit:
            increase_balance(state, participant_index, participant_reward)
            increase_balance(state, get_beacon_proposer_index(state), proposer_reward)
        else:
            decrease_balance(state, participant_index, participant_reward)
```

Similarly to how attestations are handled, the beacon block proposer includes in its block an aggregation of sync committee votes that agree with its local view of the chain. Specifically, the sync committee votes are for the head block that the proposer saw in the previous slot. (If the previous slot is empty, then the head block will be from an earlier slot.)

We validate these votes against our local view of the chain, and if they agree then we reward the participants that voted. If they do not agree with our local view, then the entire block is invalid: it is on another branch.

To perform the validation, we form the signing root of the block at the previous slot, with `DOMAIN_SYNC_COMMITTEE` mixed in. Then we check if the aggregate signature received in the [`SyncAggregate`](/part3/containers/operations/#syncaggregate) verifies against it, using the aggregate public key of the validators who claimed to have signed it. If either the signing root (that is, the head block) is wrong, or the list of participants is wrong, then the verification will fail and the block is invalid.

Like proposer rewards, but unlike attestation rewards, sync committee rewards are not weighted with the participants' effective balances. This is already taken care of by the committee selection process that weights the probability of selection with the effective balance of the validator.

Running through the calculations:

  - `total_active_increments`: the sum of the effective balances of the entire active validator set normalised with the [`EFFECTIVE_BALANCE_INCREMENT`](/part3/config/preset/#effective_balance_increment) to give the total number of increments.
  - `total_base_rewards`: the maximum rewards that will be awarded to all validators for all duties this epoch. It is at most $NB$ in the [notation](/part3/transition/epoch/#reward-and-penalty-calculations) established earlier.
  - `max_participant_rewards`: the amount of the total reward to be given to the entire sync committee in this slot.
  - `participant_reward`: the reward per participating validator, and the penalty per non-participating validator.
  - `proposer_reward`: one seventh of the participant reward.

Each committee member that voted receives a reward of `participant_reward`, and the proposer receives one seventh of this in addition.

Each committee member that failed to vote receives a penalty of `participant_reward`, and the proposer receives nothing.

In our [notation](/part3/transition/epoch/#reward-and-penalty-calculations) the maximum issuance (reward) due to sync committees per slot is as follows.

$$
I_S = \frac{W_y}{32 \cdot W_{\Sigma}}NB
$$

The per-epoch reward is thirty-two times this. The maximum reward for the proposer in respect of sync aggregates:

$$
I_{S_P} = \frac{W_p}{W_{\Sigma} - W_p}I_S
$$

|||
|-|------|
| Used&nbsp;by | [`process_operations()`](/part3/transition/block/#def_process_operations) |
| Uses | [`get_domain()`](/part3/helper/accessors/#def_get_domain), [`compute_signing_root()`](/part3/helper/misc/#def_compute_signing_root), [`eth_fast_aggregate_verify()`](/part3/helper/crypto/#def_eth_fast_aggregate_verify), [`get_total_active_balance()`](/part3/helper/accessors/#def_get_total_active_balance), [`get_base_reward_per_increment()`](/part3/transition/epoch/#def_get_base_reward_per_increment), [`increase_balance()`](/part3/helper/mutators/#def_increase_balance), [`decrease_balance()`](/part3/helper/mutators/#decrease_balance) |
| See&nbsp;also | [Incentivization weights](/part3/config/constants/#incentivization-weights), [`SYNC_COMMITTEE_SIZE`](/part3/config/preset/#sync_committee_size) |

## Initialise State <!-- /part3/initialise/ -->

### Introduction

TODO: rework and synthesis - this text is from the original Genesis.

Before the Ethereum beacon chain genesis has been triggered, and for every Ethereum proof-of-work block, let `candidate_state = initialize_beacon_state_from_eth1(eth1_block_hash, eth1_timestamp, deposits)` where:

  - `eth1_block_hash` is the hash of the Ethereum proof-of-work block
  - `eth1_timestamp` is the Unix timestamp corresponding to `eth1_block_hash`
  - `deposits` is the sequence of all deposits, ordered chronologically, up to (and including) the block with hash `eth1_block_hash`

Proof of work blocks must only be considered once they are at least `SECONDS_PER_ETH1_BLOCK` ` * ` `ETH1_FOLLOW_DISTANCE` seconds old (i.e. `eth1_timestamp` ` + ` `SECONDS_PER_ETH1_BLOCK` ` * ` `ETH1_FOLLOW_DISTANCE` ` <= ` `current_unix_time`). Due to this constraint, if `GENESIS_DELAY` ` < ` `SECONDS_PER_ETH1_BLOCK` ` * ` `ETH1_FOLLOW_DISTANCE`, then the `genesis_time` can happen before the time/state is first known. Values should be configured to avoid this case.

### Initialisation

Aka genesis.

This helper function is only for initializing the state for pure Capella testnets and tests.

<a id="def_initialize_beacon_state_from_eth1"></a>

```python
def initialize_beacon_state_from_eth1(eth1_block_hash: Hash32,
                                      eth1_timestamp: uint64,
                                      deposits: Sequence[Deposit],
                                      execution_payload_header: ExecutionPayloadHeader=ExecutionPayloadHeader()
                                      ) -> BeaconState:
    fork = Fork(
        previous_version=CAPELLA_FORK_VERSION,  # [Modified in Capella] for testing only
        current_version=CAPELLA_FORK_VERSION,  # [Modified in Capella]
        epoch=GENESIS_EPOCH,
    )
    state = BeaconState(
        genesis_time=eth1_timestamp + GENESIS_DELAY,
        fork=fork,
        eth1_data=Eth1Data(block_hash=eth1_block_hash, deposit_count=uint64(len(deposits))),
        latest_block_header=BeaconBlockHeader(body_root=hash_tree_root(BeaconBlockBody())),
        randao_mixes=[eth1_block_hash] * EPOCHS_PER_HISTORICAL_VECTOR,  # Seed RANDAO with Eth1 entropy
    )

    # Process deposits
    leaves = list(map(lambda deposit: deposit.data, deposits))
    for index, deposit in enumerate(deposits):
        deposit_data_list = List[DepositData, 2**DEPOSIT_CONTRACT_TREE_DEPTH](*leaves[:index + 1])
        state.eth1_data.deposit_root = hash_tree_root(deposit_data_list)
        process_deposit(state, deposit)

    # Process activations
    for index, validator in enumerate(state.validators):
        balance = state.balances[index]
        validator.effective_balance = min(balance - balance % EFFECTIVE_BALANCE_INCREMENT, MAX_EFFECTIVE_BALANCE)
        if validator.effective_balance == MAX_EFFECTIVE_BALANCE:
            validator.activation_eligibility_epoch = GENESIS_EPOCH
            validator.activation_epoch = GENESIS_EPOCH

    # Set genesis validators root for domain separation and chain versioning
    state.genesis_validators_root = hash_tree_root(state.validators)

    # Fill in sync committees
    # Note: A duplicate committee is assigned for the current and next committee at genesis
    state.current_sync_committee = get_next_sync_committee(state)
    state.next_sync_committee = get_next_sync_committee(state)

    # Initialize the execution payload header
    state.latest_execution_payload_header = execution_payload_header

    return state
```

Each state fields start with its [default SSZ value](/part2/building_blocks/ssz/#default-values) unless a value is explicitly provided. So, for example, `state.next_withdrawal_index` will be initialised to zero, and `state.historical_summaries` to an empty list.

### Genesis state

Let `genesis_state = candidate_state` whenever `is_valid_genesis_state(candidate_state) is True` for the first time.

<a id="def_is_valid_genesis_state"></a>

```python
def is_valid_genesis_state(state: BeaconState) -> bool:
    if state.genesis_time < MIN_GENESIS_TIME:
        return False
    if len(get_active_validator_indices(state, GENESIS_EPOCH)) < MIN_GENESIS_ACTIVE_VALIDATOR_COUNT:
        return False
    return True
```

TODO

### Genesis block

Let `genesis_block = BeaconBlock(state_root=hash_tree_root(genesis_state))`.

TODO

## Fork Choice <!-- /part3/forkchoice/ -->

### Introduction

The beacon chain's fork choice is documented separately from the main state transition specification. Like the main specification, the fork choice spec is incremental, with later versions specifying only the changes since the previous version. When annotating the main spec I combined the incremental versions into a single up-to-date document. In the following, however, I will deal separately with the original [Phase 0 fork choice](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/fork-choice.md) and the incremental [Bellatrix fork choice](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/fork-choice.md) update as the latter mainly introduced one-off functionality specific to the Merge transition.

#### What's a fork choice?

As described in the [introduction to consensus](/part2/consensus/preliminaries/#fork-choice-rules), a fork choice rule is the means by which a node decides, given the information available to it, which block is the "best" head of the chain. A good fork choice rule results in the network of nodes eventually converging on the same canonical chain: it is able to resolve forks consistently, even under a degree of faulty or adversarial behaviour.

Ethereum's proof of stake consensus introduces a [`Store`](/part3/forkchoice/phase0/#store) object that contains all the data necessary for determining a best head. A node's Store is the "source of truth" for its fork choice rule. In classical consensus terms it is a node's local view: all the relevant information that a node has about the network state. The fork choice rule can be characterised as a function, $\text{GetHead}(\text{Store}) \rightarrow \text{HeadBlock}$.

During the Merge event, the beacon chain's fork choice was temporarily augmented to be able to consider blocks on the Eth1 chain, in order to agree which (of potentially multiple candidates) would become the terminal proof of work block.

#### Overview

Ethereum's fork choice comprises the LMD GHOST fork choice rule, modified by (constrained by) the Casper FFG fork choice rule. The Casper FFG rule modifies the LMD GHOST fork choice by only allowing blocks descended from the last finalised[^fn-last-finalised] checkpoint to be candidates for the chain head. All earlier branches are effectively pruned out of a node's local view of the network state.

[^fn-last-finalised]: I'm simplifying here. LMD GHOST can only consider descendants of the last justified checkpoint at any one time. But the last justified checkpoint can change. LMD GHOST will never consider branches from before the last finalised checkpoint. More on this later.

<a id="img_annotated_forkchoice_gasper"></a>
<figure class="diagram" style="width: 90%">

![Diagram of a block tree showing that Casper FFG finalises the early chain up to a checkpoint and LMD GHOST handles fork choice after that.](images/diagrams/annotated-forkchoice-gasper.svg)

<figcaption>

Casper FFG's role is to finalise a checkpoint. History prior to the finalised checkpoint is a linear chain of blocks with all branches pruned away. LMD GHOST is used to select the best head block at any time. LMD GHOST is constrained by Casper FFG in that it operates on the block tree only after the finalised checkpoint.

</figcaption>
</figure>

This combination has [come to be known](https://arxiv.org/abs/2003.03052) as "Gasper", and appears to be relatively simple [at first sight](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760?u=benjaminion). However, the emergence of various edge cases, and a relentless stream of potential attacks has led third party researchers [to declare](https://arxiv.org/pdf/2009.04987.pdf) that "The Gasper protocol is complex". And that remark was made before implementing many of the fixes that we'll be reviewing in the following sections. Vitalik himself [has written](https://notes.ethereum.org/@vbuterin/single_slot_finality#Bad-news-hybrid-consensus-mechanisms-actually-have-many-unavoidable-problems) that

> The "interface" between Casper FFG finalization and LMD GHOST fork choice is a source of significant complexity, leading to a number of attacks that have required fairly complicated patches to fix, with more weaknesses being regularly discovered.

Despite all this, we are happily running Ethereum on top of the Gasper protocol today. We continue to incrementally add defences against known attacks, and one day we may move on from Gasper entirely - perhaps to a [single slot finality](https://ethresear.ch/t/reorg-resilience-and-security-in-post-ssf-lmd-ghost/14164?u=benjaminion) protocol, or to [Casper CBC](https://medium.com/@jonchoi/ethereum-casper-101-7a851a4f1eb0#c979). Meanwhile, Gasper is proving to be "good enough" in practice.[^fn-gasper-weaknesses]

[^fn-gasper-weaknesses]: Appendix C.1 of the Goldfish, "No More Attacks on Proof-of-Stake Ethereum?" [paper](https://arxiv.org/pdf/2209.03255.pdf) is a useful overview of known weaknesses of Gasper consensus.

#### Scope and terminology

These fork choice specification documents don't cover the whole mechanism. They are largely concerned only with the LMD GHOST fork choice; the Casper FFG side of things (justification and finalisation) is dealt with [in the main state-transition specification](/part3/transition/epoch/#justification-and-finalization).

The terms attestation, vote, and message appear frequently. An attestation is a collection of [three votes](/part3/containers/dependencies/#attestationdata): a vote for a source checkpoint, a vote for a target checkpoint, and a vote for a head block. The source and target votes are used by Casper FFG, and the head vote is used by LMD GHOST. We will mostly be concerned with head votes in the following sections, except when stated otherwise. LMD GHOST head votes are also called messages, being the "M" in "LMD".

Where we discuss attestations, they can be a single attestation from one validator, or aggregate attestations containing the attestations of multiple validators that made the same set of votes. It will be clear from the context which of these applies.

#### Decoding dev-speak

Sometimes you'll hear protocol devs say slightly obscure things like, "we can deal with that in fork choice". For example, "we can handle censorship via the fork choice".

This framing makes sense when we understand that a node's fork choice rule is its expression of which chain it prefers to follow, or prefers not to follow. No honest node wants to follow a chain that contains invalid blocks (according to the state transition), so the fork choice of all honest nodes will never select a head block that has an invalid block in its ancestry.

Similarly, nodes could modify their fork choice rule so that branches with blocks that appear to censor transactions are never selected. If nodes with sufficient validators do this, then any such block will be orphaned, strongly discouraging censorship. This works both ways, of course. A government could declare that the fork choice must ignore any branches with blocks that do _not_ censor transactions. If enough validators &ndash; over half &ndash; choose to comply, then the whole chain will become censoring.

The goal of the fork choice is for the network to converge onto a single history, so there is a strong incentive to try to agree with one's peers. However, it also provides a mechanism that can be used (perhaps as an outcome of social coordination) to be opinionated about what kind of blocks are eventually included in that history.

#### History

[TODO: insert link to history of PoS]::

Proof of stake Ethereum has a long history that we shall review elsewhere. The following milestones are significant for the current Casper FFG plus LMD GHOST implementation.

Vitalik published the original [mini-spec](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760?u=benjaminion) for the beacon chain's proof of stake consensus on July 31st 2018, shortly after we had abandoned prior designs for moving Ethereum to PoS. The initial design used IMD GHOST (Immediate Message Driven GHOST) in which attestations have a limited lifetime in the fork choice[^fn-imd-ghost]. IMD GHOST [was changed](https://ethresear.ch/t/beacon-chain-casper-mini-spec/2760/17?u=benjaminion) to LMD GHOST (Latest Message Driven GHOST) in November 2018 due to concerns about the stability property of IMD.

[^fn-imd-ghost]: If I've understood correctly. Traces of IMD GHOST are difficult to find these days, and that's probably for the better.

The [initial fork choice spec](https://github.com/ethereum/consensus-specs/blob/a103e79e676ca08cac0040f60c90fecf7e2ea3f2/specs/core/0_fork-choice.md) was published to GitHub in April 2019, numbering a mere 96 lines. The [current Phase 0 fork choice spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/fork-choice.md) has 576 lines.

Various issues have caused the fork choice specification to balloon in complexity.

In August 2019, a "decoy flip-flop attack" on LMD GHOST [was identified](https://ethresear.ch/t/decoy-flip-flop-attack-on-lmd-ghost/6001?u=benjaminion) that could be used by an adversary to delay finalisation (for a limited period of time). The defence against this is to [add a check](https://github.com/ethereum/consensus-specs/pull/1466/files) that newly considered attestations are from either the current or previous epoch only. We'll cover this under [`validate_on_attestation()`](/part3/forkchoice/phase0/#attestation-timeliness).

In September 2019 a "bouncing attack" on Casper FFG [was identified](https://ethresear.ch/t/analysis-of-bouncing-attack-on-ffg/6113?u=benjaminion) that can delay finalisation indefinitely. Up to the Capella spec release we had [a fix](https://ethresear.ch/t/prevention-of-bouncing-attack-on-ffg/6114?u=benjaminion) for this that only allowed the fork choice's justified checkpoint to be updated during the early part of an epoch. The fix was removed in the Capella upgrade since it adds significant complexity to the fork choice, and in any case can be [worked around](https://notes.ethereum.org/@fradamt/Sy6PzcRdt) by splitting honest validators' views. The bouncing attack is very difficult to set up and an adversary with the power to do this could probably attack the chain in more interesting ways. The bouncing attack and its original fix remain documented in the [Bellatrix edition](/../bellatrix/part3/forkchoice/phase0/#the-bouncing-attack).

In July 2021, an edge case [was identified](https://notes.ethereum.org/@hww/fork-choice-store-inconsistency) in which (if 1/3 of validators were prepared to be slashed) the invariant that the store's justified checkpoint must be a descendant of the finalised checkpoint could become violated. [A fix](https://github.com/ethereum/consensus-specs/pull/2518) to the [`on_tick()`](/part3/forkchoice/phase0/#on_tick) handler was implemented to maintain the invariant.

In November 2021, some overly complicated logic [was identified](https://notes.ethereum.org/@djrtwo/S1ZGAXhwK) in the [`on_block()`](/part3/forkchoice/phase0/#on_block) handler that could lead to the Store retaining inconsistent finalised and justified checkpoints, which would in turn cause [`filter_block_tree()`](/part3/forkchoice/phase0/#filter_block_tree) to fail. Over one third of validators would have had to be slashed to trigger the fault, but the [resulting fix](https://github.com/ethereum/consensus-specs/pull/2727) turned out to be a nice simplification in any case.

[Proposer boost](/part3/forkchoice/phase0/#proposer-boost) was also [added](https://github.com/ethereum/consensus-specs/pull/2730) in November 2021. This is a defence against potential [balancing attacks](https://ethresear.ch/t/a-balancing-attack-on-gasper-the-current-candidate-for-eth2s-beacon-chain/8079?u=benjaminion) on LMD GHOST that could prevent Casper FFG from finalising. We'll cover this in detail in the [proposer boost](/part3/forkchoice/phase0/#proposer-boost) section.

A [new type](https://ethresear.ch/t/balancing-attack-lmd-edition/11853?u=benjaminion) of balancing attack was published in January 2022 that relies on the attacker's validators making equivocating attestations (multiple different attestations at the same slot). To counter this, a [defence against equivocating indices](https://github.com/ethereum/consensus-specs/pull/2845) was added in March 2022. We'll discuss this when we get to the [`on_attester_slashing()`](/part3/forkchoice/phase0/#equivocation_balancing_attack) handler. This defence was bolstered in the Capella spec update by excluding all slashed validators from having an influence in the fork choice.

Several issues involving "unrealised justfication" were discovered during the first half of 2022. First, an [unrealised justification reorg](https://notes.ethereum.org/@adiasg/unrealized-justification) attack that allowed the proposer of the first block of an epoch to easily fork out up to nine blocks from the end of the previous epoch. A variant of that attack was also found to be able to cause validators to make slashable attestations. Second, a [justification withholding attack](https://hackmd.io/o9tGPQL2Q4iH3Mg7Mma9wQ) that an adversary could use to reorg arbitrary numbers of blocks at the start of an epoch. These issues were addressed in the Capella spec update with the "pull up tips" and [unrealised justification logic](/part3/forkchoice/phase0/#unrealised-justification) that it introduced.

A reader might infer from this catalogue of issues that the fork choice is fiendishly difficult to reason about, and the reader would not be wrong. Some long-overdue [formal verification](/part3/forkchoice/phase0/#formal-proofs) work on the fork choice rule has recently been completed. It seeks to prove certain desirable properties, such as that an honest validator following the rules can never make slashable attestations.

We will study each of the issues above in more detail as we work through the fork choice specification in the following two sections.

  - [Phase 0 fork choice](/part3/forkchoice/phase0/) is the main fork choice specification.
  - [Bellatrix fork choice](/part3/forkchoice/bellatrix/) covers the changes to the fork choice around the Merge.

Note that the [Capella upgrade](/part4/history/capella/) included a [substantial rewrite](https://github.com/ethereum/consensus-specs/pull/3290) of the fork choice specification. The rewrite removed the bouncing attack fix and introduced the "pull up tips" defence against a new attack, among other things. The following sections are based on the updated Capella version, but the previous annotated fork choice [remains available](/../bellatrix/part3/forkchoice/). All of these changes were quietly rolled out prior to Capella, buried within various client software updates, while the updated spec was held back until the Capella upgrade itself[^fn-fork-choice-updates]. A [public disclosure](https://notes.ethereum.org/@djrtwo/2023-fork-choice-reorg-disclosure) of the issues was made a few weeks after the Capella upgrade.

[^fn-fork-choice-updates]: When issues are found with the fork choice it is common for them to be "silently" fixed in client releases before being made public. Paradoxically, changes to the fork choice rule are not consensus breaking and do not usually require simultaneous activation in clients. Hard fork upgrades such as Capella ensure that all node operators have, of necessity, upgraded to the latest software versions, at which point it is safe to publish details of the problems and fixes. This is one good reason for keeping your software up to date, even between mandatory upgrades.

### Phase 0 Fork Choice <!-- /part3/forkchoice/phase0/ -->

This section covers the [Phase 0 Fork Choice](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/fork-choice.md) document. It is based on the Capella, [v1.3.0](https://github.com/ethereum/consensus-specs/tree/v1.3.0), spec release version. For an alternative take, I recommend [Vitalik's annotated fork choice](https://github.com/ethereum/annotated-spec/blob/master/phase0/fork-choice.md) document.

Block-quoted content below (with a sidebar) has been copied over verbatim from the specs repo, as has all the function code.

> The head block root associated with a `store` is defined as `get_head(store)`. At genesis, let `store = get_forkchoice_store(genesis_state, genesis_block)` and update `store` by running:
>
>   - `on_tick(store, time)` whenever `time > store.time` where `time` is the current Unix time
>   - `on_block(store, block)` whenever a block `block: SignedBeaconBlock` is received
>   - `on_attestation(store, attestation)` whenever an attestation `attestation` is received
>   - `on_attester_slashing(store, attester_slashing)` whenever an attester slashing `attester_slashing` is received
>
> Any of the above handlers that trigger an unhandled exception (e.g. a failed assert or an out-of-range list access) are considered invalid. Invalid calls to handlers must not modify `store`.

Updates to the Store arise only through the four handler functions: [`on_tick()`](#on_tick), [`on_block()`](#on_block), [`on_attestation()`](#on_attestation), and [`on_attester_slashing()`](#on_attester_slashing). These are the four senses through which the fork choice gains its knowledge of the world.

> _Notes_:
><!-- markdownlint-disable ol-prefix -->
> 1) **Leap seconds**: Slots will last `SECONDS_PER_SLOT` `+ 1` or `SECONDS_PER_SLOT` `- 1` seconds around leap seconds. This is automatically handled by [UNIX time](https://en.wikipedia.org/wiki/Unix_time).

Leap seconds will no longer occur [after 2035](https://www.timeanddate.com/news/astronomy/end-of-leap-seconds-2022). We can remove this note after that.

> 2) **Honest clocks**: Honest nodes are assumed to have clocks synchronized within `SECONDS_PER_SLOT` seconds of each other.

In practice, the synchrony assumptions are stronger than this. Any node whose clock is more than `SECONDS_PER_SLOT` `/` `INTERVALS_PER_SLOT` (four seconds) adrift will suffer degraded performance and can be considered Byzantine (faulty), at least for the LMD GHOST fork choice.

> 3) **Eth1 data**: The large `ETH1_FOLLOW_DISTANCE` specified in the [honest validator document](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md) should ensure that `state.latest_eth1_data` of the canonical beacon chain remains consistent with the canonical Ethereum proof-of-work chain. If not, emergency manual intervention will be required.

Post-Merge, consistency between the execution and consensus layers is no longer an issue, although we retain the [`ETH1_FOLLOW_DISTANCE`](/part3/config/configuration/#eth1_follow_distance) for now.

> 4) **Manual forks**: Manual forks may arbitrarily change the fork choice rule but are expected to be enacted at epoch transitions, with the fork details reflected in `state.fork`.

Manual forks are sometimes called hard forks or upgrades, and are planned in advance and coordinated. They are different from the inadvertent forks that the fork choice rule is designed to resolve.

> 5) **Implementation**: The implementation found in this specification is constructed for ease of understanding rather than for optimization in computation, space, or any other resource. A number of optimized alternatives can be found [here](https://github.com/protolambda/lmd-ghost).
<!-- markdownlint-enable ol-prefix -->

After reading the spec you may be puzzled by the "ease of understanding" claim. However, it is certainly true that several of the algorithms are far from efficient, and a great deal of optimisation is needed for practical implementations.

### Constant

<a id="intervals_per_slot"></a>

| Name                 | Value       |
| -------------------- | ----------- |
| `INTERVALS_PER_SLOT` | `uint64(3)` |

Only blocks that arrive during the first `1 /` `INTERVALS_PER_SLOT` of a slot's duration are eligible to have the [proposer score boost](#proposer-boost) added. This moment is the point in the slot at which validators are expected to [publish attestations](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#attesting) declaring their view of the head of the chain.

In the Ethereum consensus specification `INTERVALS_PER_SLOT` neatly divides `SECONDS_PER_SLOT`, and all time quantities are strictly `uint64` numbers of seconds. However, other chains that run the same basic protocol as Ethereum might not have this property. For example, the [Gnosis Beacon Chain](https://docs.gnosischain.com/specs) has five-second slots. We [changed](https://github.com/ConsenSys/teku/pull/5321) Teku's internal clock from seconds to milliseconds to support this, which is technically off-spec, but nothing broke.

### Configuration

<a id="proposer_score_boost"></a>

| Name                   | Value        |
| ---------------------- | ------------ |
| `PROPOSER_SCORE_BOOST` | `uint64(40)` |

>   - The proposer score boost is worth `PROPOSER_SCORE_BOOST` percentage of the committee's weight, i.e., for slot with committee weight `committee_weight` the boost weight is equal to `(committee_weight * PROPOSER_SCORE_BOOST) // 100`.

[Proposer boost](#proposer-boost) is a modification to the fork choice rule that defends against a so-called [balancing attack](https://ethresear.ch/t/a-balancing-attack-on-gasper-the-current-candidate-for-eth2s-beacon-chain/8079?u=benjaminion). When a timely block proposal is received, proposer boost temporarily adds a huge weight to that block's branch in the fork choice calculation, namely `PROPOSER_SCORE_BOOST` percent of the total effective balances of all the validators assigned to attest in that slot.

The value of `PROPOSER_SCORE_BOOST` has changed over time as the balancing attack has been analysed more thoroughly.

  - Vitalik's original [proposed mitigation](https://notes.ethereum.org/@vbuterin/lmd_ghost_mitigation) discussed using a value of 25%.
  - The initial implementation on November 23, 2021, [changed it to 70%](https://github.com/ethereum/consensus-specs/pull/2730/files) (without any recorded rationale for that number).
  - On May 9, 2022, it was [changed to 33%](https://github.com/ethereum/consensus-specs/pull/2888/files) as the result of much more [detailed analysis](https://notes.ethereum.org/@casparschwa/H1T0k7b85).
  - On May 20, 2022, it was [changed to 40%](https://github.com/ethereum/consensus-specs/pull/2895/files), due to an off-by-one calculation in the above analysis.

The basic trade-off in choosing a value for `PROPOSER_SCORE_BOOST` is between allowing an adversary to perform "ex-ante" or "ex-post" reorgs. Setting `PROPOSER_SCORE_BOOST` too high makes it easier for an adversarial proposer to perform ex-post reorgs - it gives the proposer disproportionate power compared with the votes of validators. Setting `PROPOSER_SCORE_BOOST` too low makes it easier for an adversary to perform ex-ante reorgs. Caspar Schwarz-Schilling covers these trade-offs nicely in his Liscon talk, [The game of reorgs in PoS Ethereum](https://vimeo.com/637529564).[^fn-ex-ante-ex-post]

[^fn-ex-ante-ex-post]: "Ex-post" reorgs occur when a proposer orphans the block in the previous slot by building on an ancestor. "Ex-ante" reorgs occur when a proposer arranges to orphan the next block by submitting its own proposal late. Caspar Schwarz-Schilling made a nice [Twitter thread](https://web.archive.org/web/20230630135719/https://nitter.it/casparschwa/status/1454511850821931017) explainer.

### Helpers

#### `LatestMessage`

```python
class LatestMessage(object):
    epoch: Epoch
    root: Root
```

This is just a convenience class for tracking the most recent head vote from each validator - the "LM" (latest message) in LMD&nbsp;GHOST. [`Epoch`](/part3/config/types/#epoch) is a `uint64` type, and [`Root`](/part3/config/types/#root) is a `Bytes32` type. The Store holds a mapping of validator indices to their latest messages.

#### `Store`

> The `Store` is responsible for tracking information required for the fork choice algorithm. The important fields being tracked are described below:
>
>   - `justified_checkpoint`: the justified checkpoint used as the starting point for the LMD GHOST fork choice algorithm.
>   - `finalized_checkpoint`: the highest known finalized checkpoint. The fork choice only considers blocks that are not conflicting with this checkpoint.
>   - `unrealized_justified_checkpoint` & `unrealized_finalized_checkpoint`: these track the highest justified & finalized checkpoints resp., without regard to whether on-chain _**realization**_ has occurred, i.e. FFG processing of new attestations within the state transition function. This is an important distinction from `justified_checkpoint` & `finalized_checkpoint`, because they will only track the checkpoints that are realized on-chain. Note that on-chain processing of FFG information only happens at epoch boundaries.
>   - `unrealized_justifications`: stores a map of block root to the unrealized justified checkpoint observed in that block.

These explanatory points were added in the Capella upgrade[^fn-explanatory-points]. We will expand on them below in the appropriate places.

[^fn-explanatory-points]: It's interesting to see some explanation finding its way back into the spec documents now, after it was all diligently stripped out a while ago. Turns out that people appreciate explanations, I guess.

```python
class Store(object):
    time: uint64
    genesis_time: uint64
    justified_checkpoint: Checkpoint
    finalized_checkpoint: Checkpoint
    unrealized_justified_checkpoint: Checkpoint
    unrealized_finalized_checkpoint: Checkpoint
    proposer_boost_root: Root
    equivocating_indices: Set[ValidatorIndex]
    blocks: Dict[Root, BeaconBlock] = field(default_factory=dict)
    block_states: Dict[Root, BeaconState] = field(default_factory=dict)
    checkpoint_states: Dict[Checkpoint, BeaconState] = field(default_factory=dict)
    latest_messages: Dict[ValidatorIndex, LatestMessage] = field(default_factory=dict)
    unrealized_justifications: Dict[Root, Checkpoint] = field(default_factory=dict)
```

A node's Store records all the fork choice related information that it has about the outside world. In more classical terms, the Store is the node's view of the network. The Store is updated only by the [four handler functions](#handlers).

The basic fields are as follows.

  - `time`: The wall-clock time (Unix time) of the last call to the [`on_tick()`](#on_tick) handler. In theory this is update continuously; in practice only at least two or three times per slot.
  - `justified_checkpoint`: Our node's view of the currently justified checkpoint.
  - `finalized_checkpoint`: Our node's view of the currently finalised checkpoint.
  - `blocks`: All the blocks that we know about that are descended from the `finalized_checkpoint`. The fork choice spec does not describe how to prune the Store, so we would end up with all blocks since genesis if we were to follow it precisely. However, only blocks descended from the last finalised checkpoint are ever considered in the fork choice, and the finalised checkpoint only increases in height. So it is safe for client implementations to remove from the Store all blocks (and their associated states) belonging to branches not descending from the last finalised checkpoint.
  - `block_states`: For every block in the Store, we also keep its corresponding (post-)state. These states are mostly used for information about justification and finalisation.
  - `checkpoint_states`: If there are empty slots immediately before a checkpoint then the checkpoint state will not correspond to a block state, so we store checkpoint states as well, indexed by [`Checkpoint`](/part3/containers/dependencies/#checkpoint) rather than block root. The state at the last justified checkpoint is used for validator balances, and for validating attestations in the [`on_attester_slashing()`](#on_attester_slashing) handler.
  - `latest_messages`: The set of latest head votes from validators. When the [`on_attestation()`](#on_attestation) handler processes a new head vote for a validator, it gets added to this set and the old vote is discarded.

The following fields were added at various times as new attacks and defences were found.

  - `proposer_boost_root` was [added](https://github.com/ethereum/consensus-specs/pull/2730) when proposer boost was implemented as a defence against the [LMD balancing attack](#proposer-boost). It is set to the root of the current block for the duration of a slot, as long as that block arrived within the first third of a slot.
  - The `equivocating_indices` set was [added](https://github.com/ethereum/consensus-specs/pull/2845) to defend against the [equivocation balancing attack](#equivocation_balancing_attack). It contains the indices of any validators reported as having committed an attester slashing violation. These validators must be removed from consideration in the fork choice rule until the last justified checkpoint state catches up with the fact that the validators have been slashed.
  - The `unrealized_justified_checkpoint` and `unrealized_finalized_checkpoint`fields were [added](https://github.com/ethereum/consensus-specs/pull/3290) in the Capella update. They are used to avoid certain problems with [unrealised justification](#unrealised-justification) that the old version of `filter_block_tree()` suffered.
  - Also added in the Capella update was `unrealized_justifications`, which is a map of block roots to unrealised justification checkpoints. It is maintained by [`compute_pulled_up_tip()`](#compute_pulled_up_tip). For every block, it stores the justified checkpoint that results from running [`process_justification_and_finalization()`](/part3/transition/epoch/#def_process_justification_and_finalization) on the block's post-state. In the beacon state, that calculation is done only on epoch boundaries, so, within the fork choice, we call the result "unrealised".

For non-Pythonistas, [`Set`](https://docs.python.org/3/library/typing.html#typing.Set) and [`Dict`](https://docs.python.org/3/library/typing.html#typing.Dict) are Python generic types. A `Set` is an unordered collection of objects; a `Dict` provides key&ndash;value look-up.

#### `is_previous_epoch_justified`

```python
def is_previous_epoch_justified(store: Store) -> bool:
    current_slot = get_current_slot(store)
    current_epoch = compute_epoch_at_slot(current_slot)
    return store.justified_checkpoint.epoch + 1 == current_epoch
```

Based on the current time in the Store, this function returns `True` if the checkpoint at the start of the previous epoch has been justified - that is, has received a super-majority Casper FFG vote.

|||
|-|------|
| Used&nbsp;by | [`filter_block_tree()`](#filter_block_tree) |

#### `get_forkchoice_store`

> The provided anchor-state will be regarded as a trusted state, to not roll back beyond. This should be the genesis state for a full client.
>
> _Note_ With regards to fork choice, block headers are interchangeable with blocks. The spec is likely to move to headers for reduced overhead in test vectors and better encapsulation. Full implementations store blocks as part of their database and will often use full blocks when dealing with production fork choice.

```python
def get_forkchoice_store(anchor_state: BeaconState, anchor_block: BeaconBlock) -> Store:
    assert anchor_block.state_root == hash_tree_root(anchor_state)
    anchor_root = hash_tree_root(anchor_block)
    anchor_epoch = get_current_epoch(anchor_state)
    justified_checkpoint = Checkpoint(epoch=anchor_epoch, root=anchor_root)
    finalized_checkpoint = Checkpoint(epoch=anchor_epoch, root=anchor_root)
    proposer_boost_root = Root()
    return Store(
        time=uint64(anchor_state.genesis_time + SECONDS_PER_SLOT * anchor_state.slot),
        genesis_time=anchor_state.genesis_time,
        justified_checkpoint=justified_checkpoint,
        finalized_checkpoint=finalized_checkpoint,
        unrealized_justified_checkpoint=justified_checkpoint,
        unrealized_finalized_checkpoint=finalized_checkpoint,
        proposer_boost_root=proposer_boost_root,
        equivocating_indices=set(),
        blocks={anchor_root: copy(anchor_block)},
        block_states={anchor_root: copy(anchor_state)},
        checkpoint_states={justified_checkpoint: copy(anchor_state)},
        unrealized_justifications={anchor_root: justified_checkpoint}
    )
```

`get_forkchoice_store()` initialises the fork choice Store object from an anchor state and its corresponding block (header). As noted, the anchor state could be the genesis state. Equally, when using a [checkpoint sync](https://docs.teku.consensys.net/get-started/checkpoint-start), the anchor state will be the finalised checkpoint state provided by the node operator, which will be [treated as if](https://github.com/ethereum/consensus-specs/issues/2566) it is a genesis state. In either case, the `latest_messages` store will be empty to begin with.

#### `get_slots_since_genesis`

```python
def get_slots_since_genesis(store: Store) -> int:
    return (store.time - store.genesis_time) // SECONDS_PER_SLOT
```

Self explanatory. This one of only two places that `store.time` is used, the other being in the proposer boost logic in the [`on_block()`](#on_block) handler.

|||
|-|------|
| Used&nbsp;by | [`get_current_slot()`](#get_current_slot) |

#### `get_current_slot`

```python
def get_current_slot(store: Store) -> Slot:
    return Slot(GENESIS_SLOT + get_slots_since_genesis(store))
```

Self explanatory. [`GENESIS_SLOT`](/part3/config/constants/#genesis_slot) is usually zero.

|||
|-|------|
| Used&nbsp;by | [`get_voting_source()`](#get_voting_source), [`filter_block_tree()`](#filter_block_tree), [`compute_pulled_up_tip()`](#compute_pulled_up_tip), [`on_tick_per_slot`](#on_tick_per_slot), [`validate_target_epoch_against_current_time()`](#validate_target_epoch_against_current_time), [`validate_on_attestation()`](#validate_on_attestation), [`on_tick()`](#on_tick), [`on_block()`](#on_block) |
| Uses | [`get_slots_since_genesis()`](#get_slots_since_genesis) |

#### `compute_slots_since_epoch_start`

```python
def compute_slots_since_epoch_start(slot: Slot) -> int:
    return slot - compute_start_slot_at_epoch(compute_epoch_at_slot(slot))
```

Self explanatory.

|||
|-|------|
| Used&nbsp;by | [`on_tick_per_slot()`](#on_tick_per_slot) |
| Uses | [`compute_epoch_at_slot()`](/part3/helper/misc/#def_compute_epoch_at_slot), [`compute_start_slot_at_epoch()`](/part3/helper/misc/#def_compute_start_slot_at_epoch) |

#### `get_ancestor`

```python
def get_ancestor(store: Store, root: Root, slot: Slot) -> Root:
    block = store.blocks[root]
    if block.slot > slot:
        return get_ancestor(store, block.parent_root, slot)
    return root
```

Given a block root `root`, `get_ancestor()` returns the ancestor block (on the same branch) that was published at slot `slot`. If there was no block published at `slot`, then the ancestor block most recently published prior to `slot` is returned.

This function is sometimes used just to confirm that the block with root `root` is descended from a particular block at slot `slot`, and sometimes used actually to retrieve that ancestor block's root.

|||
|-|------|
| Uses | `get_ancestor()` (recursively) |
| Used&nbsp;by | [`get_weight()`](#get_weight), [`filter_block_tree()`](#filter_block_tree), [`validate_on_attestation()`](#validate_on_attestation), [`on_block()`](#on_block), `get_ancestor()` (recursively) |

#### `get_weight`

```python
def get_weight(store: Store, root: Root) -> Gwei:
    state = store.checkpoint_states[store.justified_checkpoint]
    unslashed_and_active_indices = [
        i for i in get_active_validator_indices(state, get_current_epoch(state))
        if not state.validators[i].slashed
    ]
    attestation_score = Gwei(sum(
        state.validators[i].effective_balance for i in unslashed_and_active_indices
        if (i in store.latest_messages
            and i not in store.equivocating_indices
            and get_ancestor(store, store.latest_messages[i].root, store.blocks[root].slot) == root)
    ))
    if store.proposer_boost_root == Root():
        # Return only attestation score if ``proposer_boost_root`` is not set
        return attestation_score

    # Calculate proposer score if ``proposer_boost_root`` is set
    proposer_score = Gwei(0)
    # Boost is applied if ``root`` is an ancestor of ``proposer_boost_root``
    if get_ancestor(store, store.proposer_boost_root, store.blocks[root].slot) == root:
        committee_weight = get_total_active_balance(state) // SLOTS_PER_EPOCH
        proposer_score = (committee_weight * PROPOSER_SCORE_BOOST) // 100
    return attestation_score + proposer_score
```

Here we find the essence of the GHOST[^fn-ghost-acronym] protocol: the weight of a block is the sum of the votes for that block, _plus_ the votes for all of its descendant blocks. We include votes for descendants when calculating a block's weight because a vote for a block is an implicit vote for all of that block's ancestors as well - if a particular block gets included on chain, all its ancestors must also be included. To put it another way, we treat validators as voting for entire branches rather than just their leaves.

[^fn-ghost-acronym]: "Greedy Heaviest-Observed Sub-Tree", named by [Sompolinsky and Zohar](https://eprint.iacr.org/2013/881.pdf).

Ignoring the proposer boost part for the time being, the main calculation being performed is as follows.

```none
    state = store.checkpoint_states[store.justified_checkpoint]
    unslashed_and_active_indices = [
        i for i in get_active_validator_indices(state, get_current_epoch(state))
        if not state.validators[i].slashed
    ]
    attestation_score = Gwei(sum(
        state.validators[i].effective_balance for i in unslashed_and_active_indices
        if (i in store.latest_messages
            and i not in store.equivocating_indices
            and get_ancestor(store, store.latest_messages[i].root, store.blocks[root].slot) == root)
    ))
```

We only consider the votes of active and unslashed validators. (Slashed validators might still be in the exit queue and are technically "active", at least according to [`is_active_validator()`](/part3/helper/predicates/#def_is_active_validator).) The exclusion of validators that have been slashed in-protocol at the last justified checkpoint was added in the Capella specification to complement the [`on_attester_slashing()`](#on_attester_slashing) handler. It will additionally exclude validators slashed via proposer slashings, and validators slashed long ago (when the exit queue is long) and for which we have discarded the attester slashing from the Store.

Given a block root, `root`, this adds up all the votes for blocks that are descended from that block. More precisely, it calculates the sum of the effective balances of all validators whose latest head vote was for a descendant of `root` or for `root` itself. It's the fact that we're basing our weight calculations only on each validator's _latest_ vote that makes this "LMD" (latest message drive) GHOST.

<a id="img_annotated_forkchoice_get_weight_0"></a>
<figure class="diagram" style="width: 90%">

![Diagram of a block tree with weights and latest attesting balances shown for each block.](images/diagrams/annotated-forkchoice-get-weight-0.svg)

<figcaption>

$B_N$ is the sum of the effective balances of the validators whose most recent head vote was for block $N$, and $W_N$ is the weight of the branch starting at block $N$.

</figcaption>
</figure>

Some obvious relationships apply between the weights, $W_x$, of blocks, and $B_x$, the latest attesting balances of blocks.

  - For a leaf block $N$ (a block with no children), $W_N = B_N$.
  - The weight of a block is its own latest attesting balance plus the sum of the weights of its direct children. So, in the diagram, $W_1 = B_1 + W_2 + W_3$.

These relationships can be used to avoid repeating lots of work by memoising the results.

##### Proposer boost

In September 2020, shortly before mainnet genesis, a theoretical "[balancing attack](https://arxiv.org/abs/2009.04987)" on the LMD GHOST consensus mechanism was published, with an accompanying [Ethresear.ch post](https://ethresear.ch/t/a-balancing-attack-on-gasper-the-current-candidate-for-eth2s-beacon-chain/8079?u=benjaminion).

The balancing attack allows a very small number of validators controlled by an adversary to perpetually maintain a forked network, with half of all validators following one fork and half the other. This would delay finalisation indefinitely, which is a kind of liveness failure. Since the attack relies on some unrealistic assumptions about the power an adversary has over the network &ndash; namely, fine-grained control over who can see what and when &ndash; we felt that the potential attack was not a significant threat to the launch of the beacon chain. [Later refinements](https://arxiv.org/abs/2110.10086) to the attack appear to have made it more practical to execute, however.

A modification to the fork choice to mitigate the balancing attack was first [suggested by Vitalik](https://notes.ethereum.org/@vbuterin/lmd_ghost_mitigation). This became known as proposer boost, and a version of it [was adopted](https://github.com/ethereum/consensus-specs/pull/2730) into the consensus layer specification in late 2021 with the various client teams releasing versions with mainnet support for proposer boost in April and May 2022.

Changes to the fork choice can be made outside major protocol upgrades; it is not strictly necessary for all client implementations to make the change simultaneously, as they must for hard-fork upgrades. Given this, mainnet client releases supporting proposer boost were made at [various times](https://kyrianalex.substack.com/p/ethereums-7-block-reorg) in April and May 2022, and users were not forced to upgrade on a fixed schedule. Unfortunately, having a mix of nodes on the network, around half applying proposer boost and half not, led to a [seven block reorganisation](https://barnabe.substack.com/p/pos-ethereum-reorg) of the beacon chain on May 25, 2022. As a result, subsequent updates to the fork choice have tended to be more tightly coordinated between client teams.

##### Proposer boost details

Proposer boost modifies our nice, intuitive calculation of a branch's weight, based only on latest votes, by adding additional weight to a block that was received on time in the current slot. In this way, it introduces a kind of synchrony weighting. Vitalik [calls this](https://notes.ethereum.org/@vbuterin/lmd_ghost_mitigation#Proposed-solution) "an explicit 'synchronization bottleneck' gadget". In short, it treats a timely block as being a vote with a massive weight that is temporarily added to the branch that it is extending.

The simple intuition behind proposer boost [is summarised](https://barnabe.substack.com/p/pos-ethereum-reorg) by Barnabé Monnot as, "a block that is timely shouldn’t expect to be re-orged". In respect of the balancing attack, proposer boost is designed to overwhelm the votes from validators controlled by the adversary and instead allow the proposer of the timely block to choose the fork that will win. Quoting [Francesco D'Amato](https://ethresear.ch/t/view-merge-as-a-replacement-for-proposer-boost/13739?u=benjaminion#high-level-mitigation-idea-3), "the general strategy is to empower honest proposers to impose their view of the fork-choice, but without giving them too much power and making committees irrelevant".

The default setting for `store.proposer_boost_root` is `Root()`. That is, the "empty" or "null" [default SSZ](/part2/building_blocks/ssz/#default-values) root value, with all bytes set to zero. Whenever a block is received during the first `1 /` `INTERVALS_PER_SLOT` portion of a slot &ndash; that is, when the block is timely &ndash; `store.proposer_boost_root` is set to the hash tree root of that block by the [`on_block()`](/part3/forkchoice/phase0/#on_block) handler. At the end of each slot it is reset to `Root()` by the [`on_tick()`](/part3/forkchoice/phase0/#on_block) handler. Thus, proposer boost has an effect on the fork choice calculation from the point at which a timely block is received until the end of that slot, where "timely" on Ethereum's beacon chain means "within the first four seconds".

Proposer boost causes entire branches to be favoured when the block at their tip is timely. When proposer boost is in effect, and the timely block in the current slot (which has root, `store.proposer_boost_root`) is descended from the block we are calculating the weight for, then that block's weight is also increased, since the calculation includes the weights of all its descendants. In this way, proposer boost weighting [propagates to the boosted block's ancestors](https://github.com/ethereum/consensus-specs/pull/2760) in the same way as vote weights do.

The weight that proposer boost adds to the block's branch is a percentage `PROPOSER_SCORE_BOOST` of the total effective balance of all validators due to attest at that slot. Rather, it is an approximation to the total effective balance for that slot, derived by dividing the total effective balance of all validators by the number of slots per epoch.

The value of `PROPOSER_SCORE_BOOST` has changed over time before settling at its current 40%. See [the description there](#proposer_score_boost) for the history, and links to how the current value was calculated.

##### Proposer boost and late blocks

A side-effect of proposer boost is that it enables clients to reliably re-org out (orphan) blocks that were published late. Instead of building on a late block, the proposer can choose to build on the late block's parent.

A block proposer is supposed to publish its block at the start of the slot, so that it has time to be received and attested to by the whole committee within the first four seconds. However, post-merge, it can be profitable to delay block proposals by several seconds in order to collect more transaction income and better extractable value opportunities. Although blocks published five or six seconds into a slot will not gain many votes, they are still [likely to remain canonical](https://notes.ethereum.org/@casparschwa/ByHu1XZUq) under the basic consensus spec. As long as the next block proposer receives the late block by the end of the slot, it will usually build on it as the best available head.[^fn-legend-late-blocks] This is undesirable as it punishes the vast majority of honest validators, that (correctly) voted for an empty slot, by depriving them of their reward for correct head votes, and possibly even penalising them for incorrect target votes at the start of an epoch.

[^fn-legend-late-blocks]: For example, blocks in [slot 4939809](https://beaconcha.in/slot/4939809#votes) and [slot 4939815](https://beaconcha.in/slot/4939815#votes) had almost no votes and yet became canonical. They were almost certainly published late &ndash; apparently by the same operator, [Legend](https://beaconcha.in/slots?q=Legend) &ndash; but in time for the next proposer to build on them. The late publishing may have been due to a simple clock misconfiguration, or it may have been a deliberate strategy to gain more transaction income post-merge. In either case, it is undesirable.

Without proposer boost, it is a losing strategy for the next proposer not to build on a block that it received late. Although the late block may have few votes, it has more votes than your block initially, so validators will still attest to the late block as the head of the chain, keeping it canonical and orphaning the alternative block that you built on its parent.

With proposer boost, as long as the late block has fewer votes than the proposer boost percentage, the honest proposer can be confident that its alternative block will win the fork choice for long enough that the next proposer will build on that rather than on the late block it skipped.

<a id="img_annotated_forkchoice_late_block_0"></a>
<figure class="diagram" style="width: 90%">
<div style="width: 80%">

![Diagram showing a proposer choosing whether to build on a late block or its parent.](images/diagrams/annotated-forkchoice-late-block-0.svg)

</div>
<figcaption>

Block $B$ was published late, well after the 4 second attestation cut-off time. However, it still managed to acquire a few attestations (say, 10% of the committee) due to dishonest or misconfigured validators. Should the next proposer build $C_1$ on top of the late block, or $C_2$ on top of its parent?

</figcaption>
</figure>

<a id="img_annotated_forkchoice_late_block_1"></a>
<figure class="diagram" style="width: 90%">
<div style="width: 80%">

![Diagram showing that without proposer score boosting a proposer should build on the late block.](images/diagrams/annotated-forkchoice-late-block-1.svg)

</div>
<figcaption>

Without proposer boost, it only makes sense to build $C_1$, on top of the late block $B$. Since $B$ has some weight, albeit small, the top branch will win the fork choice (if the network is behaving synchronously at the time). Block $C_2$ would be orphaned.

</figcaption>
</figure>

<a id="img_annotated_forkchoice_late_block_2"></a>
<figure class="diagram" style="width: 90%">
<div style="width: 80%">

![Diagram showing that with proposer score boosting a proposer may build on the late block's parent.](images/diagrams/annotated-forkchoice-late-block-2.svg)

</div>
<figcaption>

With proposer boost, the proposer of $C$ can safely publish either $C_1$ or $C_2$. Due to the proposer score boost of 40%, it is safe to publish block $C_2$ that orphans $B$ since the lower branch will have greater weight during the slot.

</figcaption>
</figure>

An [implementation](https://github.com/sigp/lighthouse/pull/2860) of this strategy in the Lighthouse client seems to have been effective in reducing the number of late blocks on the network. Publishing of late blocks is strongly disincentivised when they are likely to be orphaned. It may be [adopted](https://github.com/ethereum/consensus-specs/pull/3034) as standard behaviour in the consensus specs at some point, but remains optional for the time-being. Several safe-guards are present in order to avoid liveness failures.

Note that Proposer boost does not in general allow validators to re-org out timely blocks (that is, an ex-post reorg). A timely block ought to gain enough votes from the committees that it will always remain canonical.

##### Alternatives to proposer boost

Proposer boost is not a perfect solution to balancing attacks or ex-ante reorgs. It makes ex-post reorgs easier to accomplish; it does not scale with participation, meaning that if only 40% of validators are online, then proposers can reorg at will; it can fail when an attacker controls several consecutive slots over which to store up votes.

Some changes to, or replacements for, LMD GHOST have been suggested that do not require proposer score boosting.

[View-merge](https://ethresear.ch/t/view-merge-as-a-replacement-for-proposer-boost/13739?u=benjaminion)[^fn-view-merge-first] is a mechanism in which attesters freeze their fork choice some time $\Delta$ before the end of a slot. The next proposer does not freeze its fork choice, however. The assumed maximum network delay is $\Delta$, so the proposer will see all votes in time, and it will circulate a summary of them to all validators, contained within its block. This allows the whole network to synchronise on a common view. Balancing attacks rely on giving two halves of the network different views, and would be prevented by view-merge.

[^fn-view-merge-first]: View-merge, though not by that name, was first proposed for Ethereum in October 2021 in the Ethresear.ch post, [Change fork choice rule to mitigate balancing and reorging attacks](https://ethresear.ch/t/change-fork-choice-rule-to-mitigate-balancing-and-reorging-attacks/11127?u=benjaminion). See also [this Twitter thread](https://web.archive.org/web/20230630135730/https://nitter.it/fradamt/status/1572884967461474306) for more explanation of view-merge.

The Goldfish protocol, described in the paper [No More Attacks on Proof-of-Stake Ethereum?](https://arxiv.org/abs/2209.03255), builds on view-merge (called "message buffering" there) and adds vote expiry so that head block votes expire almost immediately (hence the name - rightly or wrongly, goldfish are famed for their short memories). The resulting protocol is provably reorg resilient and supports fast confirmations.

Both view-merge and Goldfish come with nice proofs of their properties under synchronous conditions, which improve on Gasper under the same conditions. However, they may not fare so well under more realistic asynchronous conditions. The original view-merge article [says](https://ethresear.ch/t/change-fork-choice-rule-to-mitigate-balancing-and-reorging-attacks/11127?u=benjaminion#musings-on-latency-13) of latency greater than 2 seconds, "This is bad". One of the authors of the Goldfish paper [has said that](https://ethresear.ch/t/reorg-resilience-and-security-in-post-ssf-lmd-ghost/14164?u=benjaminion) Goldfish "is extremely brittle to asynchrony, allowing for catastrophic failures such as arbitrarily long reorgs"[^fn-goldfish-brittle], and [elsewhere](https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920?u=benjaminion), "even a single slot of asynchrony can lead to a catastrophic failure, jeopardizing the safety of any previously confirmed block". At least with proposer boost, we know that it only degrades to normal Gasper under conditions of high latency.

[^fn-goldfish-brittle]: To find the section 6.3 that this quote refers to, you need to see the [original v1 version](https://arxiv.org/pdf/2209.03255v1.pdf) of the Goldfish paper. That section is omitted from the later version of the paper.

Francesco D'Amato argues in [Reorg resilience and security in post-SSF LMD-GHOST](https://ethresear.ch/t/reorg-resilience-and-security-in-post-ssf-lmd-ghost/14164?u=benjaminion) that the real origin of the reorg issues with LMD GHOST is our current committee-based voting: "The crux of the issue is that honest majority of the committee of a slot does not equal a majority of the eligible fork-choice weight", since an adversary is able to influence the fork choice with votes from other slots. The ultimate cure for this would be [single slot finality](https://notes.ethereum.org/@vbuterin/single_slot_finality) (SSF), in which all validators vote at every slot. SSF is a long way from being practical today, but a candidate for its fork choice is [RLMD-GHOST](https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920?u=benjaminion) (Recent Latest Message Driven GHOST), which expires votes after a configurable time period.

|||
|-|------|
| Used&nbsp;by | [`get_head()`](#get_head) |
| Uses | [`get_active_validator_indices()`](/part3/helper/accessors/#def_get_active_validator_indices), [`get_ancestor()`](#get_ancestor), [`get_total_active_balance()`](/part3/helper/accessors/#def_get_total_active_balance) |
| See&nbsp;also | [`on_tick()`](#on_tick), [`on_block()`](#on_block), [`PROPOSER_SCORE_BOOST`](#proposer_score_boost) |

#### `get_voting_source`

```python
def get_voting_source(store: Store, block_root: Root) -> Checkpoint:
    """
    Compute the voting source checkpoint in event that block with root ``block_root`` is the head block
    """
    block = store.blocks[block_root]
    current_epoch = compute_epoch_at_slot(get_current_slot(store))
    block_epoch = compute_epoch_at_slot(block.slot)
    if current_epoch > block_epoch:
        # The block is from a prior epoch, the voting source will be pulled-up
        return store.unrealized_justifications[block_root]
    else:
        # The block is not from a prior epoch, therefore the voting source is not pulled up
        head_state = store.block_states[block_root]
        return head_state.current_justified_checkpoint
```

If the given block (which is a leaf block in the Store's block tree) is from a prior epoch, then return its [unrealised justification](#unrealised-justification). Otherwise return the justified checkpoint from its post-state (its realised justification).

Returning the unrealised justification is called "pulling up" the block (or "pulling the tip of a branch") as it is equivalent to running the end-of-epoch state transition accounting on the block's post-state: the block is notionally pulled up from its actual slot to the first slot of the next epoch.

The Casper FFG source vote is the checkpoint that a validator believes is the highest justified at the time of the vote. As such, this function returns the source checkpoint that validators with this block as head will use when casting a Casper FFG vote in the current epoch. This has an important role in [`filter_block_tree()`](#filter_block_tree) and is used in the [formal proof of non-self-slashability](#formal-proofs).

|||
|-|------|
| Used&nbsp;by | [`filter_block_tree()`](#filter_block_tree) |
| Uses | [`compute_epoch_at_slot()`](/part3/helper/misc/#def_compute_epoch_at_slot) |

#### `filter_block_tree`

> _Note_: External calls to `filter_block_tree` (i.e., any calls that are not made by the recursive logic in this function) MUST set `block_root` to `store.justified_checkpoint`.

The only external call to `filter_block_tree()` comes from [`get_filtered_block_tree()`](#get_filtered_block_tree), which uses `store.justified_checkpoint.root`. So we're all good. This is a requirement of [Hybrid LMD GHOST](#hybrid-lmd-ghost) - it enforces Casper FFG's fork choice rule.

```python
def filter_block_tree(store: Store, block_root: Root, blocks: Dict[Root, BeaconBlock]) -> bool:
    block = store.blocks[block_root]
    children = [
        root for root in store.blocks.keys()
        if store.blocks[root].parent_root == block_root
    ]

    # If any children branches contain expected finalized/justified checkpoints,
    # add to filtered block-tree and signal viability to parent.
    if any(children):
        filter_block_tree_result = [filter_block_tree(store, child, blocks) for child in children]
        if any(filter_block_tree_result):
            blocks[block_root] = block
            return True
        return False

    current_epoch = compute_epoch_at_slot(get_current_slot(store))
    voting_source = get_voting_source(store, block_root)

    # The voting source should be at the same height as the store's justified checkpoint
    correct_justified = (
        store.justified_checkpoint.epoch == GENESIS_EPOCH
        or voting_source.epoch == store.justified_checkpoint.epoch
    )

    # If the previous epoch is justified, the block should be pulled-up. In this case, check that unrealized
    # justification is higher than the store and that the voting source is not more than two epochs ago
    if not correct_justified and is_previous_epoch_justified(store):
        correct_justified = (
            store.unrealized_justifications[block_root].epoch >= store.justified_checkpoint.epoch and
            voting_source.epoch + 2 >= current_epoch
        )

    finalized_slot = compute_start_slot_at_epoch(store.finalized_checkpoint.epoch)
    correct_finalized = (
        store.finalized_checkpoint.epoch == GENESIS_EPOCH
        or store.finalized_checkpoint.root == get_ancestor(store, block_root, finalized_slot)
    )
    # If expected finalized/justified, add to viable block-tree and signal viability to parent.
    if correct_justified and correct_finalized:
        blocks[block_root] = block
        return True

    # Otherwise, branch not viable
    return False
```

The `filter_block_tree()` function is at the heart of how LMD GHOST and Casper FFG are bolted together.

The basic structure is fairly simple. Given a block, `filter_block_tree()` recursively walks the Store's block tree visiting the block's descendants in depth-first fashion. When it arrives at a leaf block (the tip of a branch), if the leaf block is "viable" as head then it and all its ancestors (the whole branch) will be added to the `blocks` list, otherwise the branch will be ignored.

In other words, the algorithm prunes out branches that terminate in an unviable head block, and keeps branches that terminate in a viable head block.

<a id="img_annotated_viable_nonviable"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing the pruning of nonviable branches.](images/diagrams/annotated-forkchoice-viable-nonviable.svg)

<figcaption>

Block $J$ is the Store's justified checkpoint. There are four candidate head blocks descended from it. Two are viable ($V$), and two are nonviable ($NV$). Blocks in branches terminating at viable heads are returned by the filter; blocks in branches terminating at nonviable heads are filtered out.

</figcaption>
</figure>

##### Viability

What dictates whether a leaf block is viable or not?

Pre-Capella, there was a fairly straightforward requirement for a leaf block to be a viable head block: viable head blocks had a post-state that agreed with the Store about the justified and finalised checkpoints. This was encapsulated in the following code from the [Bellatrix spec](/../bellatrix/part3/forkchoice/phase0/#filter_block_tree),

```none
    correct_justified = (
        store.justified_checkpoint.epoch == GENESIS_EPOCH
        or head_state.current_justified_checkpoint == store.justified_checkpoint
    )
    correct_finalized = (
        store.finalized_checkpoint.epoch == GENESIS_EPOCH
        or head_state.finalized_checkpoint == store.finalized_checkpoint
    )
    # If expected finalized/justified, add to viable block-tree and signal viability to parent.
    if correct_justified and correct_finalized:
        blocks[block_root] = block
        return True
```

The code we have in the Capella update is considerably more complex and less intuitive. But before we get to that we need to take a step back and discuss why we should filter the block tree at all.

##### Why prune unviable branches?

Filtering the block tree like this ensures that the Casper FFG fork choice rule, "follow the chain containing the justified checkpoint of the greatest height", is applied to the block tree before the LMD GHOST fork choice is evaluated.

Very early versions of the spec considered the tip of any branch descended from the Store's justified checkpoint as a potential head block. However, a scenario [was identified](https://notes.ethereum.org/Fj-gVkOSTpOyUx-zkWjuwg?view) in which this could result in a deadlock, in which finality would not be able to advance without validators getting themselves slashed - a kind of liveness failure[^fn-plausible-liveness].

[^fn-plausible-liveness]: This scenario doesn't strictly break Casper FFG's "plausible liveness" property as, in principle, voters can safely ignore the LMD GHOST fork choice and switch back to the original chain in order to advance finality. But it does create a conflict between the LMD GHOST fork choice rule and advancing finality.

The `filter_block_tree()` function was [added](https://github.com/ethereum/consensus-specs/pull/1495) as a fix for this issue. Given a Store and a block root, `filter_block_tree()` returns the list of all the blocks that we know about in the tree descending from the given block, having pruned out any branches that terminate in a leaf block that is not viable in some sense.

To illustrate the problem, consider the situation shown in the following diagrams, based on the [original description](https://notes.ethereum.org/Fj-gVkOSTpOyUx-zkWjuwg?view) of the issue. The context is that there is an adversary controlling 18% of validators that takes advantage of (or causes) a temporary network partition. We will illustrate the issue mostly in terms of checkpoints, and omit the intermediate blocks that carry the attestations - you can mentally insert these as necessary.

We begin with a justified checkpoint $A$ that all nodes agree on.

Due to the network partition, only 49% of validators, plus the adversary's 18%, see checkpoint $B$. They all make Casper FFG votes $[A \rightarrow B]$, thereby justifying $B$. A further checkpoint $C_1$ is produced on this branch, and the 49% that are honest validators dutifully make the Casper FFG vote $[B \rightarrow C_1]$, but the adversary does not, meaning that $C_1$ is not justified. Validators on this branch see $h_1$ as the head block, and have a highest justified checkpoint of $B$.

<a id="img_annotated_forkchoice_filter_0"></a>
<figure class="diagram" style="width: 90%">

![A diagram illustrating the first step in a liveness attack on the unfiltered chain, making the first branch.](images/diagrams/annotated-forkchoice-filter-0.svg)

<figcaption>

The large blocks represent checkpoints. After checkpoint $A$ there is a network partition: 49% of validators plus the adversary see checkpoints $B$ and $C_1$. Casper votes are shown by the dashed arrows. The adversary votes for $B$, but not for $C_1$.

</figcaption>
</figure>

The remaining 33% of validators do not see checkpoint $B$, but see $C_2$ instead and make Casper FFG votes $[A \rightarrow C_2]$ for it. But this is not enough votes to justify $C_2$. Checkpoint $D_2$ is produced on top of $C_2$, and a further block $h_2$. On this branch, $h_2$ is the head of the chain according to LMD GHOST, and $A$ remains the highest justified checkpoint.

<a id="img_annotated_forkchoice_filter_1"></a>
<figure class="diagram" style="width: 90%">

![A diagram illustrating the second step in a liveness attack on the unfiltered chain, making the second branch.](images/diagrams/annotated-forkchoice-filter-1.svg)

<figcaption>

Meanwhile, the remaining 33% of validators do not see the branch starting at $B$, but start a new branch containing $C_2$ and its descendants. They do not have enough collective weight to justify any of the checkpoints.

</figcaption>
</figure>

Now for the cunning part. The adversary switches its LMD GHOST vote (and implicitly its Casper FFG vote, although that does not matter for this exercise) from the first branch to the second branch, and lets the validators in the first branch see the blocks and votes on the second branch.

Block $h_2$ now has votes from the majority of validators &ndash; 33% plus the adversary's 18% &ndash; so all honest validators should make it their head block.

However, the justified checkpoint on the $h_2$ branch remains at $A$. This means that the 49% of validators who made Casper FFG vote $[B \rightarrow C]$ _cannot_ switch their chain head from $h_1$ to $h_2$ without committing a Casper FFG surround vote, and thereby getting slashed. Switching branch would cause their highest justified checkpoint to go backwards. Since they have previously voted $[B \rightarrow C_1]$, they cannot now vote $[A \rightarrow X]$ where $X$ has a height greater than $C_1$, which they must do if they were to switch to the $h_2$ branch.

<a id="img_annotated_forkchoice_filter_2"></a>
<figure class="diagram" style="width: 90%">

![A diagram illustrating the third step in a liveness attack on the unfiltered chain, changing the chain head.](images/diagrams/annotated-forkchoice-filter-2.svg)

<figcaption>

The adversary switches to the second branch, giving $h_2$ the majority LMD GHOST vote. This deadlocks finalisation: the 49% who made Casper FFG vote $[B \rightarrow C_1]$ cannot switch to $h_2$ without being slashed.

</figcaption>
</figure>

In conclusion, the chain can no longer finalise (by creating higher justified checkpoints) without a substantial proportion of validators (at least 16%) being willing to get themselves slashed.

It should never be possible for the chain to get into a situation in which honest validators, following the rules of the protocol, end up in danger of being slashed. The situation here arises due to a conflict between the Casper FFG fork choice (follow the chain containing the justified checkpoint of the greatest height) and the LMD GHOST fork choice (which, in this instance, ignores that rule). It is a symptom of the clunky way in which the two have been bolted together.

The chosen fix for all this is to filter the block tree before applying the LMD GHOST fork choice, so as to remove all "unviable" branches from consideration. That is, all branches whose head block's state does not agree with me about the current state of justification and finalisation.

<a id="img_annotated_forkchoice_filter_3"></a>
<figure class="diagram" style="width: 90%">

![A diagram showing that filter block tree prunes out the conflicting branch for validators following the first branch.](images/diagrams/annotated-forkchoice-filter-3.svg)

<figcaption>

When validators that followed branch 1 apply `filter_block_tree()`, branch 2 is pruned out (as indicated by the dashed lines). This is because their Store has $B$ as the best justified checkpoint, while branch 2's leaf block has a state with $A$ as the justified checkpoint. For these validators $h_2$ is no longer a candidate head block.

</figcaption>
</figure>

With this fix, the chain will recover the ability to finalise when the validators on the second branch eventually become aware of the first branch. On seeing $h_1$ and its ancestors, they will update their Stores' justified checkpoints to $B$ and mark the $h_2$ branch unviable.

##### Unrealised justification

A major feature of the Capella [update to the fork choice specification](https://github.com/ethereum/consensus-specs/pull/3290) is the logic for handling "unrealised justification" when filtering the block tree.

Several issues had arisen in the [former fork choice spec](/../bellatrix/part3/forkchoice/phase0/). First, an [unrealised justification reorg](https://notes.ethereum.org/@adiasg/unrealized-justification) attack that allowed the proposer of the first block of an epoch to easily fork out up to nine blocks from the end of the previous epoch. A variant of that attack was also found to be able to cause validators to make slashable attestations - the very issue the filter is intended to prevent. Second, a [justification withholding attack](https://hackmd.io/o9tGPQL2Q4iH3Mg7Mma9wQ) that an adversary could use to reorg arbitrary numbers of blocks at the start of an epoch.

The root issue is that, within the consensus layer's state transition, the calculations that update justification and finality are done only at epoch boundaries. An adversary had a couple of ways they could use this to filter out competing branches within `filter_block_tree()`. Essentially, in not accounting for unrealised justifications, the filtering was being applied too aggressively.

To be clear, both of the attacks described here apply to the [old version of `filter_block_tree()`](/../bellatrix/part3/forkchoice/phase0/#filter_block_tree) and have been remedied in the current release. This is the old, much simpler, code for evaluating `correct_justified` and `correct_finalized`,

```none
    correct_justified = (
        store.justified_checkpoint.epoch == GENESIS_EPOCH
        or head_state.current_justified_checkpoint == store.justified_checkpoint
    )
    correct_finalized = (
        store.finalized_checkpoint.epoch == GENESIS_EPOCH
        or head_state.finalized_checkpoint == store.finalized_checkpoint
    )
```

This meant that the tip of a branch was included for consideration if (a) the justified checkpoint in its post-state matched that in the store, and (b) the finalised checkpoint in its post-state matched that in the store. These nice simple criteria have been changed to the mess we have today, which we'll look at in a moment. But first, let's see what was wrong with the old criteria.

###### Unrealised justification reorg

The [unrealised justification reorg](https://notes.ethereum.org/@adiasg/unrealized-justification) allowed an adversary assigned to propose a block in the first slot of an epoch to reorg out a chain of up to nine blocks at the end of the previous epoch.

The key to this is the idea of _unrealised justification_. Towards the end of an epoch (within the last third of an epoch, that is, the last nine slots), the beacon chain might have gathered enough Casper FFG votes to justify the checkpoint at the start of that epoch. However, justification and finalisation calculations take place only at epoch boundaries, so the achieved justification is "unrealised": until the end of the epoch, all the blocks will continue have a post-state justification that points to an earlier checkpoint.

<a id="img_annotated_forkchoice_unrealised_justification-reorg_0"></a>
<figure class="diagram" style="width: 90%">

![A diagram showing the setup for an unrealised justification reorg scenario.](images/diagrams/annotated-forkchoice-unrealised-justification-reorg-0.svg)

<figcaption>

The solid vertical lines are epoch boundaries, and the squares $C_1$ and $C_2$ are their checkpoints. A block's $J$ value shows the justified checkpoint in its post-state. Its $U$ value is the hypothetical unrealised justification. During an epoch, the chain may gather enough Casper FFG votes to justify a new checkpoint, but justification in the beacon state happens only at epoch boundaries, so it is unrealised in the interim. Block $Y$ is clearly the head block.

</figcaption>
</figure>

When the adversary is the proposer in the first slot of an epoch, it could have used the unrealised justification in the previous epoch to fork out the last blocks of that epoch - up to around nine of them, depending on the FFG votes the adversary's block contains. By building a competing head block the adversary could trick `filter_block_tree()` into filtering out the previous head branch from consideration.

<a id="img_annotated_forkchoice_unrealised_justification_reorg_1"></a>
<figure class="diagram" style="width: 90%">

![A diagram showing how the adversary executes the unrealised justification reorg.](images/diagrams/annotated-forkchoice-unrealised-justification-reorg-1.svg)

<figcaption>

The adversary adds a block $Z$ in the first slot of the next epoch. It builds on $W$, which has unrealised justification. At the epoch boundary, the state's justified checkpoint is calculated, so $W$'s post-state has $C_2$. In the former fork choice, only branches with tips that agreed with the Store about the justified checkpoint could be considered. On that basis, the branch ending in $Y$ would have been excluded by the filter, making $Z$ the head, even though it might have zero LMD GHOST support. Blocks $X$ and $Y$ would be orphaned (reorged out).

</figcaption>
</figure>

###### Unrealised justification deadlock

It also became apparent that the pre-Capella formulation of `filter_block_tree()` did not fully prevent the possibility of deadlock - the very thing that filtering the block tree is [intended to prevent](#why-prune-unviable-branches). Deadlock is a situation in which honest validators are forced to choose between making a slashable attestation or not voting at all.

The setup and attack is described in [Aditya Asgaonkar's document](https://notes.ethereum.org/@adiasg/unrealized-justification), and is a variant of the reorg above. The original deadlock attack relied on the network being partitioned, so that validators had split views. This newer deadlock attack did not need a network partition, but under some fairly specific unrealised justification conditions, the adversary could make the justified checkpoint go backwards. Doing this after some honest validators had already used the higher checkpoint as their Casper FFG source vote forced them subsequently to either make a surround vote, or not to vote.

###### Justification withholding attack

The [justification withholding attack](https://hackmd.io/o9tGPQL2Q4iH3Mg7Mma9wQ) was similar to the [unrealised justification reorg](#unrealised-justification-reorg) in that it involved using unrealised justification to get `filter_block_tree()` to exclude other branches besides the adversary's.

In this attack, the adversary needs to have several proposals in a row at the end of an epoch - enough that, if the adversary does not publish the blocks, the epoch does not get justified by the time it finishes. That is, without the adversary's blocks, there is no unrealised justification, with the adversary's blocks there would be unrealised justification.

<a id="img_annotated_forkchoice_justification_withholding_0"></a>
<figure class="diagram" style="width: 90%">

![A diagram showing how an adversary sets up a justification withholding attack.](images/diagrams/annotated-forkchoice-justification-withholding-0.svg)

<figcaption>

The adversary has a string of proposals at the end of an epoch. These blocks contain enough FFG votes to justify the epoch's checkpoint $C_2$, but the adversary withholds them for now.

</figcaption>
</figure>

The remainder of the chain is unaware of the adversary's blocks, so continues to build as if they were skipped slots.

<a id="img_annotated_forkchoice_justification_withholding_1"></a>
<figure class="diagram" style="width: 90%">

![A diagram showing the chain progressing after the setup of the justification withholding attack, but before its execution.](images/diagrams/annotated-forkchoice-justification-withholding-1.svg)

<figcaption>

The remainder of the validators continue to build blocks $A$ and $B$ at the start of the next epoch. Without the adversary's blocks, checkpoint 2 is not justified, so $A$ and $B$ have $C_1$ as justified in their post-states (their unrealised justification is irrelevant here). Block $B$ is the chain's head.

</figcaption>
</figure>

The adversary has a block proposal at some point in epoch 3 - it does not matter when.

<a id="img_annotated_forkchoice_justification_withholding_2"></a>
<figure class="diagram" style="width: 90%">

![A diagram showing the execution of the justification withholding attack.](images/diagrams/annotated-forkchoice-justification-withholding-2.svg)

<figcaption>

When the adversary publishes block $Z$ in epoch 3, it releases its withheld blocks at the same time. Block $Z$ has a post-state justified checkpoint of $C_2$ (updated at the epoch boundary). Under the old `filter_block_tree()` that would have excluded $B$ from consideration as head, and the adversary's block $Z$ would have become head, even with no LMD support.

</figcaption>
</figure>

##### Viable and unviable branches

The block tree filtering is done by checking that blocks at the tip of branches have the "correct" justification and finalisation in some sense. Both the `correct_justified` and `correct_finalised` flags must be true for the branch to be considered viable.

###### `correct_justified`

The newer, more complex, `correct_justified` evaluation from the Capella update is as follows.

```none
    current_epoch = compute_epoch_at_slot(get_current_slot(store))
    voting_source = get_voting_source(store, block_root)

    # The voting source should be at the same height as the store's justified checkpoint
    correct_justified = (
        store.justified_checkpoint.epoch == GENESIS_EPOCH
        or voting_source.epoch == store.justified_checkpoint.epoch # A
    )

    # If the previous epoch is justified, the block should be pulled-up. In this case, check that unrealized
    # justification is higher than the store and that the voting source is not more than two epochs ago
    if not correct_justified and is_previous_epoch_justified(store): # B
        correct_justified = (
            store.unrealized_justifications[block_root].epoch >= store.justified_checkpoint.epoch and # C
            voting_source.epoch + 2 >= current_epoch # D
        )
```

I'm not going to pretend that I understand this fully - it seems very far from intuitive, and its correctness far from obvious[^fn-fc-fv]. For now I will quote some explanation that Aditya shared with me directly.

[^fn-fc-fv]: Some evidence for how challenging the fork choice is to reason about is provided by the [formal proofs of correctness](https://docs.google.com/document/d/1PnhDMij6w_fjLGicSF-I9sQcSWgaj5fjtGlRPIgYnVA/edit) created by Roberto Saltini and team at Consensys. [One of the proofs alone](https://docs.google.com/document/d/1V0sabk-DKnIl3BKgt-GkEFSFq-K-Ie8gmuezHRnVn0c/edit#) is 28 pages long when printed.

> The `correct_justified` condition ensures that:
> (a) we pick a head block that has a "good" justification in its chain, and
> (b) that validators can vote for the chosen head block without the risk of creating slashable messages.
>
> I describe (a) here informally, but you can think of the fork choice as a heuristic to choose where to vote so that we advance our finalized checkpoint as fast as possible. Generally, this means we vote on the highest justified checkpoint we know of, but the "that we know of" part is tricky because of the nuances of unrealized justifications and the associated reorg attacks.
>
> Now, if you ignore unrealized justification and the reorg attacks, this first appearance of `correct_justified` [Line `A`] is sufficient to address (a) & (b). Then, to fix the reorg attacks, we add an additional version of `correct_justified` for pulled-up blocks, where the first line [line `C`] addresses (a) and the second line [line `D`] addresses (b).

Recall that the chief goal of filtering the block tree is to avoid honest validators being forced to make surround votes in Casper FFG, as these are slashable. However, earlier remedies for this were too eager in filtering out candidate heads, and an adversary could use unrealised justification to force reorgs, and even the kind of self-slashing we want to avoid.

The current fork choice apparatus preserves two key properties[^fn-fc-mikhail].

[^fn-fc-mikhail]: I am grateful to Mikhail Kalinin for helping me with his very lucid and careful explanation. My explanation is based on his; any errors and over-simplifications are all my own work.

1. The `voting_source.epoch` of a candidate head block is always less than or equal to the `store.justified_checkpoint.epoch`.
   - This is because [`on_block()`](#on_block) always calls [`update_checkpoints()`](#update_checkpoints), and the way that the [`get_voting_source()`](#get_voting_source) function is constructed.
   - The `voting_source.epoch` is the Casper FFG source vote that validators with that block as head will use when making a Casper FFG vote in the current epoch.
2. `store.justified_checkpoint.epoch` never decreases (it is monotonically increasing).
   - It is only ever written to in [`update_checkpoints()`](#update_checkpoints), and it is easy to see from there that this is true.

With respect to the line I've marked `A` in the source code, if the voting source matches the Store's justified checkpoint, then all is good, we have no reason not to consider the block as head, and we can short-circuit the rest of the logic. By property 2, the Store's justified checkpoint never decreases, so a validator's Casper FFG vote based on this cannot ever be a surround vote. (Since the target vote strictly increases, it cannot be a surrounded vote either.)

<a id="img_annotated_forkchoice_correct_justified_0"></a>
<figure class="diagram" style="width: 85%">

![A diagram showing that it is always safe to vote when the voting source is the same as the store's justified checkpoint.](images/diagrams/annotated-forkchoice-correct-justified-0.svg)

<figcaption>

When the voting source is the same as the store's justified checkpoint, it is always safe to vote. The store's justified checkpoint never decreases, so we cannot commit a surround vote.

</figcaption>
</figure>

If the block is not a viable head by the first criterion, it might still be a viable head by the second (line `D`). Recall that the reorg attacks above rely on the adversary taking advantage of unrealised justification to update the Store's justified checkpoint, leaving the previous head of the chain "stale" with respect to its realised justification, although based on its unrealised justification it would still be viable. To avoid this, we wish to safely include as many viable heads as possible.

We know that any Casper FFG vote we make at this point will have `voting_source.epoch` strictly less than `store.justified_checkpoint.epoch`, by property 1, and since we already dealt with the equality case.

Line `D` in the source code says that it is safe to allow votes where `voting_source.epoch == current_epoch - 2` and `voting_source.epoch == current_epoch - 1`. Any honest vote's target epoch will be `current_epoch`, so the gap between source and target is at most two epochs, which is not enough to surround a previous vote. That is, a vote $[s_2 \rightarrow t_2]$ that surrounds a previous vote $[s_1 \rightarrow t_1]$ requires that $s_2 < s_1 < t_1 < t_2$. This is not possible if $t_2 - s_2 \le 2$. This exception is required for the analysis of the [safe block confirmation rule](/part3/safe-block/), and is discussed in section 3.3 of the [Confirmation Rule paper](https://ethresear.ch/uploads/short-url/fV7zyTggJtMn8xlUUNVXTOB532G.pdf).

<a id="img_annotated_forkchoice_correct_justified_1"></a>
<figure class="diagram" style="width: 85%">

![A diagram showing that it is safe to vote from two epochs ago.](images/diagrams/annotated-forkchoice-correct-justified-1.svg)

<figcaption>

When the voting source is within two epochs of the current epoch then it is safe to vote as a surround vote must encompass at least two intervening checkpoints.

</figcaption>
</figure>

Both line `B` (the condition on justification of the previous epoch) and line `C` seem to be unnecessary, and may be [removed in future](https://github.com/ethereum/consensus-specs/pull/3339/files#diff-f58759020c0d7542f963e2b88fef66c2179d740beb5af45b7e2c90913477be11L277-L290). See sections 3.3 and 3.4 of the (draft) [A Confirmation Rule for the Ethereum Consensus Protocol](https://ethresear.ch/uploads/short-url/fV7zyTggJtMn8xlUUNVXTOB532G.pdf) paper for discussion of this. Note, though, that the proof of the [honest viable head](https://docs.google.com/document/d/1riyJxPPCuTwxmpKWqUD9noGMPB5FtvUW81JN9kaUQBo/edit) property described [below](#formal-proofs) relies on the weaker condition that `store.justified_checkpoint.epoch >= current_epoch - 2` (that either the previous or previous but one epoch is justified).

###### `correct_finalized`

The Capella update's change to `correct_finalized` is more limited, and more intuitive.

```none
    finalized_slot = compute_start_slot_at_epoch(store.finalized_checkpoint.epoch)
    correct_finalized = (
        store.finalized_checkpoint.epoch == GENESIS_EPOCH
        or store.finalized_checkpoint.root == get_ancestor(store, block_root, finalized_slot)
    )
```

This simply ensures that a viable block is descended from the Store's finalised checkpoint

The [previous version](https://github.com/ethereum/consensus-specs/blob/v1.2.0/specs/phase0/fork-choice.md?plain=1#L231-L234) made use the post-state of the block being considered for viability, which caused it to suffer from similar complications to `correct_justified` with respect to unrealised finality. We don't actually care about the block's post-state view of finalisation, since finality is a global property: as long as the block is descended from the finalised checkpoint it should be eligible for consideration to become viable.

The `correct_finalized` check might appear redundant at first sight since we are always filtering based on a tree rooted at the last justified checkpoint, which (in the absence of a mass slashing) must be descended from the last finalised checkpoint. The check, however, guarantees the further condition that a node's own view of the finalised checkpoint is irreversible, even if there were to be a mass slashing - this is the [irreversible local finality](https://docs.google.com/document/d/194dC7UmDSY5pDsrqi2B4VWpF1ORCkftHXhmtxtEaHT4/edit) formal property in the next section. Maintaining this invariant is a huge help in building efficient clients - we are free to forget about everything prior to the last finalised checkpoint.

###### Formal proofs

As referenced in the Ethereum Foundation's [disclosure](https://notes.ethereum.org/@djrtwo/2023-fork-choice-reorg-disclosure) on the Capella fork choice spec updates, some properties of the new fork choice have been [formally verified](https://docs.google.com/document/d/1PnhDMij6w_fjLGicSF-I9sQcSWgaj5fjtGlRPIgYnVA/edit) by Roberto Saltini and his team at Consensys.

This formal verification process involves selecting some properties that we wish to be true for the fork choice, and manually constructing proofs that they are always preserved by the specification. It is a much more robust and rigorous process than manual testing, fuzz testing, or general hand-waving that have hitherto been the main approaches.

Four properties were proved in [the documents](https://docs.google.com/document/d/1PnhDMij6w_fjLGicSF-I9sQcSWgaj5fjtGlRPIgYnVA/edit).

  - [Non-Self-Slashability](https://docs.google.com/document/d/12dF-84w6G62KH68L2dO_ym7RGXEZ0k9d7mOksgzp3Ls/edit)
    - Any validator that follows the [honest validator guide](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md) will never slash itself.
  - [Honest Viable Head](https://docs.google.com/document/d/1riyJxPPCuTwxmpKWqUD9noGMPB5FtvUW81JN9kaUQBo/edit)
    - Assuming a synchronous network, `current_epoch - 2` being justified, and honest nodes having received enough attestations to justify the highest possible justified checkpoint, any block proposed by an honest node during the current epoch that is a descendent of the highest possible justified checkpoint is included in the output of [`get_filtered_block_tree()`](#get_filtered_block_tree).
  - [Deadlock Freedom](https://docs.google.com/document/d/1V0sabk-DKnIl3BKgt-GkEFSFq-K-Ie8gmuezHRnVn0c/edit)
    - A distributed system running the Ethereum protocol can never end up in a state in which it is impossible to finalise a new epoch.
  - [Irreversible Local Finality](https://docs.google.com/document/d/194dC7UmDSY5pDsrqi2B4VWpF1ORCkftHXhmtxtEaHT4/edit)
    - A block, once finalized in the local view of an honest validator, is never reverted from the canonical chain in the local view.

Given the complexity of reasoning about the fork choice, and it's rather chequered history, it is hugely reassuring now to have [these proofs](https://docs.google.com/document/d/1PnhDMij6w_fjLGicSF-I9sQcSWgaj5fjtGlRPIgYnVA/edit) of correctness[^fn-fv-assumptions].

[^fn-fv-assumptions]: However, there remain some assumptions in the proofs that are over-simplified, such as, "No bound on the amount of attestations that can be included in a block", and, "Honest nodes do not discard any attestation that they receive regardless of how old it is". There is some history of failed proofs around the fork choice that were based on assumptions that were too broad. Hopefully these will stand; I am not equipped to judge.

##### Conclusion

This has been a long section on a short function. As I said at the start of the section, `filter_block_tree()` is at the heart of how LMD GHOST and Casper FFG are bolted together, and I think it has surprised everybody how many complexities lurk here.

As an exercise for the reader, we can imagine life without having to filter the block tree. Potuz has documented some thoughts on this in, [Fork choice without on-state FFG filtering](https://hackmd.io/@potuz/rkB25feBi).

Ultimately, as with [proposer boost](#proposer-boost), the complexities around the Gasper fork choice largely arise from our slot-based voting, with votes accumulated gradually through an epoch. This results in unrealised justification and the rest of it. The long-term fix is also probably the same: moving to [single slot finality](#alternatives-to-proposer-boost).

|||
|-|------|
| Used&nbsp;by | [`get_filtered_block_tree()`](#get_filtered_block_tree), [`filter_block_tree()`](#filter_block_tree) (recursively) |
| Uses | [`filter_block_tree()`](#filter_block_tree) (recursively), [`compute_epoch_at_slot()`](/part3/helper/misc/#def_compute_epoch_at_slot), [`get_voting_source()`](#get_voting_source), [`is_previous_epoch_justified()`](#is_previous_epoch_justified), [`compute_start_slot_at_epoch()`](/part3/helper/misc/#def_compute_start_slot_at_epoch), [`get_ancestor()`](#get_ancestor) |

#### `get_filtered_block_tree`

```python
def get_filtered_block_tree(store: Store) -> Dict[Root, BeaconBlock]:
    """
    Retrieve a filtered block tree from ``store``, only returning branches
    whose leaf state's justified/finalized info agrees with that in ``store``.
    """
    base = store.justified_checkpoint.root
    blocks: Dict[Root, BeaconBlock] = {}
    filter_block_tree(store, base, blocks)
    return blocks
```

A convenience wrapper that passes the Store's justified checkpoint to [`filter_block_tree()`](#filter_block_tree). On returning, the `blocks` dictionary structure will contain the blocks from all viable branches rooted at that checkpoint, and nothing that does not descend from that checkpoint. For the meaning of "viable", see [above](#viable-and-unviable-branches).

|||
|-|------|
| Used&nbsp;by | [`get_head()`](#get_head) |
| Uses | [`filter_block_tree()`](#filter_block_tree) |

#### `get_head`

```python
def get_head(store: Store) -> Root:
    # Get filtered block tree that only includes viable branches
    blocks = get_filtered_block_tree(store)
    # Execute the LMD-GHOST fork choice
    head = store.justified_checkpoint.root
    while True:
        children = [
            root for root in blocks.keys()
            if blocks[root].parent_root == head
        ]
        if len(children) == 0:
            return head
        # Sort by latest attesting balance with ties broken lexicographically
        # Ties broken by favoring block with lexicographically higher root
        head = max(children, key=lambda root: (get_weight(store, root), root))
```

`get_head()` encapsulates the fork choice rule: given a Store it returns a head block.

The fork choice rule is objective in that, given the same Store, it will always return the same head block. But the overall process is subjective in that each node on the network will tend to have a different view, that is, a different Store, due to delays in receiving attestations or blocks, or having seen different sets of attestations or blocks because of network asynchrony or an attack.

Looking first at the `while True` loop, this implements LMD GHOST in its purest form. Starting from a given block (which would be the genesis block in unmodified LMD GHOST), we find the weights of the children of that block. We choose the child block with the largest weight and repeat the process until we end up at a leaf block (the tip of a branch). That is, we Greedily take the Heaviest Observed Sub-Tree, GHOST. Any tie between two child blocks with the same weight is broken by comparing their block hashes, so we end up at a unique leaf block - the head that we return.

<a id="img_annotated_forkchoice_lmd_ghost_0"></a>
<figure class="diagram" style="width: 85%">

![Diagram of a block tree showing the weight of each block.](images/diagrams/annotated-forkchoice-lmd-ghost-0.svg)

<figcaption>

`get_head()` starts from the root block, $A$, of a block tree. The numbers show each block's weight, which is its latest attesting balance - the sum of the effective balances of the validators that cast their latest vote for that block. [Proposer boost](#proposer-boost) can temporarily increase the latest block's score (not shown).

</figcaption>
</figure>

<a id="img_annotated_forkchoice_lmd_ghost_1"></a>
<figure class="diagram" style="width: 85%">

![Diagram of a block tree showing the weight of each block and the weight of each subtree.](images/diagrams/annotated-forkchoice-lmd-ghost-1.svg)

<figcaption>

The `get_weight()` function when applied to a block returns the total weight of the subtree of the block and all its descendants. These weights are shown on the lines between child and parent blocks.

</figcaption>
</figure>

<a id="img_annotated_forkchoice_lmd_ghost_2"></a>
<figure class="diagram" style="width: 85%">

![Diagram of a block tree showing the branch chosen by the GHOST rule.](images/diagrams/annotated-forkchoice-lmd-ghost-2.svg)

<figcaption>

Given a block, the loop in `get_head()` considers its children and selects the one that roots the subtree with the highest weight. It repeats the process with the heaviest child block[^fn-get-head-recursive] until it reaches a block with no children. In this example, it would select the branch $A \leftarrow C \leftarrow E$, returning $E$ as the head block.

</figcaption>
</figure>

[^fn-get-head-recursive]: The algorithm is recursive, although it is not written recursively here.

##### Hybrid LMD GHOST

What we've just described is the pure LMD GHOST algorithm. Starting from the genesis block, it walks the entire block tree, taking the heaviest branch at each fork until it reaches a leaf block.

What is implemented in `get_head()` however, is a modified form of this that the [Gasper paper](https://arxiv.org/pdf/2003.03052.pdf)[^fn-hlmd-ghost-definition] refers to as "hybrid LMD GHOST" (HLMD GHOST). It is not pure LMD GHOST, but LMD GHOST modified by the Casper FFG consensus.

[^fn-hlmd-ghost-definition]: See section 4.6 of that paper.

```none
    # Get filtered block tree that only includes viable branches
    blocks = get_filtered_block_tree(store)
    # Execute the LMD-GHOST fork choice
    head = store.justified_checkpoint.root
```

Specifically, rather than starting to walk the tree from the genesis block, we start from the last justified checkpoint, and rather than considering all blocks that the Store knows about, we first filter out "unviable" branches with [`get_filtered_block_tree()`](#get_filtered_block_tree).

This is the point at which the Casper FFG fork choice rule, "follow the chain containing the justified checkpoint of the greatest height", meets the LMD GHOST fork choice rule. The former modifies the latter to give us the HLMD GHOST fork choice rule.

|||
|-|------|
| Uses | [`get_filtered_block_tree()`](#get_filtered_block_tree), [`get_weight()`](#get_weight) |

#### `update_checkpoints`

```python
def update_checkpoints(store: Store, justified_checkpoint: Checkpoint, finalized_checkpoint: Checkpoint) -> None:
    """
    Update checkpoints in store if necessary
    """
    # Update justified checkpoint
    if justified_checkpoint.epoch > store.justified_checkpoint.epoch:
        store.justified_checkpoint = justified_checkpoint

    # Update finalized checkpoint
    if finalized_checkpoint.epoch > store.finalized_checkpoint.epoch:
        store.finalized_checkpoint = finalized_checkpoint
```

Update the checkpoints in the store if either of the given justified or finalised checkpoints is newer.

Justification and finalisation are supposed to be "global" properties of the chain, not specific to any one branch, so we keep our Store up to date with the highest checkpoints we've seen.

Note that, by construction, the Store's justified and finalised checkpoints can only increase monotonically. The former is important for the [formal proof of non-self-slashability](#formal-proofs).

|||
|-|------|
| Used&nbsp;by | [`compute_pulled_up_tip()`](#compute_pulled_up_tip), [`on_tick_per_slot()`](#on_tick_per_slot), [`on_block()`](#on_block) |

#### `update_unrealized_checkpoints`

```python
def update_unrealized_checkpoints(store: Store, unrealized_justified_checkpoint: Checkpoint,
                                  unrealized_finalized_checkpoint: Checkpoint) -> None:
    """
    Update unrealized checkpoints in store if necessary
    """
    # Update unrealized justified checkpoint
    if unrealized_justified_checkpoint.epoch > store.unrealized_justified_checkpoint.epoch:
        store.unrealized_justified_checkpoint = unrealized_justified_checkpoint

    # Update unrealized finalized checkpoint
    if unrealized_finalized_checkpoint.epoch > store.unrealized_finalized_checkpoint.epoch:
        store.unrealized_finalized_checkpoint = unrealized_finalized_checkpoint
```

The counterpart to [`update_checkpoints()`](#update_checkpoints) for unrealised justified and finalised checkpoints.

|||
|-|------|
| Used&nbsp;by | [`compute_pulled_up_tip()`](#compute_pulled_up_tip) |

#### Pull-up tip helpers

##### `compute_pulled_up_tip`

```python
def compute_pulled_up_tip(store: Store, block_root: Root) -> None:
    state = store.block_states[block_root].copy()
    # Pull up the post-state of the block to the next epoch boundary
    process_justification_and_finalization(state)

    store.unrealized_justifications[block_root] = state.current_justified_checkpoint
    update_unrealized_checkpoints(store, state.current_justified_checkpoint, state.finalized_checkpoint)

    # If the block is from a prior epoch, apply the realized values
    block_epoch = compute_epoch_at_slot(store.blocks[block_root].slot)
    current_epoch = compute_epoch_at_slot(get_current_slot(store))
    if block_epoch < current_epoch:
        update_checkpoints(store, state.current_justified_checkpoint, state.finalized_checkpoint)
```

`compute_pulled_up_tip()` is called for every block processed in order to maintain the `update_unrealized_checkpoints` map. It was added in the Capella spec update.

The major work in this routine is in the call to [`process_justification_and_finalization()`](/part3/transition/epoch/#def_process_justification_and_finalization). In the state transition, this is called once per epoch. Now we are calling it once per block, which would add significant load if implemented naively.

Since the state transition only calls `process_justification_and_finalization()` at epoch boundaries, the beacon state's justification and finalisation information cannot change mid-epoch. However, Casper FFG votes accumulate throughout the progress of the epoch, and at some point prior to the end of the epoch, enough votes will usually have been included on chain to justify a new checkpoint. When this occurs, we call it "unrealised justification" since it is not yet reflected on chain (in the beacon state). Unrealised justification reflects what the beacon state would be if the end of epoch accounting were to be run immediately on the block - hence the "pulled up tip" naming.

We simulate "pulling up" the block to the next epoch boundary to find out what the justification and finalisation status would be. When the next epoch begins, the unrealised values will become realised.

<a id="img_annotated_forkchoice_pull_up_tip"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing how unrealised justification becomes realised when a block is "pulled up" to the next epoch.](images/diagrams/annotated-forkchoice-pull-up-tip.svg)

<figcaption>

Each block has a $J$ value, which is the justified checkpoint its post-state knows about. $J$ is updated only at epoch boundaries. We notionally add a $U$ value, which is the justified checkpoint that would be in the block's post-state were it to be "pulled up" to the next epoch boundary, where the beacon state's justification and finalisation calculations are done (shown as $W'$).

</figcaption>
</figure>

As illustrated, the unrealised justification, $U$, can differ from the realised justification, $J$, due to unprocessed Casper FFG votes accumulating during an epoch. However, an epoch's own checkpoint cannot gain unrealised justification until at least 2/3 of the way through an epoch (23 slots, since attestations are included one slot after the slot they attest to). That is, the earliest that block $W$ could occur is in slot 22 of epoch 2 (slot counting is zero-based).

After adding the block and its unrealised justification checkpoint to the `store.unrealized_justifications` map, the Store's `unrealized_justified_checkpoint` and `unrealized_finalized_checkpoint` are updated if the block's values are newer.

If the block is from a previous epoch, then its justification and finalisation are no longer unrealised, since the beacon state has gone through an actual epoch transition since then, so we can update the Store's `justified_checkpoint` and `finalized_checkpoint` if the block has newer ones.

|||
|-|------|
| Used&nbsp;by | [`on_block()`](#on_block) |
| Uses | [`process_justification_and_finalization()`](/part3/transition/epoch/#def_process_justification_and_finalization), [`update_unrealized_checkpoints()`](#update_unrealized_checkpoints), [`compute_epoch_at_slot()`](/part3/helper/misc/#def_compute_epoch_at_slot), [`update_checkpoints()`](#update_checkpoints) |

#### `on_tick` helpers

##### `on_tick_per_slot`

```python
def on_tick_per_slot(store: Store, time: uint64) -> None:
    previous_slot = get_current_slot(store)

    # Update store time
    store.time = time

    current_slot = get_current_slot(store)

    # If this is a new slot, reset store.proposer_boost_root
    if current_slot > previous_slot:
        store.proposer_boost_root = Root()

    # If a new epoch, pull-up justification and finalization from previous epoch
    if current_slot > previous_slot and compute_slots_since_epoch_start(current_slot) == 0:
        update_checkpoints(store, store.unrealized_justified_checkpoint, store.unrealized_finalized_checkpoint)
```

The `on_tick_per_slot()` helper is called at least once every slot. If a tick hasn't been processed for multiple slots, then the [`on_tick()`](#on_tick) handler calls it repeatedly to process (synthetic) ticks for the those slots. This ensures that `update_checkpoints()` is called when there is no tick processed during the first slot of an epoch.

The `on_tick_per_slot()` helper has three duties,

  - updating the time,
  - resetting proposer boost, and
  - updating checkpoints on epoch boundaries.

###### Updating the time

```none
    # update store time
    store.time = time
```

The store has a notion of the current time that is used when calculating the [current slot](#get_current_slot) and when applying proposer boost. The time parameter does not need to be very granular. If it weren't for proposer boost, it would be fine to measure time in whole slots, at least within the fork choice[^fn-time-in-slots].

[^fn-time-in-slots]: Changing `time` from seconds to slots in the fork choice has been [suggested](https://github.com/ethereum/consensus-specs/issues/1502), but never adopted.

###### Resetting proposer boost

```none
    # Reset store.proposer_boost_root if this is a new slot
    if current_slot > previous_slot:
        store.proposer_boost_root = Root()
```

[Proposer boost](#proposer-boost) is a defence against balancing attacks on LMD GHOST. It rewards timely blocks with extra weight in the fork choice, making it unlikely that an honest proposer's block will become orphaned.

The Store's `proposer_boost_root` field is set in the [`on_block()`](#on_block) handler when a block is received and processed in a timely manner (within the first four seconds of its slot). For the remainder of the slot this allows extra weight to be added to the block in [`get_weight()`](#get_weight).

The logic here resets `proposer_boost_root` to a default value at the start of the next slot, thereby removing the extra proposer boost weight until the next timely block is processed.

###### Updating checkpoints

```none
    # If a new epoch, pull-up justification and finalization from previous epoch
    if current_slot > previous_slot and compute_slots_since_epoch_start(current_slot) == 0:
        update_checkpoints(store, store.unrealized_justified_checkpoint, store.unrealized_finalized_checkpoint)
```

If it's the first slot of an epoch, then we have gone through an epoch boundary since the last tick, and our unrealised justification and finalisation have become realised. They should now be in-sync with the justified and finalised checkpoints in the beacon state.

|||
|-|------|
| Used&nbsp;by | [`on_tick()`](#on_tick) |
| Uses | [`get_current_slot()`](#get_current_slot), [`compute_slots_since_epoch_start()`](#compute_slots_since_epoch_start), [`update_checkpoints()`](#update_checkpoints) |

#### `on_attestation` helpers

##### `validate_target_epoch_against_current_time`

```python
def validate_target_epoch_against_current_time(store: Store, attestation: Attestation) -> None:
    target = attestation.data.target

    # Attestations must be from the current or previous epoch
    current_epoch = compute_epoch_at_slot(get_current_slot(store))
    # Use GENESIS_EPOCH for previous when genesis to avoid underflow
    previous_epoch = current_epoch - 1 if current_epoch > GENESIS_EPOCH else GENESIS_EPOCH
    # If attestation target is from a future epoch, delay consideration until the epoch arrives
    assert target.epoch in [current_epoch, previous_epoch]
```

This function simply checks that an attestation came from the current or previous epoch, based on its target checkpoint vote. The Store has a notion of the current time, maintained by the [`on_tick()`](#on_tick) handler, so it's a straightforward calculation. The timeliness check was introduced to defend against the "decoy flip-flop" attack [described below](#attestation-timeliness).

Note that there is a small inconsistency here. Attestations may be [included in blocks](/part3/transition/block/#attestations) only for 32 slots after the slot in which they were published. However, they are valid for consideration in the fork choice for two epochs, which is up to 64 slots.

|||
|-|------|
| Used&nbsp;by | [`validate_on_attestation()`](#validate_on_attestation) |
| Uses | [`get_current_slot()`](#get_current_slot), [`compute_epoch_at_slot()`](/part3/helper/misc/#def_compute_epoch_at_slot) |

##### `validate_on_attestation`

```python
def validate_on_attestation(store: Store, attestation: Attestation, is_from_block: bool) -> None:
    target = attestation.data.target

    # If the given attestation is not from a beacon block message, we have to check the target epoch scope.
    if not is_from_block:
        validate_target_epoch_against_current_time(store, attestation)

    # Check that the epoch number and slot number are matching
    assert target.epoch == compute_epoch_at_slot(attestation.data.slot)

    # Attestation target must be for a known block. If target block is unknown, delay consideration until block is found
    assert target.root in store.blocks

    # Attestations must be for a known block. If block is unknown, delay consideration until the block is found
    assert attestation.data.beacon_block_root in store.blocks
    # Attestations must not be for blocks in the future. If not, the attestation should not be considered
    assert store.blocks[attestation.data.beacon_block_root].slot <= attestation.data.slot

    # LMD vote must be consistent with FFG vote target
    target_slot = compute_start_slot_at_epoch(target.epoch)
    assert target.root == get_ancestor(store, attestation.data.beacon_block_root, target_slot)

    # Attestations can only affect the fork choice of subsequent slots.
    # Delay consideration in the fork choice until their slot is in the past.
    assert get_current_slot(store) >= attestation.data.slot + 1
```

This is a utility function for the [`on_attestation()`](#on_attestation) handler that collects together the various validity checks we want to perform on an attestation before we make any changes to the Store. Recall that a failed assertion means that the handler will exit and any changes made to the Store must be rolled back.

###### Attestation timeliness

```none
    # If the given attestation is not from a beacon block message, we have to check the target epoch scope.
    if not is_from_block:
        validate_target_epoch_against_current_time(store, attestation)
```

First, we check the attestation's timeliness. Newly received attestations are considered for insertion into the Store only if they came from the [current or previous epoch](#validate_target_epoch_against_current_time) at the time when we heard about them.

This check [was introduced](https://github.com/ethereum/consensus-specs/pull/1466) to defend against a ["decoy flip-flop attack"](https://ethresear.ch/t/decoy-flip-flop-attack-on-lmd-ghost/6001?u=benjaminion) on LMD GHOST. The attack depends on two competing branches having emerged due to some network failure. An adversary with some fraction of the stake (but less than 33%) can store up votes from earlier epochs and release them at carefully timed moments to switch the winning branch (according to the LMD GHOST fork choice) so that neither branch can gain the necessary 2/3 weight for finalisation. The attack can continue until the adversary runs out of stored votes.

Allowing only attestations from the current and previous epoch to be valid for updates to the Store seems to be an effective defence as it prevents the attacker from storing up attestations from previous epochs. The PR implementing this describes it as "FMD GHOST" (fresh message driven GHOST). However, the fork choice still relies on the latest message ("LMD") from each validator in the Store, no matter how old it is. We seem to have ended up with a kind of hybrid FMD/LMD GHOST in practice[^fn-lmd-fmd-ghost].

[^fn-lmd-fmd-ghost]: FMD vs LMD GHOST is discussed further in the Ethresear.ch article, [Saving strategy and FMD GHOST](https://ethresear.ch/t/saving-strategy-and-fmd-ghost/6226?u=benjaminion). [Later work](#alternatives-to-proposer-boost), such as the Goldfish protocol and RLMD GHOST, take vote-expiry further.

As for the `if not is_from_block` test, this allows the processing of old attestations by the `on_attestation` handler if they were received in a block. It seems to have been introduced to help with test generation rather than being anything required in normal operation. Here's a [comment from the PR](https://github.com/ethereum/consensus-specs/pull/2727#pullrequestreview-812756853) that introduced it.

> Also good to move ahead with processing old attestations from blocks for now - that's the only way to make atomic updates to the store work in our current testing setup. If that changes in the future, this logic should go through security analysis (esp. for flip-flop attacks).

Attestations are valid for inclusion in a block only if they are less than 32 slots old. These will be a subset of the "fresh" votes made at the time (the "current plus previous epoch" criterion for freshness could encompass as many as 64 slots).

###### Matching epoch and slot

```none
    # Check that the epoch number and slot number are matching
    assert target.epoch == compute_epoch_at_slot(attestation.data.slot)
```

This check addresses an [edge case](https://github.com/ethereum/consensus-specs/issues/1501) in which validators could fabricate votes for a prior or subsequent epoch. It's probably not a big issue for the fork choice, more for the beacon chain state transition accounting. Nevertheless, the check [was implemented](https://github.com/ethereum/consensus-specs/pull/1509) in both places.

###### No attestations for unknown blocks

```none
    # Attestations target be for a known block. If target block is unknown, delay consideration until the block is found
    assert target.root in store.blocks
    # Attestations must be for a known block. If block is unknown, delay consideration until the block is found
    assert attestation.data.beacon_block_root in store.blocks
```

This seems like a natural check - if we don't know about a block (either a target checkpoint or the head block), there's no point processing any votes for it. These conditions [were added](https://github.com/ethereum/consensus-specs/pull/1477) to the spec without further rationale. As noted in the comments, such attestations may become valid in future and should be reconsidered then. When they receive attestations for blocks that they don't yet know about, clients will typically ask their peers to send the block to them directly.

###### No attestations for future blocks

```none
    # Attestations must not be for blocks in the future. If not, the attestation should not be considered
    assert store.blocks[attestation.data.beacon_block_root].slot <= attestation.data.slot
```

This check [was introduced](https://github.com/ethereum/consensus-specs/pull/1477) alongside the above checks for unknown blocks. Allowing votes for blocks that were published later than the attestation's assigned slot [increases the feasibility of the decoy flip-flop attack](https://github.com/ethereum/consensus-specs/issues/1406) by removing the need to have had a period of network asynchrony to set it up.

###### LMD and FFG vote consistency

```none
    # LMD vote must be consistent with FFG vote target
    target_slot = compute_start_slot_at_epoch(target.epoch)
    assert target.root == get_ancestor(store, attestation.data.beacon_block_root, target_slot)
```

This check ensures that the block in the attestation's head vote descends from the block in its target vote.

The check [was introduced](https://github.com/ethereum/consensus-specs/pull/1742) to fix three issues that had come to light.

1. [Inconsistencies](https://github.com/ethereum/consensus-specs/issues/1408) between the fork choice's validation of attestations and the state transition's validation of attestations. The issue is that, if some attestations are valid with respect to the fork choice but invalid for inclusion in blocks, it is a potential source of differing network views between validators, and could impede fork choice convergence. Validators receive attestations both via attestation gossip and via blocks. Ideally, each of these channels will contain more or less the same information.[^fn-inconsistency-of-attestations-blocks-gossip]

2. [Attestations from incompatible forks](https://github.com/ethereum/consensus-specs/issues/1456). Since committee shufflings are decided only at the [start of the previous epoch](/part2/building_blocks/randomness/#lookahead), it can lead to implementation challenges when processing attestations where the target block is from a different fork. After a while, forks end up with different shufflings. Clients often cache shufflings and it can be a source of bugs having to handle these edge cases. This check removes any ambiguity over the state to be used when validating attestations. It also prevents validators exploiting the ability to [influence their own committee assignments](https://github.com/ethereum/consensus-specs/issues/1636) in the event of multiple forks.

3. [Faulty or malicious validators](https://github.com/ethereum/consensus-specs/issues/1636) shouldn't be able to influence fork choice via exploiting this inconsistency. An attestation that fails this test would not have been produced by a correctly operating, honest validator. Therefore it is safest to ignore it.

[^fn-inconsistency-of-attestations-blocks-gossip]: One such inconsistency remains: attestations are [valid in gossip](#validate_target_epoch_against_current_time) for up to two epochs, but for only 32 slots in blocks.

###### Only future slots

```none
    # Attestations can only affect the fork choice of subsequent slots.
    # Delay consideration in the fork choice until their slot is in the past.
    assert get_current_slot(store) >= attestation.data.slot + 1
```

This criterion is discussed in section 8.4 of the [Gasper paper](https://arxiv.org/abs/2003.03052): only attestations from slots up to and including slot $N-1$ may appear in the Store at slot $N$.

If attestations were included in the Store as soon as being received, an adversary with a number of dishonest validators could use that to probabilistically split the votes of honest validators. The dishonest validators would attest early in the slot, dividing their votes between competing head blocks. Due to network delays, when honest validators run their own fork choices prior to attesting at the proper time, they are likely to see different weights for each of the candidates, based on the subset of dishonest attestations they have received by then. In which case the vote of the honest validators could end up being split. This might keep the chain from converging on a single head block.

Introducing this one slot lag for considering attestations makes it much more likely that honest validators will all vote for the same head block in slot $N$, as they will have all seen a similar set of attestations up to slot $N-1$, and cannot be influenced by an adversary's early attestations in the current slot.

|||
|-|------|
| Used&nbsp;by | [`on_attestation()`](#on_attestation) |
| Uses | [`validate_target_epoch_against_current_time()`](#validate_target_epoch_against_current_time), [`compute_epoch_at_slot()`](/part3/helper/misc/#def_compute_epoch_at_slot), [`compute_start_slot_at_epoch()`](/part3/helper/misc/#def_compute_start_slot_at_epoch), [`get_ancestor()`](#get_ancestor), [`get_ancestor()`](#get_ancestor) |

##### `store_target_checkpoint_state`

```python
def store_target_checkpoint_state(store: Store, target: Checkpoint) -> None:
    # Store target checkpoint state if not yet seen
    if target not in store.checkpoint_states:
        base_state = copy(store.block_states[target.root])
        if base_state.slot < compute_start_slot_at_epoch(target.epoch):
            process_slots(base_state, compute_start_slot_at_epoch(target.epoch))
        store.checkpoint_states[target] = base_state
```

We need checkpoint states both to provide validator balances (used for weighting votes in the fork choice) and for the validator shufflings (used when validating attestations).

<a id="checkpoint_block_epoch"></a>

A [`Checkpoint`](/part3/containers/dependencies/#checkpoint) is a reference to the first slot of an epoch and are what the Casper FFG votes in attestations point to. When an attestation targets a checkpoint that has empty slots preceding it, the checkpoint's state will not match the state of the block that it points to. Therefore, we take that block's state (`base_state`) and run the simple [`process_slots()`](/part3/transition/#def_process_slots) state transition for empty slots on it to bring the state up to date with the checkpoint.

<a id="img_annotated_forkchoice_processSlots"></a>
<figure class="diagram" style="width: 70%">

![A diagram showing the state being updated to the checkpoint by process_slots.](images/diagrams/annotated-forkchoice-processSlots.svg)

<figcaption>

Consider a checkpoint that points to $[N, B]$, where $N$ is the checkpoint height (epoch number) and $B$ is the block root of the most recent block. The shapes with dotted outlines indicate skipped slots. The `process_slots()` function takes the state $S$ associated with the block and updates it to the slot of the checkpoint by playing empty slots onto it, resulting in state $S'$.

</figcaption>
</figure>

|||
|-|------|
| Used&nbsp;by | [`on_attestation()`](#on_attestation) |
| Uses | [`compute_start_slot_at_epoch()`](/part3/helper/misc/#def_compute_start_slot_at_epoch), [`process_slots()`](/part3/transition/#def_process_slots),  |

##### `update_latest_messages`

```python
def update_latest_messages(store: Store, attesting_indices: Sequence[ValidatorIndex], attestation: Attestation) -> None:
    target = attestation.data.target
    beacon_block_root = attestation.data.beacon_block_root
    non_equivocating_attesting_indices = [i for i in attesting_indices if i not in store.equivocating_indices]
    for i in non_equivocating_attesting_indices:
        if i not in store.latest_messages or target.epoch > store.latest_messages[i].epoch:
            store.latest_messages[i] = LatestMessage(epoch=target.epoch, root=beacon_block_root)
```

A [message](#latestmessage) comprises a timestamp and a block root (head) vote. These are extracted from the containing attestation in the form of the epoch number of the target checkpoint of the attestation, and the LMD GHOST head block vote respectively. By the time we get here, [`validate_on_attestation()`](#validate_on_attestation) has already checked that the slot for which the head vote was made belongs to the epoch corresponding to the target vote. Validators vote exactly once per epoch, so the epoch number is granular enough for tracking their latest votes.

All the validators in `attesting_indices` made this same attestation. The attestation will have travelled the world as a single [aggregate attestation](/part3/containers/operations/#attestation), but it has been unpacked in [`on_attestation()`](#on_attestation) before being passed to this function. Validators on our naughty list of [equivocaters](#on_attester_slashing) are filtered out, and any that are left are considered for updates.

If the validator index is not yet in the `store.latest_messages` set then its vote is inserted; if the vote that we have is newer than the vote already stored then it is updated. Each validator has at most one entry in the `latest_messages` set.

|||
|-|------|
| Used&nbsp;by | [`on_attestation()`](#on_attestation) |
| See&nbsp;also | [`Attestation`](/part3/containers/operations/#attestation), [`LatestMessage`](#latestmessage) |

### Handlers

The four handlers below &ndash; `on_tick()`, `on_block()`, `on_attestation()`, and `on_attester_slashing()` &ndash; are the fork choice rule's four senses. These are the means by which the fork choice gains its knowledge of the outside world, and the only means by which the Store gets updated.

None of the handlers is explicitly called by any code that appears anywhere in the spec. It is expected that client implementations will call each handler as and when required. As per the introductory material at the top of the fork choice spec, they should be called as follows.

  - `on_tick(store, time)` whenever `time > store.time` where `time` is the current Unix time.
  - `on_block(store, block)` whenever a block `block` is received.
  - `on_attestation(store, attestation)` whenever an attestation `attestation` is received.
  - `on_attester_slashing(store, attester_slashing)` whenever an attester slashing `attester_slashing` is received.

#### `on_tick`

```python
def on_tick(store: Store, time: uint64) -> None:
    # If the ``store.time`` falls behind, while loop catches up slot by slot
    # to ensure that every previous slot is processed with ``on_tick_per_slot``
    tick_slot = (time - store.genesis_time) // SECONDS_PER_SLOT
    while get_current_slot(store) < tick_slot:
        previous_time = store.genesis_time + (get_current_slot(store) + 1) * SECONDS_PER_SLOT
        on_tick_per_slot(store, previous_time)
    on_tick_per_slot(store, time)
```

A "tick" is not defined in the specification. Notionally, ticks are used to continually keep the fork choice's internal clock (`store.time`) updated. In practice, calling `on_tick()` is only really required at the start of a slot, at `SECONDS_PER_SLOT` `/` `INTERVALS_PER_SLOT` into a slot, and [before proposing a block](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/phase0/validator.md#block-proposal). However, `on_tick()` processing is light and it can be convenient to call it more often.

The Teku client calls `on_tick()` regularly [twice per second](https://github.com/ConsenSys/teku/blob/727e734e2d7c31112e1e313ef3cd2c0a004f81b2/services/timer/src/main/java/tech/pegasys/teku/services/timer/TimerService.java#L37) since it is used internally to drive other things beside the fork choice. In addition, Teku uses units of milliseconds rather than seconds for its tick interval, which is strictly speaking off-spec, but is necessary for supporting other chains such as the [Gnosis Beacon Chain](https://docs.gnosischain.com/specs) for which the `SECONDS_PER_SLOT` is not a multiple of `INTERVALS_PER_SLOT`.

The `while` loop was introduced in the Capella specification. It ensures that the processing in [`on_tick_per_slot()`](#on_tick_per_slot) is done every slot. When multiple slots have passed since the last tick was processed, the loop calls `on_tick_per_slot()` for each of them so as to catch up. The only thing this makes a difference to is updating the checkpoints at epoch boundaries. Previously, if a tick was not processed during the first slot of an epoch, then the checkpoint update could be incorrectly skipped. Note that `on_tick_per_slot()` updates `store.time`, which in turn updates the output of `get_current_slot()`, so the loop will terminate.

I expect that the reason `time` is provided as a parameter rather than looked up via the machine's clock is that it simplifies testing.

|||
|-|------|
| Uses | [`get_current_slot()`](#get_current_slot), [`on_tick_per_slot()`](#on_tick_per_slot) |
| See&nbsp;also | [`SECONDS_PER_SLOT`](/part3/config/configuration/#seconds_per_slot) |

#### `on_block`

```python
def on_block(store: Store, signed_block: SignedBeaconBlock) -> None:
    block = signed_block.message
    # Parent block must be known
    assert block.parent_root in store.block_states
    # Make a copy of the state to avoid mutability issues
    pre_state = copy(store.block_states[block.parent_root])
    # Blocks cannot be in the future. If they are, their consideration must be delayed until they are in the past.
    assert get_current_slot(store) >= block.slot

    # Check that block is later than the finalized epoch slot (optimization to reduce calls to get_ancestor)
    finalized_slot = compute_start_slot_at_epoch(store.finalized_checkpoint.epoch)
    assert block.slot > finalized_slot
    # Check block is a descendant of the finalized block at the checkpoint finalized slot
    assert get_ancestor(store, block.parent_root, finalized_slot) == store.finalized_checkpoint.root

    # Check the block is valid and compute the post-state
    state = pre_state.copy()
    block_root = hash_tree_root(block)
    state_transition(state, signed_block, True)
    # Add new block to the store
    store.blocks[block_root] = block
    # Add new state for this block to the store
    store.block_states[block_root] = state

    # Add proposer score boost if the block is timely
    time_into_slot = (store.time - store.genesis_time) % SECONDS_PER_SLOT
    is_before_attesting_interval = time_into_slot < SECONDS_PER_SLOT // INTERVALS_PER_SLOT
    if get_current_slot(store) == block.slot and is_before_attesting_interval:
        store.proposer_boost_root = hash_tree_root(block)

    # Update checkpoints in store if necessary
    update_checkpoints(store, state.current_justified_checkpoint, state.finalized_checkpoint)

    # Eagerly compute unrealized justification and finality
    compute_pulled_up_tip(store, block_root)
```

The `on_block()` handler should be called whenever a new signed beacon block is received. It does the following.

  - Perform some validity checks:
  - Update the store with the block and its associated beacon state.
  - Handle proposer boost (block timeliness).
  - Update the Store's justified and finalised checkpoints if permitted and required.

The `on_block()` handler does not call the `on_attestation()` handler for the attestations it contains, so clients need to do that separately for each attestation.

##### Validity checks

```none
    # Parent block must be known
    assert block.parent_root in store.block_states
    # Make a copy of the state to avoid mutability issues
    pre_state = copy(store.block_states[block.parent_root])
    # Blocks cannot be in the future. If they are, their consideration must be delayed until they are in the past.
    assert get_current_slot(store) >= block.slot

    # Check that block is later than the finalized epoch slot (optimization to reduce calls to get_ancestor)
    finalized_slot = compute_start_slot_at_epoch(store.finalized_checkpoint.epoch)
    assert block.slot > finalized_slot
    # Check block is a descendant of the finalized block at the checkpoint finalized slot
    assert get_ancestor(store, block.parent_root, finalized_slot) == store.finalized_checkpoint.root

    # Check the block is valid and compute the post-state
    state = pre_state.copy()
    block_root = hash_tree_root(block)
    state_transition(state, signed_block, True)
```

First we do some fairly self-explanatory checks. In order to be considered in the fork choice, the block must be joined to the block tree that we already have (that is, its parent must be in the Store), it must not be from a future slot according to our Store's clock, and it must be from a branch that descends from our finalised checkpoint. By the definition of finalised, all prior branches from the canonical chain are pruned away.

The final check is to run a full state transition on the block. This has two purposes, (1) it checks that the block is valid with respect to the consensus rules, and (2) it gives us the block's post-state which we need to add to the Store. We got the block's pre-state from its parent, which we know is already in the store. The `True` parameter to [`state_transition()`](/part3/transition/#def_state_transition) ensures that the block's signature is checked, and that the result of applying the block to the state results in the same state root that the block claims it does (the "post-states" must match). Clients will be running this operation elsewhere when performing the state transition, so it is likely that the result of the `state_transition()` call will be cached somewhere in an optimal implementation.

##### Update the Store

```none
    # Add new block to the store
    store.blocks[block_root] = block
    # Add new state for this block to the store
    store.block_states[block_root] = state
```

Once the block has passed the validity checks, it and its post-state can be added to the Store.

##### Handle proposer boost

```none
    # Add proposer score boost if the block is timely
    time_into_slot = (store.time - store.genesis_time) % SECONDS_PER_SLOT
    is_before_attesting_interval = time_into_slot < SECONDS_PER_SLOT // INTERVALS_PER_SLOT
    if get_current_slot(store) == block.slot and is_before_attesting_interval:
        store.proposer_boost_root = hash_tree_root(block)
```

[Proposer boost](#proposer-boost) is a defence against balancing attacks on LMD GHOST. It rewards timely blocks with extra weight in the fork choice, making it unlikely that an honest proposer's block will become orphaned.

Here, in the `on_block()` handler, is where the block's timeliness is assessed and recorded. If the Store's time (as set by the [`on_tick()`](#on_tick) handler) is within the first third of the slot (`1 /` `INTERVALS_PER_SLOT`, that is, 4 seconds) when the block is processed, then we set `store.proposer_boost_root` to the block's root.

The `store.proposer_boost_root` field can only be set during the first four seconds of a slot, and it is cleared at the start of the next slot by the [`on_tick()`](#on_tick) handler. It is used in the [`get_weight()`](#get_weight) function to determine whether to add the extra proposer boost weight or not.

Note that, if there is a proposer equivocation in the slot, this code will apply proposer boost to the second block received rather than to the first block received. This becomes important for the security of third-party block production with [MEV-Boost](https://github.com/flashbots/mev-boost/) - it can [allow a proposer](https://lighthouse-blog.sigmaprime.io/mev-unbundling-rpc.html) to "steal" the transactions in a block builder's block (at the cost of getting slashed), which is deemed to be a Bad Thing. It would be better to apply the proposer boost only to the first block received, and a [small patch](https://github.com/ethereum/consensus-specs/pull/3352) to `on_block()` has been proposed to implement this.

##### Update justified and finalised

```none
    # Update checkpoints in store if necessary
    update_checkpoints(store, state.current_justified_checkpoint, state.finalized_checkpoint)

    # Eagerly compute unrealized justification and finality
    compute_pulled_up_tip(store, block_root)
```

[`update_checkpoints()`](#update_checkpoints) simply updates the Store's justified and finalised checkpoints if those in the block's post-state are better (that is, higher, more recent). The Store always tracks the best known justified and finalised checkpoints that it is able to validate.

[`compute_pulled_up_tip()`](#compute_pulled_up_tip) runs the epoch transition Casper FFG accounting on the block &ndash; notionally "pulling it up" from its current slot to the first slot of the next epoch &ndash; to see if it has achieved [unrealised justification](#unrealised-justification). The block's unrealised justification will be stored for later use by [`filter_block_tree()`](#filter_block_tree), and the Store's unrealised justification and unrealised finalisation trackers may be updated. If the block is from a previous epoch, then the unrealised checkpoints become realised, and [`update_checkpoints()`](#update_checkpoints) will be called again, potentially over-writing the update we just made in the line above.

|||
|-|------|
| Uses | [`get_current_slot()`](#get_current_slot), [`compute_start_slot_at_epoch()`](/part3/helper/misc/#def_compute_start_slot_at_epoch), [`get_ancestor()`](#get_ancestor), [`hash_tree_root()`](/part3/helper/crypto/#hash_tree_root), [`state_transition()`](/part3/transition/#def_state_transition), [`update_checkpoints()`](#update_checkpoints), [`compute_pulled_up_tip()`](#compute_pulled_up_tip) |
| See&nbsp;also | [`INTERVALS_PER_SLOT`](#intervals_per_slot) |

#### `on_attestation`

```python
def on_attestation(store: Store, attestation: Attestation, is_from_block: bool=False) -> None:
    """
    Run ``on_attestation`` upon receiving a new ``attestation`` from either within a block or directly on the wire.

    An ``attestation`` that is asserted as invalid may be valid at a later time,
    consider scheduling it for later processing in such case.
    """
    validate_on_attestation(store, attestation, is_from_block)

    store_target_checkpoint_state(store, attestation.data.target)

    # Get state at the `target` to fully validate attestation
    target_state = store.checkpoint_states[attestation.data.target]
    indexed_attestation = get_indexed_attestation(target_state, attestation)
    assert is_valid_indexed_attestation(target_state, indexed_attestation)

    # Update latest messages for attesting indices
    update_latest_messages(store, indexed_attestation.attesting_indices, attestation)
```

Attestations may be useful no matter how we heard about them: they might have been contained in a block, or been received individually via gossip, or via a carrier pigeon[^fn-view-merge-attestations].

[^fn-view-merge-attestations]: This would change were we to adopt [view-merge](https://ethresear.ch/t/view-merge-as-a-replacement-for-proposer-boost/13739?u=benjaminion). Only attestations that had been processed by specifically designated aggregators would be considered in the fork choice.

If the attestation was unpacked from a block then the flag `is_from_block` should be set to `True`. This causes the timeliness check in [`validate_on_attestation()`](#validate_on_attestation) to be skipped: attestations not from blocks must be received in the epoch they were produced in, or the next epoch, in order to influence the fork choice. (So, a carrier pigeon would need to be fairly swift.)

The [`validate_on_attestation()`](#validate_on_attestation) function performs a comprehensive set of validity checks on the attestation to defend against various attacks.

Assuming that the attestation passes the checks, we add its target checkpoint state to the Store for later use, as well as using it immediately. The [`store_target_checkpoint_state()`](#store_target_checkpoint_state) function is idempotent, so nothing happens if the state is already present.

Having the target checkpoint state, we can use it to look up the correct shuffling for the validators. With the shuffling in hand, calling [`get_indexed_attestation()`](/part3/helper/accessors/#def_get_indexed_attestation) turns the [`Attestation`](/part3/containers/operations/#attestation) object (containing a bitlist) into an [`IndexedAttestation`](/part3/containers/dependencies/#indexedattestation) object (containing a list of validator indices).

Finally, we can validate the indexed attestation with [`is_valid_indexed_attestation()`](/part3/helper/predicates/#def_is_valid_indexed_attestation), which amounts to checking its aggregate BLS signature against the set of public keys of this indexed validators. Checking the signatures is relatively expensive compared with the other checks, which is one reason for deferring it to last (we also don't want to be checking them against an [inconsistent target](#lmd-and-ffg-vote-consistency)).

If, and only if, everything has succeeded, we call [`update_latest_messages()`](#update_latest_messages) to refresh the Store's list of latest messages for the validators that participated in this vote.

|||
|-|------|
| Uses | [`validate_on_attestation()`](#validate_on_attestation), [`store_target_checkpoint_state()`](#store_target_checkpoint_state), [`get_indexed_attestation()`](/part3/helper/accessors/#def_get_indexed_attestation), [`is_valid_indexed_attestation()`](/part3/helper/predicates/#def_is_valid_indexed_attestation), [`update_latest_messages()`](#update_latest_messages) |

#### `on_attester_slashing`

> _Note_: `on_attester_slashing` should be called while syncing and a client MUST maintain the equivocation set of `AttesterSlashing`s from at least the latest finalized checkpoint.

```python
def on_attester_slashing(store: Store, attester_slashing: AttesterSlashing) -> None:
    """
    Run ``on_attester_slashing`` immediately upon receiving a new ``AttesterSlashing``
    from either within a block or directly on the wire.
    """
    attestation_1 = attester_slashing.attestation_1
    attestation_2 = attester_slashing.attestation_2
    assert is_slashable_attestation_data(attestation_1.data, attestation_2.data)
    state = store.block_states[store.justified_checkpoint.root]
    assert is_valid_indexed_attestation(state, attestation_1)
    assert is_valid_indexed_attestation(state, attestation_2)

    indices = set(attestation_1.attesting_indices).intersection(attestation_2.attesting_indices)
    for index in indices:
        store.equivocating_indices.add(index)
```

<a id="equivocation_balancing_attack"></a>

The `on_attester_slashing()` handler was [added](https://github.com/ethereum/consensus-specs/pull/2845) to defend against the [equivocation balancing attack](https://ethresear.ch/t/balancing-attack-lmd-edition/11853?u=benjaminion) (described more formally in [Two Attacks On Proof-of-Stake GHOST/Ethereum](https://arxiv.org/abs/2203.01315)). The attack relies on the adversary's validators equivocating about their attestations &ndash; that is, publishing multiple different attestations per epoch &ndash; and is not solved by proposer score boosting.

Of course, the equivocating attestations are slashable under the Casper FFG commandments. When the attack finally ends, those validators will be punished and ejected from the validator set. Meanwhile, however, since the fork choice calculations are based on the validator set at the last justified epoch, the adversary's validators could keep the attack going indefinitely.

Rather than add a lot of apparatus within the fork choice to track and detect conflicting attestations, the mechanism relies on third-party slashing claims received via blocks or directly from peers as [attester slashing messages](/part3/containers/operations/#attesterslashing). The validity checks are identical to those in the state-transition's [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing) method, including the use of [`is_slashable_attestation_data()`](/part3/helper/predicates/#is_slashable_attestation_data). This is broader that we need for our purposes here as it will exclude validators that make surround votes as well as validators that equivocate. But excluding all misbehaving validators is probably a good idea.

Any validators proven to have made conflicting attestations are added to the `store.equivocating_indices` set[^fn-python-set-add]. They will no longer be involved in calculating the [weight of branches](#get_weight), and their future attestations [will be ignored](#update_latest_messages) in the fork choice. We are permitted to clear any equivocating attestation information from before the last finalised checkpoint, but those validators would have been slashed by the state-transition by then, so this ban is permanent.

[^fn-python-set-add]: `store.equivocating_indices` is a Python Set. Adding an existing element again is a no-op, so it cannot grow without bounds.

|||
|-|------|
| Uses | [`is_slashable_attestation_data()`](/part3/helper/predicates/#def_is_slashable_attestation_data), [`is_valid_indexed_attestation()`](/part3/helper/predicates/#def_is_valid_indexed_attestation) |
| See&nbsp;also | [`AttesterSlashing`](/part3/containers/operations/#attesterslashing), [`process_attester_slashing()`](/part3/transition/block/#def_process_attester_slashing) |

### Bellatrix Fork Choice <!-- /part3/forkchoice/bellatrix/ -->

### Introduction

This section covers the additional Bellatrix fork choice document, [v1.3.0](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/fork-choice.md). For a complementary take, see Vitalik's [annotated Bellatrix fork choice](https://github.com/ethereum/annotated-spec/blob/master/merge/fork-choice.md) (based on a slightly older version).

As usual, text with a side bar is quoted directly from the specification.

> This is the modification of the fork choice according to the executable beacon chain proposal.
>
> _Note_: It introduces the process of transition from the last PoW block to the first PoS block.

The "executable beacon chain proposal"[^fn-executable-beacon-chain-name] is what became known as The Merge, and is specified by [EIP-3675](https://eips.ethereum.org/EIPS/eip-3675) together with the Bellatrix upgrade on the beacon chain.

[^fn-executable-beacon-chain-name]: This name comes from Mikhail Kalinin's [original article on Ethresear.ch](https://ethresear.ch/t/executable-beacon-chain/8271?u=benjaminion).

Upgrades to Ethereum's protocol are normally planned to take place at pre-determined block heights. For security reasons, the Merge upgrade used a different trigger, specifically a [terminal total difficulty](/part3/config/configuration/#transition-settings) of proof of work mining. The first proof of work block to reach that amount of accumulated difficulty became the last proof of work block: all subsequent execution blocks are now merged into the proof of stake beacon chain as execution payloads.

The only functional change to the fork choice that the Bellatrix upgrade introduced was about ensuring that a valid terminal proof of work block was picked up by the beacon chain at the point of the Merge. As such, this section is largely of only historical interest now.

The remainder of the material in this section (mostly Engine API related) isn't really relevant to the fork choice rule at all. It mainly describes one-way communication of fork choice decisions to the execution layer. Altogether, it's a bit of a weird collection of stuff, for want of a better place to put it I suppose.

### Custom types

| Name | SSZ equivalent | Description |
| - | - | - |
| `PayloadId` | `Bytes8` | Identifier of a payload building process |

`PayloadId` is used to keep track of stateful requests from the consensus client to the execution client. Specifically, the consensus client can ask the execution client to start creating a new execution payload via the [`notify_forkchoice_updated()`](#notify_forkchoice_updated) command (which maps to the [`engine_forkchoiceUpdatedV1`](https://github.com/ethereum/execution-apis/blob/main/src/engine/paris.md#engine_forkchoiceupdatedv1) RPC method in the Engine API docs). The execution client will return a `PayloadId` reference and continue to build the payload asynchronously. Later, the consensus client can obtain the payload with a call to the engine API's [`engine_getPayloadV1`](https://github.com/ethereum/execution-apis/blob/main/src/engine/paris.md#engine_getpayloadv1) method by passing it the same `PayloadId`.

### Protocols

#### `ExecutionEngine`

> _Note_: The `notify_forkchoice_updated` function is added to the `ExecutionEngine` protocol to signal the fork choice updates.
>
> The body of this function is implementation dependent. The Engine API may be used to implement it with an external execution engine.

Post-Merge, every consensus client (beacon chain client) must be paired up with an execution client (`ExecutionEngine`; formerly, Eth1 client). The execution client has several roles.

1. It validates execution payloads.
2. It executes execution payloads in order to maintain Ethereum's state (accounts, contracts, balances, receipts, etc.).
3. It provides data to applications via its RPC API.
4. It maintains a mempool of transactions from which it builds execution payloads and provides them to the consensus layer for distribution.

The first and the last of these are the ones that interest us on the consensus side. The first role is important because beacon blocks are valid only if they contain valid execution payloads. The last is important because the consensus side does not directly handle ordinary Ethereum transactions and cannot build its own execution payloads.

The interface between the two sides is called the [Engine API](https://github.com/ethereum/execution-apis/tree/main/src/engine). The Engine API is the RPC (remote procedure call) interface that the execution client provides to its companion consensus client. It is one-way in the sense that the consensus client can call methods on the Engine API, but the execution client does not call any methods on the consensus client.

[TODO: link to `EngineAPI` section when written]::

The most interesting methods that the Engine API provides are these three.

  - [`engine_newPayloadV1`](https://github.com/ethereum/execution-apis/blob/main/src/engine/paris.md#engine_newpayloadv1)
    - When the consensus client receives a new beacon block, it extracts the block's execution payload and uses this method to send it to the execution client. The execution client will validate the payload and execute the transactions it contains. The method's return value indicates whether the payload was valid or not.
  - [`engine_forkchoiceUpdatedV1`](https://github.com/ethereum/execution-apis/blob/main/src/engine/paris.md#engine_forkchoiceupdatedv1)
    - The function below, [`notify_forkchoice_updated()`](#notify_forkchoice_updated), uses this method for two purposes. First, it is used routinely to update the execution client with the latest consensus information: head block, safe head block, and finalised block. Second, it can be used to prompt the execution client to begin building an execution payload from its mempool. The consensus client will do this when it is about to propose a beacon block.
  - [`engine_getPayloadV1`](https://github.com/ethereum/execution-apis/blob/main/src/engine/paris.md#engine_getpayloadv1)
    - This is used to retrieve an execution payload previously requested via `engine_forkchoiceUpdatedV1`, using a `PayloadId` as a reference.

##### `notify_forkchoice_updated`

> This function performs three actions _atomically_:
>
>   - Re-organizes the execution payload chain and corresponding state to make `head_block_hash` the head.
>   - Updates safe block hash with the value provided by `safe_block_hash` parameter.
>   - Applies finality to the execution state: it irreversibly persists the chain of all execution payloads and corresponding state, up to and including `finalized_block_hash`.
>
> Additionally, if `payload_attributes` is provided, this function sets in motion a payload build process on top of `head_block_hash` and returns an identifier of initiated process.

```python
def notify_forkchoice_updated(self: ExecutionEngine,
                              head_block_hash: Hash32,
                              safe_block_hash: Hash32,
                              finalized_block_hash: Hash32,
                              payload_attributes: Optional[PayloadAttributes]) -> Optional[PayloadId]:
    ...
```

This is a wrapper around the Engine API's [`engine_forkchoiceUpdatedV1`](https://github.com/ethereum/execution-apis/blob/main/src/engine/paris.md#engine_forkchoiceupdatedv1) RPC method as described above. We use it to keep the execution client up to date with the latest fork choice information, and (optionally) from time to time to request it to build a new execution payload for us.

> _Note_: The `(head_block_hash, finalized_block_hash)` values of the `notify_forkchoice_updated` function call maps on the `POS_FORKCHOICE_UPDATED` event defined in the [EIP-3675](https://eips.ethereum.org/EIPS/eip-3675#definitions). As per EIP-3675, before a post-transition block is finalized, `notify_forkchoice_updated` MUST be called with `finalized_block_hash = Hash32()`.

[EIP-3675](https://eips.ethereum.org/EIPS/eip-3675) is the specification of the Merge on the execution layer side (Eth1 side) of things. The `POS_FORKCHOICE_UPDATED` event described there is triggered by the consensus layer calling the Engine API's `engine_forkchoiceUpdatedV1` method, which is in turn triggered by the consensus client calling `notify_forkchoice_updated()`. The consensus client will do this periodically, in particular whenever a reorg occurs on the beacon chain so that applications built on the execution layer can know which state is current.

Between the Merge and the first finalised epoch after the Merge there was no guarantee of finality on the execution chain, therefore we could not sent it a finalised block hash and had to use the placeholder default value instead.

> _Note_: Client software MUST NOT call this function until the transition conditions are met on the PoW network, i.e. there exists a block for which `is_valid_terminal_pow_block` function returns `True`.

The proof of work chain was not interested in the proof of stake chain's view of the world until after the Merge.

> _Note_: Client software MUST call this function to initiate the payload build process to produce the merge transition block; the `head_block_hash` parameter MUST be set to the hash of a terminal PoW block in this case.

The first beacon chain proposer after the terminal proof of work block had been detected would call `notify_forkchoice_updated()` with the `payload_attributes` parameter in order to request an execution payload to be build for the first merged block.

If there had been multiple candidate terminal PoW blocks (as there were for the Goerli testnet Merge), the beacon block proposer would have been free to choose which of them to ask its execution client to build on.

###### `safe_block_hash`

> The `safe_block_hash` parameter MUST be set to return value of [`get_safe_execution_payload_hash(store: Store)`](https://github.com/ethereum/consensus-specs/blob/v1.3.0/fork_choice/safe-block.md#get_safe_execution_payload_hash) function.

The "safe block" feature is a way for the consensus protocol to signal to the execution layer that a block is very unlikely ever to be reverted. Application developers could use the safe block information to provide better user experience to their users in the form of a pseudo fast-finality. See the later [Safe Block](/part3/safe-block/) section for more on this.

### Helpers

#### `PayloadAttributes`

> Used to signal to initiate the payload build process via `notify_forkchoice_updated`.

```python
@dataclass
class PayloadAttributes(object):
    timestamp: uint64
    prev_randao: Bytes32
    suggested_fee_recipient: ExecutionAddress
    withdrawals: Sequence[Withdrawal]  # [New in Capella]
```

This class maps onto the Engine API's [`PayloadAttributesV2`](https://github.com/ethereum/execution-apis/blob/main/src/engine/shanghai.md#payloadattributesv2) class and is used when asking the execution client to start building an execution payload.

The `prev_randao` field is the beacon state's current [RANDAO](/part2/building_blocks/randomness/) value, having been updated by the RANDAO reveal in the previous beacon block. It is made available to execution layer applications via the EVM's new [`PREVRANDAO`](https://eips.ethereum.org/EIPS/eip-4399) opcode.

`suggested_fee_recipient` is the Ethereum account that any fee income from transaction tips should be sent to when the payload is executed (formerly known as the `COINBASE`). The execution client may override this if it has its own setting for fee recipient, hence "suggested". But allowing it to be set via the Engine API makes it possible for a beacon node hosting multiple validators to use a different fee recipient address for each validator, whereas setting it on the execution side would force them all to use the same fee recipient address.

The `withdrawals` field was added in the [Capella upgrade](/part4/history/capella/). It allows the consensus layer to pass a list of withdrawals to the execution layer to include in an execution payload. There will be at most [`MAX_WITHDRAWALS_PER_PAYLOAD`](/part3/config/preset/#max_withdrawals_per_payload) of them. When the block containing the payload is processed, for each withdrawal, the amount will be deducted from validator's balance on the beacon chain, and will be added to the balance of the Ethereum account in the [`Withdrawal`](/part3/containers/dependencies/#withdrawal) object's `ExecutionAddress` field. The `ExecutionAddress` is derived from the validator's [withdrawal credentials](/part3/config/constants/#eth1_address_withdrawal_prefix).

### `PowBlock`

```python
class PowBlock(Container):
    block_hash: Hash32
    parent_hash: Hash32
    total_difficulty: uint256
```

This class is just a succinct way to wrap up the information we need for checking proof of work blocks around the Merge. It is returned by [`get_pow_block()`](#get_pow_block) and consumed by [`is_valid_terminal_pow_block()`](#is_valid_terminal_pow_block).

#### `get_pow_block`

> Let `get_pow_block(block_hash: Hash32) -> Optional[PowBlock]` be the function that given the hash of the PoW block returns its data. It may result in `None` if the requested block is not yet available.
>
> _Note_: The `eth_getBlockByHash` JSON-RPC method may be used to pull this information from an execution client.

As noted, `get_pow_block()` is a wrapper around Ethereum's [`eth_getBlockByHash`](https://ethereum.org/en/developers/docs/apis/json-rpc/#eth_getblockbyhash) JSON-RPC method. Given a block hash (not its hash tree root! - Eth1 blocks are encoded with RLP rather than SSZ), it returns the information in the [`PowBlock`](#powblock) structure.

`eth_getBlockByHash` is a standard Eth1 client RPC method rather than a specific Engine API method. For convenience, execution clients often provide access to this method via the Engine API port in addition to the standard RPC API port so that consensus clients can be configured to connect to only one port on the execution client.

#### `is_valid_terminal_pow_block`

> Used by fork-choice handler, `on_block`.

```python
def is_valid_terminal_pow_block(block: PowBlock, parent: PowBlock) -> bool:
    is_total_difficulty_reached = block.total_difficulty >= TERMINAL_TOTAL_DIFFICULTY
    is_parent_total_difficulty_valid = parent.total_difficulty < TERMINAL_TOTAL_DIFFICULTY
    return is_total_difficulty_reached and is_parent_total_difficulty_valid
```

Given two [`PowBlock`](#powblock) objects (corresponding to a proof of work block and its parent proof of work block respectively), this function checks whether the block meets the criteria for being the terminal proof of work block. That is, that its total difficulty exceeds the terminal total difficulty and that its parent's total difficulty does not.

#### `validate_merge_block`

```python
def validate_merge_block(block: BeaconBlock) -> None:
    """
    Check the parent PoW block of execution payload is a valid terminal PoW block.

    Note: Unavailable PoW block(s) may later become available,
    and a client software MAY delay a call to ``validate_merge_block``
    until the PoW block(s) become available.
    """
    if TERMINAL_BLOCK_HASH != Hash32():
        # If `TERMINAL_BLOCK_HASH` is used as an override, the activation epoch must be reached.
        assert compute_epoch_at_slot(block.slot) >= TERMINAL_BLOCK_HASH_ACTIVATION_EPOCH
        assert block.body.execution_payload.parent_hash == TERMINAL_BLOCK_HASH
        return

    pow_block = get_pow_block(block.body.execution_payload.parent_hash)
    # Check if `pow_block` is available
    assert pow_block is not None
    pow_parent = get_pow_block(pow_block.parent_hash)
    # Check if `pow_parent` is available
    assert pow_parent is not None
    # Check if `pow_block` is a valid terminal PoW block
    assert is_valid_terminal_pow_block(pow_block, pow_parent)
```

This is used by the Bellatrix `on_block()` handler. The `block` parameter is a beacon block that claims to be the first merged block. That is, it is the first beacon block (on the current branch) to contain a non-default [`ExecutionPayload`](/part3/containers/execution/#executionpayload).

The [`TERMINAL_BLOCK_HASH`](/part3/config/configuration/#transition-settings) is a parameter that client operators could have agreed to use to override the terminal total difficulty mechanism if necessary. For example, if the Merge had resulted in beacon chain forks they could have been resolved by manually agreeing an Eth1 Merge block and setting `TERMINAL_BLOCK_HASH` to its value via client command line parameters. In the event, this was not needed and `TERMINAL_BLOCK_HASH` remains at its default value of `Hash32()`.

The remainder of the function checks, (a) that the PoW block that's the parent of the execution payload exists, and has total difficulty greater than the [`TERMINAL_TOTAL_DIFFICULTY`](/part3/config/configuration/#transition-settings), and (b) that the parent of that block exists and has a total difficulty less than the `TERMINAL_TOTAL_DIFFICULTY`. (The difficulty checks are performed in [`is_valid_terminal_pow_block()`](#is_valid_terminal_pow_block).)

<a id="img_annotated_forkchoice_the_merge_block"></a>
<figure class="diagram" style="width: 80%">

![A diagram showing the relationship between the merge block and the terminal proof of work block.](images/diagrams/annotated-forkchoice-the-merge-block.svg)

<figcaption>

The first beacon chain merged block contains the execution payload whose parent PoW block was the terminal PoW block. The terminal PoW block is the first PoW block to have a total difficulty exceeding the [`TERMINAL_TOTAL_DIFFICULTY`](/part3/config/configuration/#transition-settings).

</figcaption>
</figure>

The parent and grandparent PoW blocks are retrieved via the [`get_pow_block()`](#get_pow_block) function, which in practice involves making RPC calls to the attached Eth1/execution client. If either of these calls fails, an `assert` will be triggered, and the `on_block()` handler will bail out without making any changes.

### Updated fork-choice handlers

#### `on_block`

> _Note_: The only modification is the addition of the verification of transition block conditions.

```python
def on_block(store: Store, signed_block: SignedBeaconBlock) -> None:
    """
    Run ``on_block`` upon receiving a new block.

    A block that is asserted as invalid due to unavailable PoW block may be valid at a later time,
    consider scheduling it for later processing in such case.
    """
    block = signed_block.message
    # Parent block must be known
    assert block.parent_root in store.block_states
    # Make a copy of the state to avoid mutability issues
    pre_state = copy(store.block_states[block.parent_root])
    # Blocks cannot be in the future. If they are, their consideration must be delayed until they are in the past.
    assert get_current_slot(store) >= block.slot

    # Check that block is later than the finalized epoch slot (optimization to reduce calls to get_ancestor)
    finalized_slot = compute_start_slot_at_epoch(store.finalized_checkpoint.epoch)
    assert block.slot > finalized_slot
    # Check block is a descendant of the finalized block at the checkpoint finalized slot
    assert get_ancestor(store, block.parent_root, finalized_slot) == store.finalized_checkpoint.root

    # Check the block is valid and compute the post-state
    state = pre_state.copy()
    block_root = hash_tree_root(block)
    state_transition(state, signed_block, True)

    # [New in Bellatrix]
    if is_merge_transition_block(pre_state, block.body):
        validate_merge_block(block)

    # Add new block to the store
    store.blocks[block_root] = block
    # Add new state for this block to the store
    store.block_states[block_root] = state

    # Add proposer score boost if the block is timely
    time_into_slot = (store.time - store.genesis_time) % SECONDS_PER_SLOT
    is_before_attesting_interval = time_into_slot < SECONDS_PER_SLOT // INTERVALS_PER_SLOT
    if get_current_slot(store) == block.slot and is_before_attesting_interval:
        store.proposer_boost_root = hash_tree_root(block)

    # Update checkpoints in store if necessary
    update_checkpoints(store, state.current_justified_checkpoint, state.finalized_checkpoint)

    # Eagerly compute unrealized justification and finality.
    compute_pulled_up_tip(store, block_root)
```

As noted, the only addition here to the normal [`on_block()`](/part3/forkchoice/phase0/#on_block) handler is the lines,

```none
    # [New in Bellatrix]
    if is_merge_transition_block(pre_state, block.body):
        validate_merge_block(block)
```

The [`is_merge_transition_block()`](/part3/helper/predicates/#def_is_merge_transition_block) function will return `True` when the given block is the first beacon block that contains an execution payload, and `False` otherwise.

To ensure consistency between the execution chain and the beacon chain at the Merge, this first merged beacon block requires some extra processing. We must check that the PoW block its execution payload is derived from has indeed met the [criteria for the merge](#is_valid_terminal_pow_block). Essentially, its total difficulty must exceed the terminal total difficulty and its parent's total difficulty must not. If this test fails then something has gone wrong and the beacon block must be excluded from the fork choice.

There might be several candidate execution blocks that meet this criterion in the event of PoW forks at the point of the Merge &ndash; [this occurred](https://web.archive.org/web/20230630134924/https://nitter.it/vdWijden/status/1557555377314701312) when merging one of the testnets[^fn-teku-besu-goerli-merge] &ndash; but that's fine. The proposer of the first merged beacon block[^fn-first-merged-beacon-block] that becomes canonical gets to decide which terminal execution block wins.

[^fn-teku-besu-goerli-merge]: And triggered [an issue](https://hackmd.io/@ajsutton/SJJYWezC9) with some client implementations.

[^fn-first-merged-beacon-block]: For the record, the first merged beacon block on mainnet was at [slot 4700013](https://beaconcha.in/slot/4700013).

## Safe Block <!-- /part3/safe-block/ -->

### Introduction

The [Fork Choice Safe Block spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/fork_choice/safe-block.md) is not really part of the beacon chain's fork choice and is located in a different document in the consensus repo. It is an heuristic for using the fork choice's Store data to identify a block that will not be reverted, under some reasonable assumptions. It could be used, for example, by applications to implement a settlement period for transactions. There is an analogy with the assumption that, under proof of work, in the absence of a 51% attack, a block becomes safe from reorgs after a certain number of blocks (say, fifteen) have been built on top of it.

> Under honest majority and certain network synchronicity assumptions there exist a block that is safe from re-orgs. Normally this block is pretty close to the head of canonical chain which makes it valuable to expose a safe block to users.
>
> This section describes an algorithm to find a safe block.

Of course, the ultimate safe block is the last finalised checkpoint. But that could be several minutes in the past, even under ideal network conditions. If we assume (a) that there is an honest majority of validators, and (b) that their messages are received in a timely fashion, then we can in principle identify a more recent block that will not be at risk of reversion.

#### `get_safe_beacon_block_root`

```python
def get_safe_beacon_block_root(store: Store) -> Root:
    # Use most recent justified block as a stopgap
    return store.justified_checkpoint.root
```

> _Note_: Currently safe block algorithm simply returns `store.justified_checkpoint.root` and is meant to be improved in the future.

Returning the justified checkpoint is certainly safe under the assumptions above, but we can almost certainly do better. Substantial progress has been made recently towards providing a more useful safe block. There's more on this in the [Confirmation rule](/part2/consensus/lmd_ghost/#confirmation-rule) section of the Consensus chapter.

#### `get_safe_execution_payload_hash`

```python
def get_safe_execution_payload_hash(store: Store) -> Hash32:
    safe_block_root = get_safe_beacon_block_root(store)
    safe_block = store.blocks[safe_block_root]

    # Return Hash32() if no payload is yet justified
    if compute_epoch_at_slot(safe_block.slot) >= BELLATRIX_FORK_EPOCH:
        return safe_block.body.execution_payload.block_hash
    else:
        return Hash32()
```

> _Note_: This helper uses beacon block container extended in [Bellatrix](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/beacon-chain.md).

Bellatrix was the pre-Merge upgrade that added the execution payload hash to beacon blocks in readiness for the Merge itself. Applications on Ethereum are largely unaware of the beacon chain and will use the execution payload hash rather than the beacon block root as their reference point in the Eth1 blockchain.

# Part 4: Upgrades <!-- /part4/ -->

<!-- Protocol upgrades, sometimes called hard forks, are backward-incompatible changes to the specification. This is how Ethereum has historically delivered improvements and extra capabilities to the Eth1 chain, and now to the Eth2 beacon chain. -->

TODO

## Hard forks <!-- /part4/forks/* -->

TODO

### Fork Digest <!-- /part4/forks/digest/* -->

TODO

## Upgrade History <!-- /part4/history/ -->

### Introduction

Through an open process in February 2021 we decided that beacon chain (consensus layer) upgrades would be [named after stars](https://github.com/ethereum/eth2.0-pm/issues/202#issuecomment-775789449). We're taking them in English alphabetical order, with the first being [Altair](https://github.com/ethereum/consensus-specs/issues/2218). The genesis configuration remains Phase&nbsp;0 due to its origin in the now defunct [three-phase plan](https://web.archive.org/web/20220916204934/https://docs.ethhub.io/ethereum-roadmap/ethereum-2.0/eth-2.0-phases/) for delivering Ethereum&nbsp;2.0.

A summary of upgrades to date is below, with more detailed descriptions in the following sections.

| Name | Epoch | Date&nbsp;(UTC) | Comments | Spec&nbsp;tag | Release&nbsp;name |
| - | - | - | - | - | - |
| [Phase&nbsp;0](/part4/history/phase0/) | 0      | 2020-12-01 12:00:23 | The genesis configuration | [v1.0.0](https://github.com/ethereum/consensus-specs/releases/tag/v1.0.0) | Cosmic Egg |
| [Altair](/part4/history/altair/)       | 74240  | 2021-10-27 10:56:23 | Sync committees and economic reforms | [v1.1.0](https://github.com/ethereum/consensus-specs/releases/tag/v1.1.0) |  The Great Machine |
| [Bellatrix](/part4/history/bellatrix/) | 144896 | 2022-09-06 11:34:47 | Merge-readiness upgrade | [v1.2.0](https://github.com/ethereum/consensus-specs/releases/tag/v1.2.0) | Ailuropoda melanoleuca[^fn-giant-panda] |
| [Capella](/part4/history/capella/)     | 194048 | 2023-04-12 22:27:35 | Withdrawals enabled | [v1.3.0](https://github.com/ethereum/consensus-specs/releases/tag/v1.3.0)  | Gamlum[^fn-gamlum] |
| [Deneb](/part4/history/deneb/)         | TBD    | TBD                 | EIP-4844 data availability | TBD  | TBD |
| [Electra](/part4/history/deneb/)       | TBD    | TBD                 | TBD | TBD  | TBD |

[^fn-giant-panda]: Ailuropoda melanoleuca is the formal name of the [giant panda](https://en.wikipedia.org/wiki/Giant_panda).

[^fn-gamlum]: Gamlum may have been a goat-related ancient name for [Capella](https://en.wikipedia.org/wiki/Capella).

The Merge was a special kind of upgrade in that it was not a manual hard fork. The protocol changes required to support the Merge were done in the Bellatrix upgrade. The Merge itself happened nine days later without any further intervention, simultaneously with the execution layer's [Paris upgrade](https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/paris.md).

[TODO: link to Merge section when done]::

The consensus layer specifications are written incrementally. Thus, each version (such as the current Bellatrix [v1.3.0](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs) version) contains the unchanged specs for previous versions, plus a separate set of documents detailing the changes for the new version. Thus, to build Bellatrix, for example, you need the [Phase&nbsp;0](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs/phase0) specs, the [Altair](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs/altair) "diff" specs on top of that, and the [Bellatrix](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs/bellatrix) "diff" specs on top of that, all with the same GitHub release tag (in this case, v1.3.0).

The consensus specs repo contains some other, unreleased, versions such as [das](https://github.com/ethereum/consensus-specs/tree/dev/specs/_features/das) (data-availability sampling), [custody_game](https://github.com/ethereum/consensus-specs/tree/dev/specs/_features/custody_game), and [sharding](https://github.com/ethereum/consensus-specs/tree/dev/specs/_features/sharding). These reflect different research directions and are in varying states of currency.

#### Upgrade timing

Under proof of work, upgrades (with the exception of the Merge) were performed at block heights that had been chosen several weeks in advance. Due to changes in hash power, predicting their timing was difficult - they could occur several hours, or even a day or two, adrift from their target time.

Under proof of stake, we have the luxury of being able to time network upgrades to the second. Nevertheless, we aim to do upgrades on 256-epoch boundaries. These boundaries correspond both to the [batching interval](/part3/transition/epoch/#historical-summaries-updates) of block and state roots ([`SLOTS_PER_HISTORICAL_ROOT`](/part3/config/preset/#slots_per_historical_root) slots), and the sync committee period ([`EPOCHS_PER_SYNC_COMMITTEE_PERIOD`](/part3/config/preset/#sync-committee) epochs). Having the protocol not change in the middle of these periods will make it easier to verify proofs using their data later.

A period of 256 epochs is around 27 hours, so we get about one opportunity per day to perform an upgrade.

### Phase 0 <!-- /part4/history/phase0/ -->

For historical reasons, the initial configuration of the beacon chain at its genesis was called Phase&nbsp;0.

Beacon chain genesis took place at 12:00:23 UTC on the 1st of December 2020. The extra 23 seconds comes from the timestamp of the first Eth1 block to meet the [genesis criteria](/part3/initialise/#genesis-state), [block 11320899](https://etherscan.io/block/11320899). It is a little remnant of proof of work forever embedded in the beacon chain's history.

|||
| - | - |
| `MIN_GENESIS_TIME` | `uint64(1606824000)` (Dec 1, 2020, 12pm UTC) |
| `GENESIS_FORK_VERSION` | `Version('0x00000000')` |

See the [Phase 0 specs](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs/phase0) for the full description. These specs still apply to the beacon chain today, except where they were superseded by the [Altair](/part4/history/altair/), [Bellatrix](/part4/history/bellatrix/), or later upgrades.

My [Phase&nbsp;0 annotated specification](https://benjaminion.xyz/eth2-annotated-spec/phase0/beacon-chain/) remains available.

### Altair <!-- /part4/history/altair/ -->

The Altair upgrade took place at 10:56:23 UTC on October the 27th, 2021.

|||
| - | - |
| `ALTAIR_FORK_VERSION` | `Version('0x01000000')` |
| `ALTAIR_FORK_EPOCH` | `Epoch(74240)` (Oct 27, 2021, 10:56:23am UTC) |

The main goals of the Altair upgrade were to

1. introduce sync committees for supporting light clients,
2. significantly rework the beacon chain reward and penalty accounting, and
3. begin increasing some penalty parameters towards their final values.

The following changes were made for sync committee support.

  - New [cryptographic domains](/part3/config/constants/#domain-types) for the sync committee functions.
  - New data structures to support sync committees. Namely, [`SyncAggregate`](/part3/containers/operations/#syncaggregate) and [`SyncCommittee`](/part3/containers/dependencies/#synccommittee).
  - Functions for managing sync committees:
    - [`get_next_sync_committee_indices()`](/part3/helper/accessors/#def_get_next_sync_committee_indices);
    - [`get_next_sync_committee()`](/part3/helper/accessors/#def_get_next_sync_committee);
    - [`process_sync_aggregate()`](/part3/transition/block/#def_process_sync_aggregate), which takes care of reward and penalty accounting for sync committee participation; and
    - [`process_sync_committee_updates()`](/part3/transition/epoch/#def_process_sync_committee_updates).
  - Gossip topics were added to the [P2P specification](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/altair/p2p-interface.md) to support sync committee activity

The reforms to the accounting were extensive, and I won't list them all here as they are thoroughly covered elsewhere in the annotated spec and book. But in summary,

  - There was a [move away](https://github.com/ethereum/consensus-specs/pull/2176#issue-779590549) from doing all the accounting for attestation inclusion at epoch boundaries to performing much of the work on an ongoing basis during epochs. The epoch transition is quite heavy in any case; this spreads the workload and is simpler overall.
  - Incentives were tweaked for different behaviours, such as late attestations and block proposal rewards. We also took the opportunity to simplify the rewards and penalties calculations.
  - The [inactivity leak](/part2/incentives/inactivity/) was changed so that it is now applied on a per-validator basis rather than globally.

As for the penalty parameters, the following parameters were updated. These had been softened at genesis as we got used to running the beacon chain:

  - [`INACTIVITY_PENALTY_QUOTIENT`](/part3/config/preset/#inactivity_penalty_quotient_bellatrix) decreased from $2^{26}$ to $3 \times 2^{24}$. This reduces stakes more quickly during an inactivity leak.
  - [`MIN_SLASHING_PENALTY_QUOTIENT`](/part3/config/preset/#min_slashing_penalty_quotient_bellatrix) decreased from 128 to 64. This sets the initial slashing penalty to 0.5&nbsp;ETH for a validator with a full stake rather than the 0.25&nbsp;ETH of Phase&nbsp;0.
  - [`PROPORTIONAL_SLASHING_MULTIPLIER`](/part3/config/preset/#proportional_slashing_multiplier_bellatrix) increased from 1 to 2 so that, in the event of over one-third of validators being slashed together, the full penalty would be the removal of two-thirds of their stakes rather than the one-third of Phase&nbsp;0.

The full description of the changes between Phase&nbsp;0 and Altair is in the [Altair specs](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs/altair).

My [Altair annotated specification](/../altair/part3/) remains available.

### Bellatrix <!-- /part4/history/bellatrix/ -->

The Bellatrix upgrade took place at 11:34:47 UTC on September the 6th, 2022.

|||
| - | - |
| `BELLATRIX_FORK_VERSION` | `Version('0x02000000')` |
| `BELLATRIX_FORK_EPOCH` | `Epoch(144896)` (Sept 6, 2022, 11:34:47am UTC) |

The primary goal of Bellatrix was to ready the beacon chain for [the Merge](/part4/merge/) that took place nine days later. It included the following elements.

  - Data structures for holding execution payloads were added, namely [`ExecutionPayload`](/part3/containers/execution/#executionpayload) and [`ExecutionPayloadHeader`](/part3/containers/execution/#executionpayloadheader).
  - The [processing of execution payloads](/part3/transition/block/#process_execution_payload) was added to [block processing](/part3/transition/block/#block-processing).
  - The [fork choice](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/fork-choice.md) was updated to recognise the transition from proof of work to proof of stake on the beacon chain side.
  - The maximum size of gossip messages and Req/Resp chunks was increased in the [P2P spec](https://github.com/ethereum/consensus-specs/blob/v1.3.0/specs/bellatrix/p2p-interface.md) to allow for the extra size of beacon blocks due to the execution payload. Also, the validity rules for gossiped blocks were updated.

In addition, continuing the changes from Altair, some penalty parameters were updated to their final values. These had been softened for the pre-Merge releases as we got used to running the beacon chain:

  - [`INACTIVITY_PENALTY_QUOTIENT`](/part3/config/preset/#inactivity_penalty_quotient_bellatrix) decreased from $3 \times 2^{24}$ to $2^{24}$. This reduces stakes more quickly during an inactivity leak.
  - [`MIN_SLASHING_PENALTY_QUOTIENT`](/part3/config/preset/#min_slashing_penalty_quotient_bellatrix) decreased from 64 to 32. This sets the initial slashing penalty to 1&nbsp;ETH for a validator with a full stake rather than 0.5&nbsp;ETH.
  - [`PROPORTIONAL_SLASHING_MULTIPLIER`](/part3/config/preset/#proportional_slashing_multiplier_bellatrix) increased from 2 to 3 so that, in the event of over one-third of validators being slashed together, the full penalty would be the removal of their entire stakes.

The full description of the changes between Altair and Bellatrix is in the [Bellatrix specs](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs/bellatrix).

My [Bellatrix annotated specification](/../bellatrix/part3/) remains available.

### Capella <!-- /part4/history/capella/ -->

The Capella upgrade took place at 22:27:35 UTC on April the 12th, 2023.

Capella included the following updates:

  - it enabled [automatic withdrawals](/part3/transition/block/#def_process_withdrawals) from the beacon chain to Eth1 accounts;
  - it provided a way for stakers to make a [one-time change](/part3/transition/block/#def_process_bls_to_execution_change) from BLS-style withdrawal credentials to Eth1-style withdrawal credentials;
  - the way historical roots are stored in the beacon state [was modified](/part3/transition/epoch/#def_process_historical_summaries_update); and,
  - there was a big refactor of the [fork choice rule](/part3/forkchoice/) specification, along with some behavioural changes.

By finally enabling withdrawals, Capella in some sense completed The Merge, fulfilling the promise to stakers made 2.5 years earlier that they would eventually be able to claim their rewards and reclaim their stakes.

Two withdrawal mechanisms were implemented.

  1. Exited and withdrawable validators have their full balances automatically transferred to their withdrawal addresses.
  2. Excess balances from active validators are regularly swept into their withdrawal addresses.

The consensus layer's Capella upgrade took place at the same time as the execution layer's [Shanghai upgrade](https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/shanghai.md).

The full description of the changes between Bellatrix and Capella is in the [Capella specs](https://github.com/ethereum/consensus-specs/tree/v1.3.0/specs/capella).

### Deneb <!-- /part4/history/deneb/ -->

The consensus layer upgrade following Capella has been given the name [Deneb](https://hackmd.io/@benjaminion/Hkm5x5acj#d-star-name) and will take place simultaneously with the execution layer's [Cancun upgrade](https://github.com/ethereum/execution-specs/blob/master/network-upgrades/mainnet-upgrades/cancun.md).

The main work included in Deneb will be [the work needed](https://github.com/ethereum/consensus-specs/tree/dev/specs/deneb) for the consensus layer to support [EIP-4844](https://eips.ethereum.org/EIPS/eip-4844) data availability.

The following changes are also [planned for inclusion](https://github.com/ethereum/consensus-specs/releases/tag/v1.4.0-beta.0).

  - [EIP-7044](https://eips.ethereum.org/EIPS/eip-7044): Lock voluntary exit domain on Capella [#3288](https://github.com/ethereum/consensus-specs/pull/3288)
  - [EIP-7045](https://eips.ethereum.org/EIPS/eip-7045): Increase max attestation inclusion slot [#3360](https://github.com/ethereum/consensus-specs/pull/3360)
  - [EIP-4788](https://eips.ethereum.org/EIPS/eip-4788): Expose parent beacon block root in execution layer [#3421](https://github.com/ethereum/consensus-specs/pull/3421)
  - [EIP-7514](https://eips.ethereum.org/EIPS/eip-7514): Add Max Epoch Churn Limit [#3499](https://github.com/ethereum/consensus-specs/pull/3499)
  - Apply proposer boost to first block in case of equivocation [#3352](https://github.com/ethereum/consensus-specs/pull/3352)

### Electra <!-- /part4/history/electra/ -->

The consensus layer upgrade following Deneb has been given the name Electra and will take place simultaneously with the execution layer's Prague upgrade.

An early stage discussion around the potential scope of the Electra upgrade is taking place in [Issue 3449](https://github.com/ethereum/consensus-specs/issues/3449) on the consensus specs repo.

## The Merge <!-- /part4/merge/* -->

TODO

### History <!-- /part4/merge/history/* -->

TODO

#### Testing the Merge

TODO

### Architecture <!-- /part4/merge/architecture/* -->

TODO

### Transition <!-- /part4/merge/transition/* -->

TODO

### Engine API <!-- /part4/merge/api/* -->

TODO

### Optimistic sync <!-- /part4/merge/optimistic-sync/* -->

TODO

# Part 5: Future <!-- /part5/ -->

## Introduction <!-- /part5/introduction/* -->

TODO

## Withdrawals <!-- /part5/withdrawals/* -->

TODO

## Data Availability Sampling <!-- /part5/das/* -->

TODO

### Proto-Danksharding <!-- /part5/das/proto/* -->

TODO

### Full Danksharding <!-- /part5/das/danksharding/* -->

TODO

## Distributed Validator Technology <!-- /part5/dvt/* -->

### Introduction

TODO

### Multi-party Compute <!-- /part5/dvt/mpc/* -->

TODO

### Consensus <!-- /part5/dvt/consensus/* -->

TODO

## Light Clients <!-- /part5/light_clients/* -->

### Introduction

TODO

### Syncing <!-- /part5/light_clients/syncing/* -->

TODO

### Protocol <!-- /part5/light_clients/protocol/* -->

TODO

## Active Research Topics <!-- /part5/research/* -->

### Introduction

TODO

### Proofs of Custody <!-- /part5/research/custody/* -->

TODO

### Builder / proposer split <!-- /part5/research/builders_proposers/* -->

TODO

### Consensus changes <!-- /part5/research/consensus/* -->

TODO

### Single slot finality <!-- /part5/research/single_slot_finality/* -->

TODO

References:

  - [Paths toward single-slot finality](https://notes.ethereum.org/@vbuterin/single_slot_finality)
  - [A simple Single Slot Finality protocol](https://ethresear.ch/t/a-simple-single-slot-finality-protocol/14920?u=benjaminion)

### Verkle trees <!-- /part5/research/verkle_trees/* -->

TODO

### Statelessness <!-- /part5/research/statelessness/* -->

TODO

### Single Secret Leader Election <!-- /part5/research/ssle/* -->

TODO

### Verifiable Delay Function <!-- /part5/research/vdf/* -->

TODO

### Post-quantum crypto <!-- /part5/research/post-quantum/* -->

TODO

### S[NT]ARK-friendly state transitions <!-- /part5/research/snark/* -->

TODO

# Appendices <!-- /appendices/ -->

## Staking <!-- /appendices/staking/* -->

### Introduction

TODO

### Ways to Stake <!-- /appendices/staking/ways/* -->

TODO

### Client Diversity <!-- /appendices/staking/diversity/* -->

TODO

### FAQ <!-- /appendices/staking/faq/* -->

TODO

## How to become a core dev <!-- /appendices/core-dev/* -->

### So you wanna be a core dev?

TODO

### Resources <!-- /appendices/core-dev/resources/* -->

TODO

## Reference <!-- /appendices/reference/ -->

TODO

### Running the spec <!-- /appendices/running/ -->

#### Introduction

Being written in Python, the spec itself is executable. This is wonderful for generating test cases and there is a whole [infrastructure](https://github.com/ethereum/consensus-specs/tree/dev/tests/generators) in the specs repo for doing just that.

We can also run the spec ourselves to do interesting things. In this exercise we will calculate the minimum and maximum sizes of the various [containers](/part3/containers/) defined by the spec. The following code is from [Protolambda](https://gist.github.com/protolambda/db75c7faa1e94f2464787a480e5d613e#file-compute_bounds-py), lightly modified to simplify and update it.

```python
from inspect import getmembers, isclass
from eth2spec.utils.ssz.ssz_typing import Container
from eth2spec.capella import mainnet

def get_spec_ssz_types():
    return [
        value for (_, value) in getmembers(mainnet, isclass)
        if issubclass(value, Container) and value != Container  # only the subclasses, not the imported base class
    ]

type_bounds = {
    value.__name__: ({
        'size': value.type_byte_length()
    } if value.is_fixed_byte_length() else {
        'min_size': value.min_byte_length(),
        'max_size': value.max_byte_length(),
    }) for value in get_spec_ssz_types()
}

import json
print(json.dumps(type_bounds))
```

#### Set up

We have a few hoops to jump through to get things installed for the first time. The below works well for me on Linux, but I haven't tested extensive variations. Just use the commands prefixed with `>`. I've included some output so that you can check whether things are on the right lines.

First, set up a Python virtual environment.

```bash
> git clone https://github.com/ethereum/consensus-specs.git
Cloning into 'consensus-specs'...
...
> cd consensus-specs/
> python3 -m venv .
> source bin/activate
(consensus-specs) > python --version
Python 3.10.6
```

Now we install and build all the dependencies required for the actual specs.

```bash
(consensus-specs) > python setup.py install
... tons of output ...
(consensus-specs) > make install_test
... lots more output ...
(consensus-specs) > python setup.py pyspecdev
running pyspecdev
running build_py command
running pyspec
...
```

#### Run

Finally, we can simply run the Python script from above. Copy it into a file called `sizes.py` and run it as follows.

```bash
(consensus-specs) > python sizes.py | jq
{
  "AggregateAndProof": {
    "min_size": 337,
    "max_size": 593
  },
...
```

The pipe to `jq` is optional, you will just get less pretty output without it.

<details>
<summary>Full output</summary>

Values are bytes. Don't be alarmed that the maximum size of `BeaconState` turns out to be 139&nbsp;TiB, or that `BeaconBlockBody` can be enormous. These sizes are based on the notional [maximum SSZ list lengths](/part2/building_blocks/ssz/#lists) they contain, and are not realistic in practice.

```none
{
  "AggregateAndProof": {
    "min_size": 337,
    "max_size": 593
  },
  "Attestation": {
    "min_size": 229,
    "max_size": 485
  },
  "AttestationData": {
    "size": 128
  },
  "AttesterSlashing": {
    "min_size": 464,
    "max_size": 33232
  },
  "BLSToExecutionChange": {
    "size": 76
  },
  "BeaconBlock": {
    "min_size": 984,
    "max_size": 1125899911198752
  },
  "BeaconBlockBody": {
    "min_size": 900,
    "max_size": 1125899911198668
  },
  "BeaconBlockHeader": {
    "size": 112
  },
  "BeaconState": {
    "min_size": 2737221,
    "max_size": 152833729758309
  },
  "Checkpoint": {
    "size": 40
  },
  "ContributionAndProof": {
    "size": 264
  },
  "Deposit": {
    "size": 1240
  },
  "DepositData": {
    "size": 184
  },
  "DepositMessage": {
    "size": 88
  },
  "Eth1Block": {
    "size": 48
  },
  "Eth1Data": {
    "size": 72
  },
  "ExecutionPayload": {
    "min_size": 512,
    "max_size": 1125899911038176
  },
  "ExecutionPayloadHeader": {
    "min_size": 568,
    "max_size": 600
  },
  "Fork": {
    "size": 16
  },
  "ForkData": {
    "size": 36
  },
  "HistoricalBatch": {
    "size": 524288
  },
  "HistoricalSummary": {
    "size": 64
  },
  "IndexedAttestation": {
    "min_size": 228,
    "max_size": 16612
  },
  "LightClientBootstrap": {
    "min_size": 25600,
    "max_size": 25632
  },
  "LightClientFinalityUpdate": {
    "min_size": 1992,
    "max_size": 2056
  },
  "LightClientHeader": {
    "min_size": 812,
    "max_size": 844
  },
  "LightClientOptimisticUpdate": {
    "min_size": 984,
    "max_size": 1016
  },
  "LightClientUpdate": {
    "min_size": 26776,
    "max_size": 26840
  },
  "PendingAttestation": {
    "min_size": 149,
    "max_size": 405
  },
  "PowBlock": {
    "size": 96
  },
  "ProposerSlashing": {
    "size": 416
  },
  "SignedAggregateAndProof": {
    "min_size": 437,
    "max_size": 693
  },
  "SignedBLSToExecutionChange": {
    "size": 172
  },
  "SignedBeaconBlock": {
    "min_size": 1084,
    "max_size": 1125899911198852
  },
  "SignedBeaconBlockHeader": {
    "size": 208
  },
  "SignedContributionAndProof": {
    "size": 360
  },
  "SignedVoluntaryExit": {
    "size": 112
  },
  "SigningData": {
    "size": 64
  },
  "SyncAggregate": {
    "size": 160
  },
  "SyncAggregatorSelectionData": {
    "size": 16
  },
  "SyncCommittee": {
    "size": 24624
  },
  "SyncCommitteeContribution": {
    "size": 160
  },
  "SyncCommitteeMessage": {
    "size": 144
  },
  "Validator": {
    "size": 121
  },
  "VoluntaryExit": {
    "size": 16
  },
  "Withdrawal": {
    "size": 44
  }
}
```

</details>

#### See also

Hsiao-Wei Wang gave a [Lightning Talk](https://archive.devcon.org/archive/watch/6/how-to-use-executable-consensus-pyspec/) on the consensus Pyspec at Devcon VI. She swiftly covers how it is structured, how to run it, and how to build test cases. The [presentation slides](https://docs.google.com/presentation/d/10HdtwTaFdTVLaiIGQJClyCs8AzrPXS20i78LZnPXHyo/edit?usp=sharing) are available.

### Sizes of containers <!-- /appendices/reference/sizes/* -->

TODO

## Glossary <!-- /appendices/reference/glossary/* -->

TODO
